WARNING: python3 and any of its children processes will be profiled.

Exported graph: graph(%x.1 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], requires_grad=0, device=cuda:0),
      %k.1 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], requires_grad=0, device=cuda:0),
      %v.1 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], requires_grad=0, device=cuda:0),
      %weight_q : Float(12, 64, 64, strides=[4096, 64, 1], requires_grad=0, device=cuda:0),
      %weight_k : Float(12, 64, 64, strides=[4096, 64, 1], requires_grad=0, device=cuda:0),
      %weight_v : Float(12, 64, 64, strides=[4096, 64, 1], requires_grad=0, device=cuda:0),
      %weight_o : Float(12, 64, 64, strides=[4096, 64, 1], requires_grad=0, device=cuda:0)):
  %self.start_len : Long(device=cpu) = onnx::Constant[value={32}]()
  %/Constant_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:51:12
  %/Add_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::Add[onnx_name="/Add"](%k.1, %/Constant_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:51:12
  %/Constant_1_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_1"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:52:12
  %/Add_1_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::Add[onnx_name="/Add_1"](%v.1, %/Constant_1_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:52:12
  %/MatMul_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul"](%x.1, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_1_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_1"](%x.1, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Constant_2_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_2"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_3_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_3"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_4_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_4"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_5_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_5"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_6_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_6"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_7_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_7"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_8_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_8"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_9_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_9"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_10_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_10"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_11_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_11"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_12_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_12"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_13_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_13"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_14_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_14"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_15_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_15"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_16_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_16"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_17_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_17"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_18_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_18"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_19_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_19"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_20_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_20"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_21_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_21"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_22_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_22"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_23_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_23"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_24_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_24"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_25_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_25"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_26_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_26"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_27_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_27"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_28_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_28"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_29_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_29"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_30_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_30"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_31_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_31"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_32_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_32"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_33_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_33"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_34_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_34"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_35_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_35"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_36_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_36"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_37_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_37"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_38_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_38"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_39_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_39"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_40_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_40"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_41_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_41"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_42_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_42"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_43_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_43"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_44_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_44"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_45_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_45"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_46_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_46"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_47_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_47"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_48_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_48"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_49_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_49"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_50_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_50"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_51_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_51"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_52_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_52"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_53_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_53"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_54_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_54"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_55_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_55"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_56_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_56"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_57_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_57"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_58_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_58"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_59_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_59"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_60_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_60"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_61_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_61"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_62_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_62"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_63_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_63"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Constant_64_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_64"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Constant_65_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12  64 [ CPULongType{3} ], onnx_name="/Constant_65"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Reshape_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape"](%/MatMul_1_output_0, %/Constant_2_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather"](%/Add_output_0, %self.start_len), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape"](%/Gather_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand"](%/Reshape_output_0, %/Shape_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_66_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_66"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_67_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_67"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_68_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_68"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_69_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={32}, onnx_name="/Constant_69"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_70_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_70"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_71_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_71"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape"](%/Constant_71_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_72_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_72"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul"](%/ConstantOfShape_output_0, %/Constant_72_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal"](%/Constant_70_output_0, %/Mul_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where"](%/Equal_output_0, %/ConstantOfShape_output_0, %/Constant_70_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_1_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_1"](%/Constant_67_output_0, %/Where_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze"](%/Expand_1_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_73_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_73"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_1_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_1"](%/Constant_73_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_74_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_74"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_1_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_1"](%/ConstantOfShape_1_output_0, %/Constant_74_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_1_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_1"](%/Constant_70_output_0, %/Mul_1_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_1_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_1"](%/Equal_1_output_0, %/ConstantOfShape_1_output_0, %/Constant_70_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_2_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_2"](%/Constant_68_output_0, %/Where_1_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_1_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_1"](%/Expand_2_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_75_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_75"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_2_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_2"](%/Constant_75_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_76_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_76"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_2_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_2"](%/ConstantOfShape_2_output_0, %/Constant_76_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_2_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_2"](%/Constant_70_output_0, %/Mul_2_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_2_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_2"](%/Equal_2_output_0, %/ConstantOfShape_2_output_0, %/Constant_70_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_3_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_3"](%/Constant_69_output_0, %/Where_2_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_2_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_2"](%/Expand_3_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_77_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_77"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_3_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_3"](%/Constant_77_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_78_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_78"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_3_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_3"](%/ConstantOfShape_3_output_0, %/Constant_78_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_3_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_3"](%/Constant_70_output_0, %/Mul_3_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_3_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_3"](%/Equal_3_output_0, %/ConstantOfShape_3_output_0, %/Constant_70_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_4_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_4"](%/Constant_66_output_0, %/Where_3_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_3_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_3"](%/Expand_4_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat"](%/Unsqueeze_output_0, %/Unsqueeze_1_output_0, %/Unsqueeze_2_output_0, %/Unsqueeze_3_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_1_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_1"](%/Add_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_79_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_79"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_80_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_80"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_81_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_81"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice"](%/Shape_1_output_0, %/Constant_80_output_0, %/Constant_81_output_0, %/Constant_79_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_1_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_1"](%/Constant_70_output_0, %/Slice_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_1_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_1"](%/Expand_output_0, %/Concat_1_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND"](%/Add_output_0, %/Concat_output_0, %/Reshape_1_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_2_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_2"](%x.1, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_2_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_2"](%/MatMul_2_output_0, %/Constant_3_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_1_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_1"](%/Add_1_output_0, %self.start_len), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_2_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_2"](%/Gather_1_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_5_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_5"](%/Reshape_2_output_0, %/Shape_2_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_82_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_82"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_83_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_83"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_84_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_84"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_85_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={32}, onnx_name="/Constant_85"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_86_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_86"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_87_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_87"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_4_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_4"](%/Constant_87_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_88_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_88"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_4_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_4"](%/ConstantOfShape_4_output_0, %/Constant_88_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_4_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_4"](%/Constant_86_output_0, %/Mul_4_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_4_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_4"](%/Equal_4_output_0, %/ConstantOfShape_4_output_0, %/Constant_86_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_6_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_6"](%/Constant_83_output_0, %/Where_4_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_4_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_4"](%/Expand_6_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_89_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_89"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_5_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_5"](%/Constant_89_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_90_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_90"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_5_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_5"](%/ConstantOfShape_5_output_0, %/Constant_90_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_5_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_5"](%/Constant_86_output_0, %/Mul_5_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_5_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_5"](%/Equal_5_output_0, %/ConstantOfShape_5_output_0, %/Constant_86_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_7_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_7"](%/Constant_84_output_0, %/Where_5_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_5_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_5"](%/Expand_7_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_91_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_91"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_6_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_6"](%/Constant_91_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_92_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_92"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_6_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_6"](%/ConstantOfShape_6_output_0, %/Constant_92_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_6_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_6"](%/Constant_86_output_0, %/Mul_6_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_6_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_6"](%/Equal_6_output_0, %/ConstantOfShape_6_output_0, %/Constant_86_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_8_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_8"](%/Constant_85_output_0, %/Where_6_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_6_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_6"](%/Expand_8_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_93_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_93"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_7_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_7"](%/Constant_93_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_94_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_94"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_7_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_7"](%/ConstantOfShape_7_output_0, %/Constant_94_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_7_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_7"](%/Constant_86_output_0, %/Mul_7_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_7_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_7"](%/Equal_7_output_0, %/ConstantOfShape_7_output_0, %/Constant_86_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_9_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_9"](%/Constant_82_output_0, %/Where_7_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_7_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_7"](%/Expand_9_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_2_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_2"](%/Unsqueeze_4_output_0, %/Unsqueeze_5_output_0, %/Unsqueeze_6_output_0, %/Unsqueeze_7_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_3_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_3"](%/Add_1_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_95_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_95"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_96_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_96"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_97_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_97"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_1_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_1"](%/Shape_3_output_0, %/Constant_96_output_0, %/Constant_97_output_0, %/Constant_95_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_3_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_3"](%/Constant_86_output_0, %/Slice_1_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_3_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_3"](%/Expand_5_output_0, %/Concat_3_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_1_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_1"](%/Add_1_output_0, %/Concat_2_output_0, %/Reshape_3_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose"](%/MatMul_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_3_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_3"](%/ScatterND_output_0, %/Transpose_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_1_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_1"](%/MatMul_3_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_98_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_98"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_8_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_8"](%/Transpose_1_output_0, %/Constant_98_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax"](%/Mul_8_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_4_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_4"](%/Softmax_output_0, %/ScatterND_1_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_5_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_5"](%/MatMul_4_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_396 : Long(device=cpu) = onnx::Constant[value={33}]()
  %/MatMul_6_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_6"](%/MatMul_5_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_7_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_7"](%/MatMul_5_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_4_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_4"](%/MatMul_7_output_0, %/Constant_4_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_2_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_2"](%/ScatterND_output_0, %onnx::Gather_396), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_4_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_4"](%/Gather_2_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_10_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_10"](%/Reshape_4_output_0, %/Shape_4_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_99_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_99"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_100_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_100"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_101_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_101"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_102_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={33}, onnx_name="/Constant_102"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_103_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_103"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_104_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_104"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_8_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_8"](%/Constant_104_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_105_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_105"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_9_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_9"](%/ConstantOfShape_8_output_0, %/Constant_105_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_8_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_8"](%/Constant_103_output_0, %/Mul_9_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_8_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_8"](%/Equal_8_output_0, %/ConstantOfShape_8_output_0, %/Constant_103_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_11_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_11"](%/Constant_100_output_0, %/Where_8_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_8_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_8"](%/Expand_11_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_106_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_106"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_9_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_9"](%/Constant_106_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_107_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_107"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_10_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_10"](%/ConstantOfShape_9_output_0, %/Constant_107_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_9_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_9"](%/Constant_103_output_0, %/Mul_10_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_9_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_9"](%/Equal_9_output_0, %/ConstantOfShape_9_output_0, %/Constant_103_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_12_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_12"](%/Constant_101_output_0, %/Where_9_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_9_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_9"](%/Expand_12_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_108_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_108"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_10_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_10"](%/Constant_108_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_109_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_109"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_11_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_11"](%/ConstantOfShape_10_output_0, %/Constant_109_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_10_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_10"](%/Constant_103_output_0, %/Mul_11_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_10_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_10"](%/Equal_10_output_0, %/ConstantOfShape_10_output_0, %/Constant_103_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_13_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_13"](%/Constant_102_output_0, %/Where_10_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_10_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_10"](%/Expand_13_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_110_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_110"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_11_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_11"](%/Constant_110_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_111_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_111"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_12_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_12"](%/ConstantOfShape_11_output_0, %/Constant_111_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_11_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_11"](%/Constant_103_output_0, %/Mul_12_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_11_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_11"](%/Equal_11_output_0, %/ConstantOfShape_11_output_0, %/Constant_103_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_14_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_14"](%/Constant_99_output_0, %/Where_11_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_11_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_11"](%/Expand_14_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_4_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_4"](%/Unsqueeze_8_output_0, %/Unsqueeze_9_output_0, %/Unsqueeze_10_output_0, %/Unsqueeze_11_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_5_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_5"](%/ScatterND_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_112_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_112"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_113_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_113"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_114_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_114"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_2_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_2"](%/Shape_5_output_0, %/Constant_113_output_0, %/Constant_114_output_0, %/Constant_112_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_5_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_5"](%/Constant_103_output_0, %/Slice_2_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_5_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_5"](%/Expand_10_output_0, %/Concat_5_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_2_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_2"](%/ScatterND_output_0, %/Concat_4_output_0, %/Reshape_5_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_8_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_8"](%/MatMul_5_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_6_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_6"](%/MatMul_8_output_0, %/Constant_5_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_3_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_3"](%/ScatterND_1_output_0, %onnx::Gather_396), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_6_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_6"](%/Gather_3_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_15_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_15"](%/Reshape_6_output_0, %/Shape_6_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_115_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_115"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_116_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_116"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_117_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_117"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_118_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={33}, onnx_name="/Constant_118"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_119_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_119"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_120_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_120"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_12_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_12"](%/Constant_120_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_121_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_121"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_13_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_13"](%/ConstantOfShape_12_output_0, %/Constant_121_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_12_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_12"](%/Constant_119_output_0, %/Mul_13_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_12_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_12"](%/Equal_12_output_0, %/ConstantOfShape_12_output_0, %/Constant_119_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_16_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_16"](%/Constant_116_output_0, %/Where_12_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_12_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_12"](%/Expand_16_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_122_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_122"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_13_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_13"](%/Constant_122_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_123_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_123"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_14_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_14"](%/ConstantOfShape_13_output_0, %/Constant_123_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_13_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_13"](%/Constant_119_output_0, %/Mul_14_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_13_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_13"](%/Equal_13_output_0, %/ConstantOfShape_13_output_0, %/Constant_119_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_17_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_17"](%/Constant_117_output_0, %/Where_13_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_13_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_13"](%/Expand_17_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_124_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_124"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_14_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_14"](%/Constant_124_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_125_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_125"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_15_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_15"](%/ConstantOfShape_14_output_0, %/Constant_125_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_14_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_14"](%/Constant_119_output_0, %/Mul_15_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_14_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_14"](%/Equal_14_output_0, %/ConstantOfShape_14_output_0, %/Constant_119_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_18_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_18"](%/Constant_118_output_0, %/Where_14_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_14_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_14"](%/Expand_18_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_126_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_126"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_15_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_15"](%/Constant_126_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_127_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_127"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_16_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_16"](%/ConstantOfShape_15_output_0, %/Constant_127_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_15_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_15"](%/Constant_119_output_0, %/Mul_16_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_15_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_15"](%/Equal_15_output_0, %/ConstantOfShape_15_output_0, %/Constant_119_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_19_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_19"](%/Constant_115_output_0, %/Where_15_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_15_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_15"](%/Expand_19_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_6_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_6"](%/Unsqueeze_12_output_0, %/Unsqueeze_13_output_0, %/Unsqueeze_14_output_0, %/Unsqueeze_15_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_7_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_7"](%/ScatterND_1_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_128_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_128"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_129_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_129"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_130_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_130"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_3_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_3"](%/Shape_7_output_0, %/Constant_129_output_0, %/Constant_130_output_0, %/Constant_128_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_7_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_7"](%/Constant_119_output_0, %/Slice_3_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_7_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_7"](%/Expand_15_output_0, %/Concat_7_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_3_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_3"](%/ScatterND_1_output_0, %/Concat_6_output_0, %/Reshape_7_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_2_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_2"](%/MatMul_6_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_9_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_9"](%/ScatterND_2_output_0, %/Transpose_2_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_3_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_3"](%/MatMul_9_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_131_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_131"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_17_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_17"](%/Transpose_3_output_0, %/Constant_131_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_1_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_1"](%/Mul_17_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_10_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_10"](%/Softmax_1_output_0, %/ScatterND_3_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_11_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_11"](%/MatMul_10_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_522 : Long(device=cpu) = onnx::Constant[value={34}]()
  %/MatMul_12_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_12"](%/MatMul_11_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_13_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_13"](%/MatMul_11_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_8_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_8"](%/MatMul_13_output_0, %/Constant_6_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_4_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_4"](%/ScatterND_2_output_0, %onnx::Gather_522), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_8_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_8"](%/Gather_4_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_20_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_20"](%/Reshape_8_output_0, %/Shape_8_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_132_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_132"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_133_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_133"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_134_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_134"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_135_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={34}, onnx_name="/Constant_135"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_136_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_136"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_137_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_137"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_16_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_16"](%/Constant_137_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_138_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_138"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_18_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_18"](%/ConstantOfShape_16_output_0, %/Constant_138_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_16_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_16"](%/Constant_136_output_0, %/Mul_18_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_16_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_16"](%/Equal_16_output_0, %/ConstantOfShape_16_output_0, %/Constant_136_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_21_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_21"](%/Constant_133_output_0, %/Where_16_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_16_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_16"](%/Expand_21_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_139_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_139"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_17_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_17"](%/Constant_139_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_140_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_140"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_19_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_19"](%/ConstantOfShape_17_output_0, %/Constant_140_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_17_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_17"](%/Constant_136_output_0, %/Mul_19_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_17_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_17"](%/Equal_17_output_0, %/ConstantOfShape_17_output_0, %/Constant_136_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_22_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_22"](%/Constant_134_output_0, %/Where_17_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_17_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_17"](%/Expand_22_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_141_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_141"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_18_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_18"](%/Constant_141_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_142_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_142"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_20_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_20"](%/ConstantOfShape_18_output_0, %/Constant_142_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_18_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_18"](%/Constant_136_output_0, %/Mul_20_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_18_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_18"](%/Equal_18_output_0, %/ConstantOfShape_18_output_0, %/Constant_136_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_23_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_23"](%/Constant_135_output_0, %/Where_18_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_18_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_18"](%/Expand_23_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_143_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_143"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_19_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_19"](%/Constant_143_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_144_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_144"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_21_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_21"](%/ConstantOfShape_19_output_0, %/Constant_144_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_19_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_19"](%/Constant_136_output_0, %/Mul_21_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_19_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_19"](%/Equal_19_output_0, %/ConstantOfShape_19_output_0, %/Constant_136_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_24_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_24"](%/Constant_132_output_0, %/Where_19_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_19_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_19"](%/Expand_24_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_8_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_8"](%/Unsqueeze_16_output_0, %/Unsqueeze_17_output_0, %/Unsqueeze_18_output_0, %/Unsqueeze_19_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_9_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_9"](%/ScatterND_2_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_145_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_145"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_146_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_146"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_147_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_147"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_4_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_4"](%/Shape_9_output_0, %/Constant_146_output_0, %/Constant_147_output_0, %/Constant_145_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_9_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_9"](%/Constant_136_output_0, %/Slice_4_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_9_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_9"](%/Expand_20_output_0, %/Concat_9_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_4_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_4"](%/ScatterND_2_output_0, %/Concat_8_output_0, %/Reshape_9_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_14_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_14"](%/MatMul_11_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_10_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_10"](%/MatMul_14_output_0, %/Constant_7_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_5_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_5"](%/ScatterND_3_output_0, %onnx::Gather_522), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_10_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_10"](%/Gather_5_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_25_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_25"](%/Reshape_10_output_0, %/Shape_10_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_148_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_148"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_149_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_149"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_150_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_150"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_151_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={34}, onnx_name="/Constant_151"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_152_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_152"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_153_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_153"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_20_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_20"](%/Constant_153_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_154_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_154"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_22_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_22"](%/ConstantOfShape_20_output_0, %/Constant_154_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_20_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_20"](%/Constant_152_output_0, %/Mul_22_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_20_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_20"](%/Equal_20_output_0, %/ConstantOfShape_20_output_0, %/Constant_152_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_26_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_26"](%/Constant_149_output_0, %/Where_20_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_20_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_20"](%/Expand_26_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_155_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_155"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_21_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_21"](%/Constant_155_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_156_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_156"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_23_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_23"](%/ConstantOfShape_21_output_0, %/Constant_156_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_21_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_21"](%/Constant_152_output_0, %/Mul_23_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_21_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_21"](%/Equal_21_output_0, %/ConstantOfShape_21_output_0, %/Constant_152_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_27_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_27"](%/Constant_150_output_0, %/Where_21_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_21_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_21"](%/Expand_27_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_157_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_157"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_22_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_22"](%/Constant_157_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_158_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_158"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_24_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_24"](%/ConstantOfShape_22_output_0, %/Constant_158_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_22_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_22"](%/Constant_152_output_0, %/Mul_24_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_22_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_22"](%/Equal_22_output_0, %/ConstantOfShape_22_output_0, %/Constant_152_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_28_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_28"](%/Constant_151_output_0, %/Where_22_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_22_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_22"](%/Expand_28_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_159_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_159"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_23_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_23"](%/Constant_159_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_160_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_160"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_25_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_25"](%/ConstantOfShape_23_output_0, %/Constant_160_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_23_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_23"](%/Constant_152_output_0, %/Mul_25_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_23_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_23"](%/Equal_23_output_0, %/ConstantOfShape_23_output_0, %/Constant_152_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_29_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_29"](%/Constant_148_output_0, %/Where_23_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_23_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_23"](%/Expand_29_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_10_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_10"](%/Unsqueeze_20_output_0, %/Unsqueeze_21_output_0, %/Unsqueeze_22_output_0, %/Unsqueeze_23_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_11_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_11"](%/ScatterND_3_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_161_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_161"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_162_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_162"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_163_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_163"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_5_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_5"](%/Shape_11_output_0, %/Constant_162_output_0, %/Constant_163_output_0, %/Constant_161_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_11_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_11"](%/Constant_152_output_0, %/Slice_5_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_11_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_11"](%/Expand_25_output_0, %/Concat_11_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_5_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_5"](%/ScatterND_3_output_0, %/Concat_10_output_0, %/Reshape_11_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_4_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_4"](%/MatMul_12_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_15_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_15"](%/ScatterND_4_output_0, %/Transpose_4_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_5_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_5"](%/MatMul_15_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_164_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_164"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_26_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_26"](%/Transpose_5_output_0, %/Constant_164_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_2_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_2"](%/Mul_26_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_16_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_16"](%/Softmax_2_output_0, %/ScatterND_5_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_17_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_17"](%/MatMul_16_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_648 : Long(device=cpu) = onnx::Constant[value={35}]()
  %/MatMul_18_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_18"](%/MatMul_17_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_19_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_19"](%/MatMul_17_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_12_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_12"](%/MatMul_19_output_0, %/Constant_8_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_6_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_6"](%/ScatterND_4_output_0, %onnx::Gather_648), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_12_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_12"](%/Gather_6_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_30_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_30"](%/Reshape_12_output_0, %/Shape_12_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_165_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_165"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_166_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_166"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_167_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_167"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_168_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={35}, onnx_name="/Constant_168"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_169_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_169"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_170_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_170"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_24_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_24"](%/Constant_170_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_171_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_171"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_27_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_27"](%/ConstantOfShape_24_output_0, %/Constant_171_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_24_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_24"](%/Constant_169_output_0, %/Mul_27_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_24_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_24"](%/Equal_24_output_0, %/ConstantOfShape_24_output_0, %/Constant_169_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_31_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_31"](%/Constant_166_output_0, %/Where_24_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_24_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_24"](%/Expand_31_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_172_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_172"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_25_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_25"](%/Constant_172_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_173_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_173"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_28_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_28"](%/ConstantOfShape_25_output_0, %/Constant_173_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_25_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_25"](%/Constant_169_output_0, %/Mul_28_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_25_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_25"](%/Equal_25_output_0, %/ConstantOfShape_25_output_0, %/Constant_169_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_32_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_32"](%/Constant_167_output_0, %/Where_25_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_25_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_25"](%/Expand_32_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_174_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_174"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_26_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_26"](%/Constant_174_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_175_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_175"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_29_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_29"](%/ConstantOfShape_26_output_0, %/Constant_175_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_26_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_26"](%/Constant_169_output_0, %/Mul_29_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_26_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_26"](%/Equal_26_output_0, %/ConstantOfShape_26_output_0, %/Constant_169_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_33_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_33"](%/Constant_168_output_0, %/Where_26_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_26_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_26"](%/Expand_33_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_176_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_176"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_27_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_27"](%/Constant_176_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_177_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_177"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_30_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_30"](%/ConstantOfShape_27_output_0, %/Constant_177_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_27_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_27"](%/Constant_169_output_0, %/Mul_30_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_27_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_27"](%/Equal_27_output_0, %/ConstantOfShape_27_output_0, %/Constant_169_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_34_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_34"](%/Constant_165_output_0, %/Where_27_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_27_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_27"](%/Expand_34_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_12_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_12"](%/Unsqueeze_24_output_0, %/Unsqueeze_25_output_0, %/Unsqueeze_26_output_0, %/Unsqueeze_27_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_13_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_13"](%/ScatterND_4_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_178_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_178"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_179_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_179"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_180_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_180"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_6_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_6"](%/Shape_13_output_0, %/Constant_179_output_0, %/Constant_180_output_0, %/Constant_178_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_13_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_13"](%/Constant_169_output_0, %/Slice_6_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_13_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_13"](%/Expand_30_output_0, %/Concat_13_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_6_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_6"](%/ScatterND_4_output_0, %/Concat_12_output_0, %/Reshape_13_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_20_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_20"](%/MatMul_17_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_14_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_14"](%/MatMul_20_output_0, %/Constant_9_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_7_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_7"](%/ScatterND_5_output_0, %onnx::Gather_648), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_14_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_14"](%/Gather_7_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_35_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_35"](%/Reshape_14_output_0, %/Shape_14_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_181_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_181"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_182_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_182"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_183_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_183"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_184_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={35}, onnx_name="/Constant_184"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_185_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_185"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_186_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_186"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_28_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_28"](%/Constant_186_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_187_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_187"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_31_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_31"](%/ConstantOfShape_28_output_0, %/Constant_187_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_28_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_28"](%/Constant_185_output_0, %/Mul_31_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_28_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_28"](%/Equal_28_output_0, %/ConstantOfShape_28_output_0, %/Constant_185_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_36_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_36"](%/Constant_182_output_0, %/Where_28_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_28_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_28"](%/Expand_36_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_188_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_188"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_29_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_29"](%/Constant_188_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_189_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_189"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_32_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_32"](%/ConstantOfShape_29_output_0, %/Constant_189_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_29_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_29"](%/Constant_185_output_0, %/Mul_32_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_29_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_29"](%/Equal_29_output_0, %/ConstantOfShape_29_output_0, %/Constant_185_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_37_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_37"](%/Constant_183_output_0, %/Where_29_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_29_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_29"](%/Expand_37_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_190_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_190"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_30_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_30"](%/Constant_190_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_191_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_191"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_33_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_33"](%/ConstantOfShape_30_output_0, %/Constant_191_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_30_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_30"](%/Constant_185_output_0, %/Mul_33_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_30_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_30"](%/Equal_30_output_0, %/ConstantOfShape_30_output_0, %/Constant_185_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_38_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_38"](%/Constant_184_output_0, %/Where_30_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_30_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_30"](%/Expand_38_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_192_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_192"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_31_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_31"](%/Constant_192_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_193_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_193"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_34_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_34"](%/ConstantOfShape_31_output_0, %/Constant_193_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_31_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_31"](%/Constant_185_output_0, %/Mul_34_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_31_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_31"](%/Equal_31_output_0, %/ConstantOfShape_31_output_0, %/Constant_185_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_39_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_39"](%/Constant_181_output_0, %/Where_31_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_31_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_31"](%/Expand_39_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_14_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_14"](%/Unsqueeze_28_output_0, %/Unsqueeze_29_output_0, %/Unsqueeze_30_output_0, %/Unsqueeze_31_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_15_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_15"](%/ScatterND_5_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_194_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_194"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_195_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_195"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_196_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_196"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_7_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_7"](%/Shape_15_output_0, %/Constant_195_output_0, %/Constant_196_output_0, %/Constant_194_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_15_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_15"](%/Constant_185_output_0, %/Slice_7_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_15_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_15"](%/Expand_35_output_0, %/Concat_15_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_7_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_7"](%/ScatterND_5_output_0, %/Concat_14_output_0, %/Reshape_15_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_6_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_6"](%/MatMul_18_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_21_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_21"](%/ScatterND_6_output_0, %/Transpose_6_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_7_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_7"](%/MatMul_21_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_197_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_197"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_35_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_35"](%/Transpose_7_output_0, %/Constant_197_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_3_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_3"](%/Mul_35_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_22_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_22"](%/Softmax_3_output_0, %/ScatterND_7_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_23_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_23"](%/MatMul_22_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_774 : Long(device=cpu) = onnx::Constant[value={36}]()
  %/MatMul_24_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_24"](%/MatMul_23_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_25_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_25"](%/MatMul_23_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_16_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_16"](%/MatMul_25_output_0, %/Constant_10_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_8_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_8"](%/ScatterND_6_output_0, %onnx::Gather_774), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_16_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_16"](%/Gather_8_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_40_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_40"](%/Reshape_16_output_0, %/Shape_16_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_198_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_198"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_199_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_199"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_200_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_200"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_201_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={36}, onnx_name="/Constant_201"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_202_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_202"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_203_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_203"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_32_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_32"](%/Constant_203_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_204_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_204"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_36_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_36"](%/ConstantOfShape_32_output_0, %/Constant_204_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_32_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_32"](%/Constant_202_output_0, %/Mul_36_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_32_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_32"](%/Equal_32_output_0, %/ConstantOfShape_32_output_0, %/Constant_202_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_41_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_41"](%/Constant_199_output_0, %/Where_32_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_32_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_32"](%/Expand_41_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_205_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_205"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_33_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_33"](%/Constant_205_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_206_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_206"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_37_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_37"](%/ConstantOfShape_33_output_0, %/Constant_206_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_33_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_33"](%/Constant_202_output_0, %/Mul_37_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_33_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_33"](%/Equal_33_output_0, %/ConstantOfShape_33_output_0, %/Constant_202_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_42_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_42"](%/Constant_200_output_0, %/Where_33_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_33_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_33"](%/Expand_42_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_207_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_207"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_34_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_34"](%/Constant_207_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_208_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_208"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_38_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_38"](%/ConstantOfShape_34_output_0, %/Constant_208_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_34_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_34"](%/Constant_202_output_0, %/Mul_38_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_34_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_34"](%/Equal_34_output_0, %/ConstantOfShape_34_output_0, %/Constant_202_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_43_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_43"](%/Constant_201_output_0, %/Where_34_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_34_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_34"](%/Expand_43_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_209_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_209"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_35_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_35"](%/Constant_209_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_210_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_210"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_39_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_39"](%/ConstantOfShape_35_output_0, %/Constant_210_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_35_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_35"](%/Constant_202_output_0, %/Mul_39_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_35_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_35"](%/Equal_35_output_0, %/ConstantOfShape_35_output_0, %/Constant_202_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_44_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_44"](%/Constant_198_output_0, %/Where_35_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_35_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_35"](%/Expand_44_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_16_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_16"](%/Unsqueeze_32_output_0, %/Unsqueeze_33_output_0, %/Unsqueeze_34_output_0, %/Unsqueeze_35_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_17_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_17"](%/ScatterND_6_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_211_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_211"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_212_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_212"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_213_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_213"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_8_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_8"](%/Shape_17_output_0, %/Constant_212_output_0, %/Constant_213_output_0, %/Constant_211_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_17_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_17"](%/Constant_202_output_0, %/Slice_8_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_17_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_17"](%/Expand_40_output_0, %/Concat_17_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_8_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_8"](%/ScatterND_6_output_0, %/Concat_16_output_0, %/Reshape_17_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_26_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_26"](%/MatMul_23_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_18_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_18"](%/MatMul_26_output_0, %/Constant_11_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_9_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_9"](%/ScatterND_7_output_0, %onnx::Gather_774), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_18_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_18"](%/Gather_9_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_45_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_45"](%/Reshape_18_output_0, %/Shape_18_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_214_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_214"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_215_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_215"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_216_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_216"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_217_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={36}, onnx_name="/Constant_217"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_218_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_218"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_219_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_219"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_36_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_36"](%/Constant_219_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_220_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_220"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_40_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_40"](%/ConstantOfShape_36_output_0, %/Constant_220_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_36_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_36"](%/Constant_218_output_0, %/Mul_40_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_36_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_36"](%/Equal_36_output_0, %/ConstantOfShape_36_output_0, %/Constant_218_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_46_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_46"](%/Constant_215_output_0, %/Where_36_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_36_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_36"](%/Expand_46_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_221_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_221"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_37_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_37"](%/Constant_221_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_222_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_222"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_41_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_41"](%/ConstantOfShape_37_output_0, %/Constant_222_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_37_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_37"](%/Constant_218_output_0, %/Mul_41_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_37_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_37"](%/Equal_37_output_0, %/ConstantOfShape_37_output_0, %/Constant_218_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_47_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_47"](%/Constant_216_output_0, %/Where_37_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_37_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_37"](%/Expand_47_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_223_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_223"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_38_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_38"](%/Constant_223_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_224_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_224"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_42_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_42"](%/ConstantOfShape_38_output_0, %/Constant_224_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_38_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_38"](%/Constant_218_output_0, %/Mul_42_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_38_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_38"](%/Equal_38_output_0, %/ConstantOfShape_38_output_0, %/Constant_218_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_48_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_48"](%/Constant_217_output_0, %/Where_38_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_38_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_38"](%/Expand_48_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_225_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_225"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_39_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_39"](%/Constant_225_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_226_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_226"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_43_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_43"](%/ConstantOfShape_39_output_0, %/Constant_226_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_39_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_39"](%/Constant_218_output_0, %/Mul_43_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_39_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_39"](%/Equal_39_output_0, %/ConstantOfShape_39_output_0, %/Constant_218_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_49_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_49"](%/Constant_214_output_0, %/Where_39_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_39_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_39"](%/Expand_49_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_18_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_18"](%/Unsqueeze_36_output_0, %/Unsqueeze_37_output_0, %/Unsqueeze_38_output_0, %/Unsqueeze_39_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_19_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_19"](%/ScatterND_7_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_227_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_227"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_228_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_228"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_229_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_229"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_9_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_9"](%/Shape_19_output_0, %/Constant_228_output_0, %/Constant_229_output_0, %/Constant_227_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_19_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_19"](%/Constant_218_output_0, %/Slice_9_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_19_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_19"](%/Expand_45_output_0, %/Concat_19_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_9_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_9"](%/ScatterND_7_output_0, %/Concat_18_output_0, %/Reshape_19_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_8_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_8"](%/MatMul_24_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_27_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_27"](%/ScatterND_8_output_0, %/Transpose_8_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_9_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_9"](%/MatMul_27_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_230_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_230"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_44_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_44"](%/Transpose_9_output_0, %/Constant_230_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_4_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_4"](%/Mul_44_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_28_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_28"](%/Softmax_4_output_0, %/ScatterND_9_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_29_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_29"](%/MatMul_28_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_900 : Long(device=cpu) = onnx::Constant[value={37}]()
  %/MatMul_30_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_30"](%/MatMul_29_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_31_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_31"](%/MatMul_29_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_20_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_20"](%/MatMul_31_output_0, %/Constant_12_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_10_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_10"](%/ScatterND_8_output_0, %onnx::Gather_900), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_20_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_20"](%/Gather_10_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_50_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_50"](%/Reshape_20_output_0, %/Shape_20_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_231_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_231"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_232_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_232"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_233_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_233"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_234_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={37}, onnx_name="/Constant_234"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_235_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_235"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_236_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_236"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_40_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_40"](%/Constant_236_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_237_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_237"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_45_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_45"](%/ConstantOfShape_40_output_0, %/Constant_237_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_40_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_40"](%/Constant_235_output_0, %/Mul_45_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_40_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_40"](%/Equal_40_output_0, %/ConstantOfShape_40_output_0, %/Constant_235_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_51_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_51"](%/Constant_232_output_0, %/Where_40_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_40_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_40"](%/Expand_51_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_238_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_238"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_41_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_41"](%/Constant_238_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_239_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_239"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_46_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_46"](%/ConstantOfShape_41_output_0, %/Constant_239_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_41_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_41"](%/Constant_235_output_0, %/Mul_46_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_41_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_41"](%/Equal_41_output_0, %/ConstantOfShape_41_output_0, %/Constant_235_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_52_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_52"](%/Constant_233_output_0, %/Where_41_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_41_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_41"](%/Expand_52_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_240_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_240"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_42_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_42"](%/Constant_240_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_241_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_241"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_47_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_47"](%/ConstantOfShape_42_output_0, %/Constant_241_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_42_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_42"](%/Constant_235_output_0, %/Mul_47_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_42_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_42"](%/Equal_42_output_0, %/ConstantOfShape_42_output_0, %/Constant_235_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_53_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_53"](%/Constant_234_output_0, %/Where_42_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_42_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_42"](%/Expand_53_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_242_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_242"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_43_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_43"](%/Constant_242_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_243_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_243"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_48_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_48"](%/ConstantOfShape_43_output_0, %/Constant_243_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_43_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_43"](%/Constant_235_output_0, %/Mul_48_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_43_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_43"](%/Equal_43_output_0, %/ConstantOfShape_43_output_0, %/Constant_235_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_54_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_54"](%/Constant_231_output_0, %/Where_43_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_43_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_43"](%/Expand_54_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_20_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_20"](%/Unsqueeze_40_output_0, %/Unsqueeze_41_output_0, %/Unsqueeze_42_output_0, %/Unsqueeze_43_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_21_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_21"](%/ScatterND_8_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_244_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_244"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_245_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_245"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_246_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_246"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_10_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_10"](%/Shape_21_output_0, %/Constant_245_output_0, %/Constant_246_output_0, %/Constant_244_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_21_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_21"](%/Constant_235_output_0, %/Slice_10_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_21_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_21"](%/Expand_50_output_0, %/Concat_21_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_10_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_10"](%/ScatterND_8_output_0, %/Concat_20_output_0, %/Reshape_21_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_32_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_32"](%/MatMul_29_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_22_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_22"](%/MatMul_32_output_0, %/Constant_13_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_11_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_11"](%/ScatterND_9_output_0, %onnx::Gather_900), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_22_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_22"](%/Gather_11_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_55_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_55"](%/Reshape_22_output_0, %/Shape_22_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_247_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_247"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_248_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_248"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_249_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_249"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_250_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={37}, onnx_name="/Constant_250"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_251_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_251"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_252_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_252"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_44_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_44"](%/Constant_252_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_253_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_253"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_49_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_49"](%/ConstantOfShape_44_output_0, %/Constant_253_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_44_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_44"](%/Constant_251_output_0, %/Mul_49_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_44_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_44"](%/Equal_44_output_0, %/ConstantOfShape_44_output_0, %/Constant_251_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_56_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_56"](%/Constant_248_output_0, %/Where_44_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_44_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_44"](%/Expand_56_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_254_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_254"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_45_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_45"](%/Constant_254_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_255_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_255"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_50_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_50"](%/ConstantOfShape_45_output_0, %/Constant_255_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_45_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_45"](%/Constant_251_output_0, %/Mul_50_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_45_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_45"](%/Equal_45_output_0, %/ConstantOfShape_45_output_0, %/Constant_251_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_57_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_57"](%/Constant_249_output_0, %/Where_45_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_45_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_45"](%/Expand_57_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_256_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_256"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_46_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_46"](%/Constant_256_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_257_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_257"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_51_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_51"](%/ConstantOfShape_46_output_0, %/Constant_257_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_46_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_46"](%/Constant_251_output_0, %/Mul_51_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_46_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_46"](%/Equal_46_output_0, %/ConstantOfShape_46_output_0, %/Constant_251_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_58_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_58"](%/Constant_250_output_0, %/Where_46_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_46_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_46"](%/Expand_58_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_258_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_258"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_47_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_47"](%/Constant_258_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_259_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_259"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_52_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_52"](%/ConstantOfShape_47_output_0, %/Constant_259_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_47_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_47"](%/Constant_251_output_0, %/Mul_52_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_47_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_47"](%/Equal_47_output_0, %/ConstantOfShape_47_output_0, %/Constant_251_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_59_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_59"](%/Constant_247_output_0, %/Where_47_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_47_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_47"](%/Expand_59_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_22_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_22"](%/Unsqueeze_44_output_0, %/Unsqueeze_45_output_0, %/Unsqueeze_46_output_0, %/Unsqueeze_47_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_23_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_23"](%/ScatterND_9_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_260_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_260"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_261_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_261"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_262_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_262"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_11_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_11"](%/Shape_23_output_0, %/Constant_261_output_0, %/Constant_262_output_0, %/Constant_260_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_23_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_23"](%/Constant_251_output_0, %/Slice_11_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_23_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_23"](%/Expand_55_output_0, %/Concat_23_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_11_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_11"](%/ScatterND_9_output_0, %/Concat_22_output_0, %/Reshape_23_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_10_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_10"](%/MatMul_30_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_33_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_33"](%/ScatterND_10_output_0, %/Transpose_10_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_11_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_11"](%/MatMul_33_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_263_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_263"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_53_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_53"](%/Transpose_11_output_0, %/Constant_263_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_5_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_5"](%/Mul_53_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_34_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_34"](%/Softmax_5_output_0, %/ScatterND_11_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_35_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_35"](%/MatMul_34_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_1026 : Long(device=cpu) = onnx::Constant[value={38}]()
  %/MatMul_36_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_36"](%/MatMul_35_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_37_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_37"](%/MatMul_35_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_24_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_24"](%/MatMul_37_output_0, %/Constant_14_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_12_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_12"](%/ScatterND_10_output_0, %onnx::Gather_1026), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_24_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_24"](%/Gather_12_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_60_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_60"](%/Reshape_24_output_0, %/Shape_24_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_264_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_264"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_265_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_265"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_266_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_266"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_267_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={38}, onnx_name="/Constant_267"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_268_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_268"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_269_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_269"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_48_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_48"](%/Constant_269_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_270_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_270"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_54_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_54"](%/ConstantOfShape_48_output_0, %/Constant_270_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_48_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_48"](%/Constant_268_output_0, %/Mul_54_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_48_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_48"](%/Equal_48_output_0, %/ConstantOfShape_48_output_0, %/Constant_268_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_61_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_61"](%/Constant_265_output_0, %/Where_48_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_48_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_48"](%/Expand_61_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_271_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_271"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_49_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_49"](%/Constant_271_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_272_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_272"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_55_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_55"](%/ConstantOfShape_49_output_0, %/Constant_272_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_49_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_49"](%/Constant_268_output_0, %/Mul_55_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_49_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_49"](%/Equal_49_output_0, %/ConstantOfShape_49_output_0, %/Constant_268_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_62_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_62"](%/Constant_266_output_0, %/Where_49_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_49_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_49"](%/Expand_62_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_273_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_273"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_50_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_50"](%/Constant_273_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_274_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_274"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_56_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_56"](%/ConstantOfShape_50_output_0, %/Constant_274_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_50_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_50"](%/Constant_268_output_0, %/Mul_56_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_50_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_50"](%/Equal_50_output_0, %/ConstantOfShape_50_output_0, %/Constant_268_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_63_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_63"](%/Constant_267_output_0, %/Where_50_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_50_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_50"](%/Expand_63_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_275_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_275"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_51_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_51"](%/Constant_275_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_276_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_276"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_57_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_57"](%/ConstantOfShape_51_output_0, %/Constant_276_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_51_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_51"](%/Constant_268_output_0, %/Mul_57_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_51_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_51"](%/Equal_51_output_0, %/ConstantOfShape_51_output_0, %/Constant_268_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_64_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_64"](%/Constant_264_output_0, %/Where_51_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_51_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_51"](%/Expand_64_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_24_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_24"](%/Unsqueeze_48_output_0, %/Unsqueeze_49_output_0, %/Unsqueeze_50_output_0, %/Unsqueeze_51_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_25_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_25"](%/ScatterND_10_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_277_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_277"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_278_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_278"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_279_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_279"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_12_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_12"](%/Shape_25_output_0, %/Constant_278_output_0, %/Constant_279_output_0, %/Constant_277_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_25_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_25"](%/Constant_268_output_0, %/Slice_12_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_25_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_25"](%/Expand_60_output_0, %/Concat_25_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_12_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_12"](%/ScatterND_10_output_0, %/Concat_24_output_0, %/Reshape_25_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_38_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_38"](%/MatMul_35_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_26_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_26"](%/MatMul_38_output_0, %/Constant_15_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_13_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_13"](%/ScatterND_11_output_0, %onnx::Gather_1026), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_26_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_26"](%/Gather_13_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_65_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_65"](%/Reshape_26_output_0, %/Shape_26_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_280_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_280"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_281_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_281"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_282_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_282"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_283_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={38}, onnx_name="/Constant_283"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_284_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_284"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_285_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_285"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_52_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_52"](%/Constant_285_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_286_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_286"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_58_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_58"](%/ConstantOfShape_52_output_0, %/Constant_286_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_52_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_52"](%/Constant_284_output_0, %/Mul_58_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_52_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_52"](%/Equal_52_output_0, %/ConstantOfShape_52_output_0, %/Constant_284_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_66_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_66"](%/Constant_281_output_0, %/Where_52_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_52_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_52"](%/Expand_66_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_287_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_287"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_53_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_53"](%/Constant_287_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_288_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_288"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_59_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_59"](%/ConstantOfShape_53_output_0, %/Constant_288_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_53_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_53"](%/Constant_284_output_0, %/Mul_59_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_53_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_53"](%/Equal_53_output_0, %/ConstantOfShape_53_output_0, %/Constant_284_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_67_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_67"](%/Constant_282_output_0, %/Where_53_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_53_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_53"](%/Expand_67_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_289_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_289"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_54_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_54"](%/Constant_289_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_290_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_290"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_60_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_60"](%/ConstantOfShape_54_output_0, %/Constant_290_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_54_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_54"](%/Constant_284_output_0, %/Mul_60_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_54_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_54"](%/Equal_54_output_0, %/ConstantOfShape_54_output_0, %/Constant_284_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_68_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_68"](%/Constant_283_output_0, %/Where_54_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_54_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_54"](%/Expand_68_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_291_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_291"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_55_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_55"](%/Constant_291_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_292_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_292"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_61_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_61"](%/ConstantOfShape_55_output_0, %/Constant_292_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_55_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_55"](%/Constant_284_output_0, %/Mul_61_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_55_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_55"](%/Equal_55_output_0, %/ConstantOfShape_55_output_0, %/Constant_284_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_69_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_69"](%/Constant_280_output_0, %/Where_55_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_55_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_55"](%/Expand_69_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_26_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_26"](%/Unsqueeze_52_output_0, %/Unsqueeze_53_output_0, %/Unsqueeze_54_output_0, %/Unsqueeze_55_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_27_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_27"](%/ScatterND_11_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_293_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_293"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_294_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_294"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_295_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_295"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_13_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_13"](%/Shape_27_output_0, %/Constant_294_output_0, %/Constant_295_output_0, %/Constant_293_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_27_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_27"](%/Constant_284_output_0, %/Slice_13_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_27_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_27"](%/Expand_65_output_0, %/Concat_27_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_13_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_13"](%/ScatterND_11_output_0, %/Concat_26_output_0, %/Reshape_27_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_12_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_12"](%/MatMul_36_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_39_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_39"](%/ScatterND_12_output_0, %/Transpose_12_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_13_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_13"](%/MatMul_39_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_296_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_296"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_62_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_62"](%/Transpose_13_output_0, %/Constant_296_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_6_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_6"](%/Mul_62_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_40_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_40"](%/Softmax_6_output_0, %/ScatterND_13_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_41_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_41"](%/MatMul_40_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_1152 : Long(device=cpu) = onnx::Constant[value={39}]()
  %/MatMul_42_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_42"](%/MatMul_41_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_43_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_43"](%/MatMul_41_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_28_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_28"](%/MatMul_43_output_0, %/Constant_16_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_14_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_14"](%/ScatterND_12_output_0, %onnx::Gather_1152), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_28_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_28"](%/Gather_14_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_70_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_70"](%/Reshape_28_output_0, %/Shape_28_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_297_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_297"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_298_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_298"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_299_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_299"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_300_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={39}, onnx_name="/Constant_300"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_301_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_301"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_302_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_302"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_56_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_56"](%/Constant_302_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_303_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_303"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_63_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_63"](%/ConstantOfShape_56_output_0, %/Constant_303_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_56_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_56"](%/Constant_301_output_0, %/Mul_63_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_56_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_56"](%/Equal_56_output_0, %/ConstantOfShape_56_output_0, %/Constant_301_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_71_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_71"](%/Constant_298_output_0, %/Where_56_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_56_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_56"](%/Expand_71_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_304_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_304"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_57_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_57"](%/Constant_304_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_305_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_305"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_64_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_64"](%/ConstantOfShape_57_output_0, %/Constant_305_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_57_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_57"](%/Constant_301_output_0, %/Mul_64_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_57_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_57"](%/Equal_57_output_0, %/ConstantOfShape_57_output_0, %/Constant_301_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_72_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_72"](%/Constant_299_output_0, %/Where_57_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_57_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_57"](%/Expand_72_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_306_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_306"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_58_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_58"](%/Constant_306_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_307_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_307"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_65_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_65"](%/ConstantOfShape_58_output_0, %/Constant_307_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_58_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_58"](%/Constant_301_output_0, %/Mul_65_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_58_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_58"](%/Equal_58_output_0, %/ConstantOfShape_58_output_0, %/Constant_301_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_73_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_73"](%/Constant_300_output_0, %/Where_58_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_58_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_58"](%/Expand_73_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_308_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_308"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_59_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_59"](%/Constant_308_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_309_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_309"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_66_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_66"](%/ConstantOfShape_59_output_0, %/Constant_309_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_59_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_59"](%/Constant_301_output_0, %/Mul_66_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_59_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_59"](%/Equal_59_output_0, %/ConstantOfShape_59_output_0, %/Constant_301_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_74_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_74"](%/Constant_297_output_0, %/Where_59_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_59_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_59"](%/Expand_74_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_28_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_28"](%/Unsqueeze_56_output_0, %/Unsqueeze_57_output_0, %/Unsqueeze_58_output_0, %/Unsqueeze_59_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_29_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_29"](%/ScatterND_12_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_310_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_310"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_311_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_311"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_312_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_312"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_14_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_14"](%/Shape_29_output_0, %/Constant_311_output_0, %/Constant_312_output_0, %/Constant_310_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_29_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_29"](%/Constant_301_output_0, %/Slice_14_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_29_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_29"](%/Expand_70_output_0, %/Concat_29_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_14_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_14"](%/ScatterND_12_output_0, %/Concat_28_output_0, %/Reshape_29_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_44_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_44"](%/MatMul_41_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_30_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_30"](%/MatMul_44_output_0, %/Constant_17_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_15_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_15"](%/ScatterND_13_output_0, %onnx::Gather_1152), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_30_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_30"](%/Gather_15_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_75_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_75"](%/Reshape_30_output_0, %/Shape_30_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_313_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_313"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_314_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_314"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_315_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_315"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_316_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={39}, onnx_name="/Constant_316"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_317_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_317"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_318_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_318"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_60_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_60"](%/Constant_318_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_319_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_319"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_67_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_67"](%/ConstantOfShape_60_output_0, %/Constant_319_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_60_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_60"](%/Constant_317_output_0, %/Mul_67_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_60_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_60"](%/Equal_60_output_0, %/ConstantOfShape_60_output_0, %/Constant_317_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_76_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_76"](%/Constant_314_output_0, %/Where_60_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_60_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_60"](%/Expand_76_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_320_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_320"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_61_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_61"](%/Constant_320_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_321_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_321"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_68_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_68"](%/ConstantOfShape_61_output_0, %/Constant_321_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_61_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_61"](%/Constant_317_output_0, %/Mul_68_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_61_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_61"](%/Equal_61_output_0, %/ConstantOfShape_61_output_0, %/Constant_317_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_77_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_77"](%/Constant_315_output_0, %/Where_61_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_61_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_61"](%/Expand_77_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_322_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_322"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_62_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_62"](%/Constant_322_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_323_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_323"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_69_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_69"](%/ConstantOfShape_62_output_0, %/Constant_323_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_62_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_62"](%/Constant_317_output_0, %/Mul_69_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_62_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_62"](%/Equal_62_output_0, %/ConstantOfShape_62_output_0, %/Constant_317_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_78_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_78"](%/Constant_316_output_0, %/Where_62_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_62_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_62"](%/Expand_78_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_324_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_324"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_63_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_63"](%/Constant_324_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_325_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_325"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_70_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_70"](%/ConstantOfShape_63_output_0, %/Constant_325_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_63_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_63"](%/Constant_317_output_0, %/Mul_70_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_63_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_63"](%/Equal_63_output_0, %/ConstantOfShape_63_output_0, %/Constant_317_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_79_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_79"](%/Constant_313_output_0, %/Where_63_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_63_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_63"](%/Expand_79_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_30_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_30"](%/Unsqueeze_60_output_0, %/Unsqueeze_61_output_0, %/Unsqueeze_62_output_0, %/Unsqueeze_63_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_31_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_31"](%/ScatterND_13_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_326_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_326"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_327_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_327"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_328_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_328"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_15_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_15"](%/Shape_31_output_0, %/Constant_327_output_0, %/Constant_328_output_0, %/Constant_326_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_31_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_31"](%/Constant_317_output_0, %/Slice_15_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_31_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_31"](%/Expand_75_output_0, %/Concat_31_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_15_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_15"](%/ScatterND_13_output_0, %/Concat_30_output_0, %/Reshape_31_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_14_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_14"](%/MatMul_42_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_45_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_45"](%/ScatterND_14_output_0, %/Transpose_14_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_15_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_15"](%/MatMul_45_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_329_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_329"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_71_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_71"](%/Transpose_15_output_0, %/Constant_329_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_7_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_7"](%/Mul_71_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_46_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_46"](%/Softmax_7_output_0, %/ScatterND_15_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_47_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_47"](%/MatMul_46_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_1278 : Long(device=cpu) = onnx::Constant[value={40}]()
  %/MatMul_48_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_48"](%/MatMul_47_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_49_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_49"](%/MatMul_47_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_32_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_32"](%/MatMul_49_output_0, %/Constant_18_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_16_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_16"](%/ScatterND_14_output_0, %onnx::Gather_1278), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_32_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_32"](%/Gather_16_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_80_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_80"](%/Reshape_32_output_0, %/Shape_32_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_330_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_330"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_331_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_331"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_332_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_332"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_333_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={40}, onnx_name="/Constant_333"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_334_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_334"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_335_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_335"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_64_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_64"](%/Constant_335_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_336_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_336"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_72_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_72"](%/ConstantOfShape_64_output_0, %/Constant_336_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_64_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_64"](%/Constant_334_output_0, %/Mul_72_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_64_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_64"](%/Equal_64_output_0, %/ConstantOfShape_64_output_0, %/Constant_334_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_81_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_81"](%/Constant_331_output_0, %/Where_64_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_64_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_64"](%/Expand_81_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_337_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_337"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_65_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_65"](%/Constant_337_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_338_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_338"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_73_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_73"](%/ConstantOfShape_65_output_0, %/Constant_338_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_65_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_65"](%/Constant_334_output_0, %/Mul_73_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_65_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_65"](%/Equal_65_output_0, %/ConstantOfShape_65_output_0, %/Constant_334_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_82_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_82"](%/Constant_332_output_0, %/Where_65_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_65_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_65"](%/Expand_82_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_339_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_339"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_66_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_66"](%/Constant_339_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_340_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_340"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_74_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_74"](%/ConstantOfShape_66_output_0, %/Constant_340_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_66_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_66"](%/Constant_334_output_0, %/Mul_74_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_66_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_66"](%/Equal_66_output_0, %/ConstantOfShape_66_output_0, %/Constant_334_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_83_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_83"](%/Constant_333_output_0, %/Where_66_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_66_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_66"](%/Expand_83_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_341_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_341"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_67_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_67"](%/Constant_341_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_342_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_342"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_75_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_75"](%/ConstantOfShape_67_output_0, %/Constant_342_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_67_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_67"](%/Constant_334_output_0, %/Mul_75_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_67_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_67"](%/Equal_67_output_0, %/ConstantOfShape_67_output_0, %/Constant_334_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_84_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_84"](%/Constant_330_output_0, %/Where_67_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_67_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_67"](%/Expand_84_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_32_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_32"](%/Unsqueeze_64_output_0, %/Unsqueeze_65_output_0, %/Unsqueeze_66_output_0, %/Unsqueeze_67_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_33_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_33"](%/ScatterND_14_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_343_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_343"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_344_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_344"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_345_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_345"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_16_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_16"](%/Shape_33_output_0, %/Constant_344_output_0, %/Constant_345_output_0, %/Constant_343_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_33_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_33"](%/Constant_334_output_0, %/Slice_16_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_33_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_33"](%/Expand_80_output_0, %/Concat_33_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_16_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_16"](%/ScatterND_14_output_0, %/Concat_32_output_0, %/Reshape_33_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_50_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_50"](%/MatMul_47_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_34_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_34"](%/MatMul_50_output_0, %/Constant_19_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_17_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_17"](%/ScatterND_15_output_0, %onnx::Gather_1278), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_34_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_34"](%/Gather_17_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_85_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_85"](%/Reshape_34_output_0, %/Shape_34_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_346_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_346"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_347_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_347"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_348_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_348"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_349_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={40}, onnx_name="/Constant_349"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_350_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_350"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_351_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_351"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_68_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_68"](%/Constant_351_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_352_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_352"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_76_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_76"](%/ConstantOfShape_68_output_0, %/Constant_352_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_68_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_68"](%/Constant_350_output_0, %/Mul_76_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_68_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_68"](%/Equal_68_output_0, %/ConstantOfShape_68_output_0, %/Constant_350_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_86_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_86"](%/Constant_347_output_0, %/Where_68_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_68_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_68"](%/Expand_86_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_353_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_353"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_69_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_69"](%/Constant_353_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_354_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_354"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_77_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_77"](%/ConstantOfShape_69_output_0, %/Constant_354_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_69_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_69"](%/Constant_350_output_0, %/Mul_77_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_69_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_69"](%/Equal_69_output_0, %/ConstantOfShape_69_output_0, %/Constant_350_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_87_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_87"](%/Constant_348_output_0, %/Where_69_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_69_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_69"](%/Expand_87_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_355_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_355"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_70_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_70"](%/Constant_355_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_356_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_356"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_78_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_78"](%/ConstantOfShape_70_output_0, %/Constant_356_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_70_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_70"](%/Constant_350_output_0, %/Mul_78_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_70_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_70"](%/Equal_70_output_0, %/ConstantOfShape_70_output_0, %/Constant_350_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_88_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_88"](%/Constant_349_output_0, %/Where_70_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_70_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_70"](%/Expand_88_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_357_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_357"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_71_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_71"](%/Constant_357_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_358_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_358"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_79_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_79"](%/ConstantOfShape_71_output_0, %/Constant_358_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_71_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_71"](%/Constant_350_output_0, %/Mul_79_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_71_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_71"](%/Equal_71_output_0, %/ConstantOfShape_71_output_0, %/Constant_350_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_89_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_89"](%/Constant_346_output_0, %/Where_71_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_71_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_71"](%/Expand_89_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_34_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_34"](%/Unsqueeze_68_output_0, %/Unsqueeze_69_output_0, %/Unsqueeze_70_output_0, %/Unsqueeze_71_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_35_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_35"](%/ScatterND_15_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_359_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_359"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_360_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_360"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_361_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_361"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_17_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_17"](%/Shape_35_output_0, %/Constant_360_output_0, %/Constant_361_output_0, %/Constant_359_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_35_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_35"](%/Constant_350_output_0, %/Slice_17_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_35_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_35"](%/Expand_85_output_0, %/Concat_35_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_17_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_17"](%/ScatterND_15_output_0, %/Concat_34_output_0, %/Reshape_35_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_16_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_16"](%/MatMul_48_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_51_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_51"](%/ScatterND_16_output_0, %/Transpose_16_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_17_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_17"](%/MatMul_51_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_362_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_362"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_80_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_80"](%/Transpose_17_output_0, %/Constant_362_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_8_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_8"](%/Mul_80_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_52_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_52"](%/Softmax_8_output_0, %/ScatterND_17_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_53_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_53"](%/MatMul_52_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_1404 : Long(device=cpu) = onnx::Constant[value={41}]()
  %/MatMul_54_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_54"](%/MatMul_53_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_55_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_55"](%/MatMul_53_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_36_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_36"](%/MatMul_55_output_0, %/Constant_20_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_18_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_18"](%/ScatterND_16_output_0, %onnx::Gather_1404), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_36_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_36"](%/Gather_18_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_90_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_90"](%/Reshape_36_output_0, %/Shape_36_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_363_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_363"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_364_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_364"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_365_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_365"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_366_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={41}, onnx_name="/Constant_366"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_367_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_367"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_368_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_368"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_72_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_72"](%/Constant_368_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_369_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_369"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_81_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_81"](%/ConstantOfShape_72_output_0, %/Constant_369_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_72_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_72"](%/Constant_367_output_0, %/Mul_81_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_72_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_72"](%/Equal_72_output_0, %/ConstantOfShape_72_output_0, %/Constant_367_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_91_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_91"](%/Constant_364_output_0, %/Where_72_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_72_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_72"](%/Expand_91_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_370_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_370"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_73_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_73"](%/Constant_370_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_371_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_371"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_82_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_82"](%/ConstantOfShape_73_output_0, %/Constant_371_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_73_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_73"](%/Constant_367_output_0, %/Mul_82_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_73_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_73"](%/Equal_73_output_0, %/ConstantOfShape_73_output_0, %/Constant_367_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_92_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_92"](%/Constant_365_output_0, %/Where_73_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_73_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_73"](%/Expand_92_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_372_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_372"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_74_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_74"](%/Constant_372_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_373_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_373"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_83_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_83"](%/ConstantOfShape_74_output_0, %/Constant_373_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_74_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_74"](%/Constant_367_output_0, %/Mul_83_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_74_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_74"](%/Equal_74_output_0, %/ConstantOfShape_74_output_0, %/Constant_367_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_93_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_93"](%/Constant_366_output_0, %/Where_74_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_74_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_74"](%/Expand_93_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_374_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_374"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_75_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_75"](%/Constant_374_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_375_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_375"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_84_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_84"](%/ConstantOfShape_75_output_0, %/Constant_375_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_75_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_75"](%/Constant_367_output_0, %/Mul_84_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_75_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_75"](%/Equal_75_output_0, %/ConstantOfShape_75_output_0, %/Constant_367_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_94_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_94"](%/Constant_363_output_0, %/Where_75_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_75_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_75"](%/Expand_94_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_36_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_36"](%/Unsqueeze_72_output_0, %/Unsqueeze_73_output_0, %/Unsqueeze_74_output_0, %/Unsqueeze_75_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_37_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_37"](%/ScatterND_16_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_376_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_376"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_377_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_377"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_378_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_378"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_18_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_18"](%/Shape_37_output_0, %/Constant_377_output_0, %/Constant_378_output_0, %/Constant_376_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_37_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_37"](%/Constant_367_output_0, %/Slice_18_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_37_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_37"](%/Expand_90_output_0, %/Concat_37_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_18_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_18"](%/ScatterND_16_output_0, %/Concat_36_output_0, %/Reshape_37_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_56_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_56"](%/MatMul_53_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_38_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_38"](%/MatMul_56_output_0, %/Constant_21_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_19_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_19"](%/ScatterND_17_output_0, %onnx::Gather_1404), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_38_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_38"](%/Gather_19_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_95_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_95"](%/Reshape_38_output_0, %/Shape_38_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_379_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_379"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_380_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_380"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_381_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_381"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_382_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={41}, onnx_name="/Constant_382"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_383_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_383"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_384_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_384"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_76_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_76"](%/Constant_384_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_385_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_385"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_85_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_85"](%/ConstantOfShape_76_output_0, %/Constant_385_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_76_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_76"](%/Constant_383_output_0, %/Mul_85_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_76_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_76"](%/Equal_76_output_0, %/ConstantOfShape_76_output_0, %/Constant_383_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_96_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_96"](%/Constant_380_output_0, %/Where_76_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_76_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_76"](%/Expand_96_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_386_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_386"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_77_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_77"](%/Constant_386_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_387_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_387"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_86_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_86"](%/ConstantOfShape_77_output_0, %/Constant_387_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_77_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_77"](%/Constant_383_output_0, %/Mul_86_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_77_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_77"](%/Equal_77_output_0, %/ConstantOfShape_77_output_0, %/Constant_383_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_97_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_97"](%/Constant_381_output_0, %/Where_77_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_77_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_77"](%/Expand_97_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_388_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_388"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_78_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_78"](%/Constant_388_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_389_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_389"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_87_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_87"](%/ConstantOfShape_78_output_0, %/Constant_389_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_78_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_78"](%/Constant_383_output_0, %/Mul_87_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_78_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_78"](%/Equal_78_output_0, %/ConstantOfShape_78_output_0, %/Constant_383_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_98_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_98"](%/Constant_382_output_0, %/Where_78_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_78_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_78"](%/Expand_98_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_390_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_390"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_79_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_79"](%/Constant_390_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_391_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_391"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_88_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_88"](%/ConstantOfShape_79_output_0, %/Constant_391_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_79_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_79"](%/Constant_383_output_0, %/Mul_88_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_79_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_79"](%/Equal_79_output_0, %/ConstantOfShape_79_output_0, %/Constant_383_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_99_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_99"](%/Constant_379_output_0, %/Where_79_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_79_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_79"](%/Expand_99_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_38_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_38"](%/Unsqueeze_76_output_0, %/Unsqueeze_77_output_0, %/Unsqueeze_78_output_0, %/Unsqueeze_79_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_39_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_39"](%/ScatterND_17_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_392_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_392"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_393_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_393"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_394_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_394"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_19_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_19"](%/Shape_39_output_0, %/Constant_393_output_0, %/Constant_394_output_0, %/Constant_392_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_39_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_39"](%/Constant_383_output_0, %/Slice_19_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_39_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_39"](%/Expand_95_output_0, %/Concat_39_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_19_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_19"](%/ScatterND_17_output_0, %/Concat_38_output_0, %/Reshape_39_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_18_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_18"](%/MatMul_54_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_57_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_57"](%/ScatterND_18_output_0, %/Transpose_18_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_19_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_19"](%/MatMul_57_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_395_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_395"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_89_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_89"](%/Transpose_19_output_0, %/Constant_395_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_9_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_9"](%/Mul_89_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_58_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_58"](%/Softmax_9_output_0, %/ScatterND_19_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_59_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_59"](%/MatMul_58_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_1530 : Long(device=cpu) = onnx::Constant[value={42}]()
  %/MatMul_60_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_60"](%/MatMul_59_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_61_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_61"](%/MatMul_59_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_40_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_40"](%/MatMul_61_output_0, %/Constant_22_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_20_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_20"](%/ScatterND_18_output_0, %onnx::Gather_1530), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_40_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_40"](%/Gather_20_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_100_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_100"](%/Reshape_40_output_0, %/Shape_40_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_396_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_396"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_397_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_397"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_398_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_398"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_399_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={42}, onnx_name="/Constant_399"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_400_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_400"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_401_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_401"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_80_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_80"](%/Constant_401_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_402_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_402"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_90_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_90"](%/ConstantOfShape_80_output_0, %/Constant_402_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_80_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_80"](%/Constant_400_output_0, %/Mul_90_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_80_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_80"](%/Equal_80_output_0, %/ConstantOfShape_80_output_0, %/Constant_400_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_101_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_101"](%/Constant_397_output_0, %/Where_80_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_80_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_80"](%/Expand_101_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_403_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_403"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_81_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_81"](%/Constant_403_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_404_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_404"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_91_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_91"](%/ConstantOfShape_81_output_0, %/Constant_404_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_81_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_81"](%/Constant_400_output_0, %/Mul_91_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_81_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_81"](%/Equal_81_output_0, %/ConstantOfShape_81_output_0, %/Constant_400_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_102_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_102"](%/Constant_398_output_0, %/Where_81_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_81_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_81"](%/Expand_102_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_405_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_405"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_82_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_82"](%/Constant_405_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_406_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_406"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_92_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_92"](%/ConstantOfShape_82_output_0, %/Constant_406_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_82_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_82"](%/Constant_400_output_0, %/Mul_92_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_82_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_82"](%/Equal_82_output_0, %/ConstantOfShape_82_output_0, %/Constant_400_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_103_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_103"](%/Constant_399_output_0, %/Where_82_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_82_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_82"](%/Expand_103_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_407_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_407"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_83_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_83"](%/Constant_407_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_408_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_408"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_93_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_93"](%/ConstantOfShape_83_output_0, %/Constant_408_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_83_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_83"](%/Constant_400_output_0, %/Mul_93_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_83_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_83"](%/Equal_83_output_0, %/ConstantOfShape_83_output_0, %/Constant_400_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_104_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_104"](%/Constant_396_output_0, %/Where_83_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_83_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_83"](%/Expand_104_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_40_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_40"](%/Unsqueeze_80_output_0, %/Unsqueeze_81_output_0, %/Unsqueeze_82_output_0, %/Unsqueeze_83_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_41_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_41"](%/ScatterND_18_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_409_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_409"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_410_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_410"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_411_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_411"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_20_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_20"](%/Shape_41_output_0, %/Constant_410_output_0, %/Constant_411_output_0, %/Constant_409_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_41_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_41"](%/Constant_400_output_0, %/Slice_20_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_41_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_41"](%/Expand_100_output_0, %/Concat_41_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_20_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_20"](%/ScatterND_18_output_0, %/Concat_40_output_0, %/Reshape_41_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_62_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_62"](%/MatMul_59_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_42_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_42"](%/MatMul_62_output_0, %/Constant_23_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_21_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_21"](%/ScatterND_19_output_0, %onnx::Gather_1530), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_42_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_42"](%/Gather_21_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_105_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_105"](%/Reshape_42_output_0, %/Shape_42_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_412_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_412"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_413_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_413"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_414_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_414"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_415_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={42}, onnx_name="/Constant_415"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_416_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_416"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_417_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_417"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_84_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_84"](%/Constant_417_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_418_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_418"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_94_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_94"](%/ConstantOfShape_84_output_0, %/Constant_418_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_84_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_84"](%/Constant_416_output_0, %/Mul_94_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_84_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_84"](%/Equal_84_output_0, %/ConstantOfShape_84_output_0, %/Constant_416_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_106_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_106"](%/Constant_413_output_0, %/Where_84_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_84_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_84"](%/Expand_106_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_419_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_419"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_85_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_85"](%/Constant_419_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_420_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_420"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_95_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_95"](%/ConstantOfShape_85_output_0, %/Constant_420_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_85_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_85"](%/Constant_416_output_0, %/Mul_95_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_85_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_85"](%/Equal_85_output_0, %/ConstantOfShape_85_output_0, %/Constant_416_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_107_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_107"](%/Constant_414_output_0, %/Where_85_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_85_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_85"](%/Expand_107_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_421_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_421"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_86_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_86"](%/Constant_421_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_422_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_422"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_96_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_96"](%/ConstantOfShape_86_output_0, %/Constant_422_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_86_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_86"](%/Constant_416_output_0, %/Mul_96_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_86_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_86"](%/Equal_86_output_0, %/ConstantOfShape_86_output_0, %/Constant_416_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_108_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_108"](%/Constant_415_output_0, %/Where_86_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_86_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_86"](%/Expand_108_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_423_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_423"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_87_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_87"](%/Constant_423_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_424_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_424"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_97_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_97"](%/ConstantOfShape_87_output_0, %/Constant_424_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_87_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_87"](%/Constant_416_output_0, %/Mul_97_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_87_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_87"](%/Equal_87_output_0, %/ConstantOfShape_87_output_0, %/Constant_416_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_109_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_109"](%/Constant_412_output_0, %/Where_87_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_87_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_87"](%/Expand_109_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_42_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_42"](%/Unsqueeze_84_output_0, %/Unsqueeze_85_output_0, %/Unsqueeze_86_output_0, %/Unsqueeze_87_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_43_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_43"](%/ScatterND_19_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_425_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_425"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_426_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_426"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_427_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_427"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_21_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_21"](%/Shape_43_output_0, %/Constant_426_output_0, %/Constant_427_output_0, %/Constant_425_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_43_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_43"](%/Constant_416_output_0, %/Slice_21_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_43_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_43"](%/Expand_105_output_0, %/Concat_43_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_21_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_21"](%/ScatterND_19_output_0, %/Concat_42_output_0, %/Reshape_43_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_20_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_20"](%/MatMul_60_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_63_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_63"](%/ScatterND_20_output_0, %/Transpose_20_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_21_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_21"](%/MatMul_63_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_428_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_428"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_98_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_98"](%/Transpose_21_output_0, %/Constant_428_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_10_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_10"](%/Mul_98_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_64_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_64"](%/Softmax_10_output_0, %/ScatterND_21_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_65_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_65"](%/MatMul_64_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_1656 : Long(device=cpu) = onnx::Constant[value={43}]()
  %/MatMul_66_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_66"](%/MatMul_65_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_67_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_67"](%/MatMul_65_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_44_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_44"](%/MatMul_67_output_0, %/Constant_24_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_22_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_22"](%/ScatterND_20_output_0, %onnx::Gather_1656), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_44_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_44"](%/Gather_22_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_110_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_110"](%/Reshape_44_output_0, %/Shape_44_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_429_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_429"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_430_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_430"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_431_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_431"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_432_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={43}, onnx_name="/Constant_432"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_433_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_433"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_434_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_434"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_88_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_88"](%/Constant_434_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_435_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_435"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_99_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_99"](%/ConstantOfShape_88_output_0, %/Constant_435_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_88_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_88"](%/Constant_433_output_0, %/Mul_99_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_88_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_88"](%/Equal_88_output_0, %/ConstantOfShape_88_output_0, %/Constant_433_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_111_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_111"](%/Constant_430_output_0, %/Where_88_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_88_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_88"](%/Expand_111_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_436_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_436"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_89_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_89"](%/Constant_436_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_437_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_437"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_100_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_100"](%/ConstantOfShape_89_output_0, %/Constant_437_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_89_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_89"](%/Constant_433_output_0, %/Mul_100_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_89_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_89"](%/Equal_89_output_0, %/ConstantOfShape_89_output_0, %/Constant_433_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_112_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_112"](%/Constant_431_output_0, %/Where_89_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_89_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_89"](%/Expand_112_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_438_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_438"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_90_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_90"](%/Constant_438_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_439_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_439"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_101_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_101"](%/ConstantOfShape_90_output_0, %/Constant_439_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_90_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_90"](%/Constant_433_output_0, %/Mul_101_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_90_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_90"](%/Equal_90_output_0, %/ConstantOfShape_90_output_0, %/Constant_433_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_113_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_113"](%/Constant_432_output_0, %/Where_90_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_90_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_90"](%/Expand_113_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_440_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_440"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_91_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_91"](%/Constant_440_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_441_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_441"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_102_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_102"](%/ConstantOfShape_91_output_0, %/Constant_441_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_91_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_91"](%/Constant_433_output_0, %/Mul_102_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_91_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_91"](%/Equal_91_output_0, %/ConstantOfShape_91_output_0, %/Constant_433_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_114_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_114"](%/Constant_429_output_0, %/Where_91_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_91_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_91"](%/Expand_114_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_44_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_44"](%/Unsqueeze_88_output_0, %/Unsqueeze_89_output_0, %/Unsqueeze_90_output_0, %/Unsqueeze_91_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_45_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_45"](%/ScatterND_20_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_442_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_442"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_443_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_443"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_444_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_444"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_22_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_22"](%/Shape_45_output_0, %/Constant_443_output_0, %/Constant_444_output_0, %/Constant_442_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_45_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_45"](%/Constant_433_output_0, %/Slice_22_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_45_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_45"](%/Expand_110_output_0, %/Concat_45_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_22_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_22"](%/ScatterND_20_output_0, %/Concat_44_output_0, %/Reshape_45_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_68_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_68"](%/MatMul_65_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_46_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_46"](%/MatMul_68_output_0, %/Constant_25_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_23_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_23"](%/ScatterND_21_output_0, %onnx::Gather_1656), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_46_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_46"](%/Gather_23_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_115_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_115"](%/Reshape_46_output_0, %/Shape_46_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_445_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_445"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_446_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_446"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_447_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_447"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_448_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={43}, onnx_name="/Constant_448"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_449_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_449"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_450_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_450"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_92_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_92"](%/Constant_450_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_451_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_451"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_103_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_103"](%/ConstantOfShape_92_output_0, %/Constant_451_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_92_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_92"](%/Constant_449_output_0, %/Mul_103_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_92_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_92"](%/Equal_92_output_0, %/ConstantOfShape_92_output_0, %/Constant_449_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_116_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_116"](%/Constant_446_output_0, %/Where_92_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_92_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_92"](%/Expand_116_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_452_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_452"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_93_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_93"](%/Constant_452_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_453_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_453"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_104_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_104"](%/ConstantOfShape_93_output_0, %/Constant_453_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_93_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_93"](%/Constant_449_output_0, %/Mul_104_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_93_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_93"](%/Equal_93_output_0, %/ConstantOfShape_93_output_0, %/Constant_449_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_117_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_117"](%/Constant_447_output_0, %/Where_93_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_93_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_93"](%/Expand_117_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_454_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_454"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_94_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_94"](%/Constant_454_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_455_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_455"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_105_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_105"](%/ConstantOfShape_94_output_0, %/Constant_455_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_94_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_94"](%/Constant_449_output_0, %/Mul_105_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_94_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_94"](%/Equal_94_output_0, %/ConstantOfShape_94_output_0, %/Constant_449_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_118_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_118"](%/Constant_448_output_0, %/Where_94_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_94_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_94"](%/Expand_118_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_456_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_456"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_95_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_95"](%/Constant_456_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_457_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_457"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_106_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_106"](%/ConstantOfShape_95_output_0, %/Constant_457_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_95_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_95"](%/Constant_449_output_0, %/Mul_106_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_95_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_95"](%/Equal_95_output_0, %/ConstantOfShape_95_output_0, %/Constant_449_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_119_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_119"](%/Constant_445_output_0, %/Where_95_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_95_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_95"](%/Expand_119_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_46_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_46"](%/Unsqueeze_92_output_0, %/Unsqueeze_93_output_0, %/Unsqueeze_94_output_0, %/Unsqueeze_95_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_47_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_47"](%/ScatterND_21_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_458_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_458"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_459_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_459"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_460_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_460"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_23_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_23"](%/Shape_47_output_0, %/Constant_459_output_0, %/Constant_460_output_0, %/Constant_458_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_47_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_47"](%/Constant_449_output_0, %/Slice_23_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_47_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_47"](%/Expand_115_output_0, %/Concat_47_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_23_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_23"](%/ScatterND_21_output_0, %/Concat_46_output_0, %/Reshape_47_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_22_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_22"](%/MatMul_66_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_69_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_69"](%/ScatterND_22_output_0, %/Transpose_22_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_23_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_23"](%/MatMul_69_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_461_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_461"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_107_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_107"](%/Transpose_23_output_0, %/Constant_461_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_11_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_11"](%/Mul_107_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_70_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_70"](%/Softmax_11_output_0, %/ScatterND_23_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_71_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_71"](%/MatMul_70_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_1782 : Long(device=cpu) = onnx::Constant[value={44}]()
  %/MatMul_72_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_72"](%/MatMul_71_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_73_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_73"](%/MatMul_71_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_48_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_48"](%/MatMul_73_output_0, %/Constant_26_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_24_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_24"](%/ScatterND_22_output_0, %onnx::Gather_1782), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_48_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_48"](%/Gather_24_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_120_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_120"](%/Reshape_48_output_0, %/Shape_48_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_462_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_462"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_463_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_463"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_464_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_464"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_465_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={44}, onnx_name="/Constant_465"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_466_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_466"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_467_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_467"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_96_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_96"](%/Constant_467_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_468_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_468"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_108_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_108"](%/ConstantOfShape_96_output_0, %/Constant_468_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_96_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_96"](%/Constant_466_output_0, %/Mul_108_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_96_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_96"](%/Equal_96_output_0, %/ConstantOfShape_96_output_0, %/Constant_466_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_121_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_121"](%/Constant_463_output_0, %/Where_96_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_96_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_96"](%/Expand_121_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_469_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_469"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_97_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_97"](%/Constant_469_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_470_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_470"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_109_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_109"](%/ConstantOfShape_97_output_0, %/Constant_470_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_97_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_97"](%/Constant_466_output_0, %/Mul_109_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_97_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_97"](%/Equal_97_output_0, %/ConstantOfShape_97_output_0, %/Constant_466_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_122_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_122"](%/Constant_464_output_0, %/Where_97_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_97_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_97"](%/Expand_122_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_471_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_471"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_98_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_98"](%/Constant_471_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_472_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_472"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_110_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_110"](%/ConstantOfShape_98_output_0, %/Constant_472_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_98_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_98"](%/Constant_466_output_0, %/Mul_110_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_98_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_98"](%/Equal_98_output_0, %/ConstantOfShape_98_output_0, %/Constant_466_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_123_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_123"](%/Constant_465_output_0, %/Where_98_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_98_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_98"](%/Expand_123_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_473_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_473"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_99_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_99"](%/Constant_473_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_474_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_474"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_111_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_111"](%/ConstantOfShape_99_output_0, %/Constant_474_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_99_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_99"](%/Constant_466_output_0, %/Mul_111_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_99_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_99"](%/Equal_99_output_0, %/ConstantOfShape_99_output_0, %/Constant_466_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_124_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_124"](%/Constant_462_output_0, %/Where_99_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_99_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_99"](%/Expand_124_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_48_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_48"](%/Unsqueeze_96_output_0, %/Unsqueeze_97_output_0, %/Unsqueeze_98_output_0, %/Unsqueeze_99_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_49_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_49"](%/ScatterND_22_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_475_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_475"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_476_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_476"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_477_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_477"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_24_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_24"](%/Shape_49_output_0, %/Constant_476_output_0, %/Constant_477_output_0, %/Constant_475_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_49_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_49"](%/Constant_466_output_0, %/Slice_24_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_49_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_49"](%/Expand_120_output_0, %/Concat_49_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_24_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_24"](%/ScatterND_22_output_0, %/Concat_48_output_0, %/Reshape_49_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_74_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_74"](%/MatMul_71_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_50_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_50"](%/MatMul_74_output_0, %/Constant_27_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_25_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_25"](%/ScatterND_23_output_0, %onnx::Gather_1782), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_50_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_50"](%/Gather_25_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_125_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_125"](%/Reshape_50_output_0, %/Shape_50_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_478_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_478"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_479_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_479"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_480_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_480"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_481_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={44}, onnx_name="/Constant_481"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_482_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_482"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_483_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_483"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_100_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_100"](%/Constant_483_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_484_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_484"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_112_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_112"](%/ConstantOfShape_100_output_0, %/Constant_484_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_100_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_100"](%/Constant_482_output_0, %/Mul_112_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_100_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_100"](%/Equal_100_output_0, %/ConstantOfShape_100_output_0, %/Constant_482_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_126_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_126"](%/Constant_479_output_0, %/Where_100_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_100_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_100"](%/Expand_126_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_485_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_485"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_101_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_101"](%/Constant_485_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_486_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_486"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_113_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_113"](%/ConstantOfShape_101_output_0, %/Constant_486_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_101_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_101"](%/Constant_482_output_0, %/Mul_113_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_101_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_101"](%/Equal_101_output_0, %/ConstantOfShape_101_output_0, %/Constant_482_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_127_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_127"](%/Constant_480_output_0, %/Where_101_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_101_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_101"](%/Expand_127_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_487_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_487"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_102_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_102"](%/Constant_487_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_488_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_488"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_114_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_114"](%/ConstantOfShape_102_output_0, %/Constant_488_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_102_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_102"](%/Constant_482_output_0, %/Mul_114_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_102_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_102"](%/Equal_102_output_0, %/ConstantOfShape_102_output_0, %/Constant_482_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_128_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_128"](%/Constant_481_output_0, %/Where_102_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_102_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_102"](%/Expand_128_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_489_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_489"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_103_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_103"](%/Constant_489_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_490_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_490"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_115_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_115"](%/ConstantOfShape_103_output_0, %/Constant_490_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_103_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_103"](%/Constant_482_output_0, %/Mul_115_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_103_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_103"](%/Equal_103_output_0, %/ConstantOfShape_103_output_0, %/Constant_482_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_129_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_129"](%/Constant_478_output_0, %/Where_103_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_103_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_103"](%/Expand_129_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_50_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_50"](%/Unsqueeze_100_output_0, %/Unsqueeze_101_output_0, %/Unsqueeze_102_output_0, %/Unsqueeze_103_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_51_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_51"](%/ScatterND_23_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_491_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_491"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_492_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_492"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_493_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_493"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_25_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_25"](%/Shape_51_output_0, %/Constant_492_output_0, %/Constant_493_output_0, %/Constant_491_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_51_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_51"](%/Constant_482_output_0, %/Slice_25_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_51_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_51"](%/Expand_125_output_0, %/Concat_51_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_25_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_25"](%/ScatterND_23_output_0, %/Concat_50_output_0, %/Reshape_51_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_24_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_24"](%/MatMul_72_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_75_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_75"](%/ScatterND_24_output_0, %/Transpose_24_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_25_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_25"](%/MatMul_75_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_494_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_494"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_116_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_116"](%/Transpose_25_output_0, %/Constant_494_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_12_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_12"](%/Mul_116_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_76_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_76"](%/Softmax_12_output_0, %/ScatterND_25_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_77_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_77"](%/MatMul_76_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_1908 : Long(device=cpu) = onnx::Constant[value={45}]()
  %/MatMul_78_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_78"](%/MatMul_77_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_79_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_79"](%/MatMul_77_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_52_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_52"](%/MatMul_79_output_0, %/Constant_28_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_26_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_26"](%/ScatterND_24_output_0, %onnx::Gather_1908), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_52_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_52"](%/Gather_26_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_130_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_130"](%/Reshape_52_output_0, %/Shape_52_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_495_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_495"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_496_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_496"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_497_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_497"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_498_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={45}, onnx_name="/Constant_498"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_499_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_499"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_500_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_500"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_104_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_104"](%/Constant_500_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_501_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_501"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_117_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_117"](%/ConstantOfShape_104_output_0, %/Constant_501_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_104_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_104"](%/Constant_499_output_0, %/Mul_117_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_104_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_104"](%/Equal_104_output_0, %/ConstantOfShape_104_output_0, %/Constant_499_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_131_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_131"](%/Constant_496_output_0, %/Where_104_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_104_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_104"](%/Expand_131_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_502_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_502"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_105_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_105"](%/Constant_502_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_503_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_503"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_118_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_118"](%/ConstantOfShape_105_output_0, %/Constant_503_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_105_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_105"](%/Constant_499_output_0, %/Mul_118_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_105_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_105"](%/Equal_105_output_0, %/ConstantOfShape_105_output_0, %/Constant_499_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_132_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_132"](%/Constant_497_output_0, %/Where_105_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_105_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_105"](%/Expand_132_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_504_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_504"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_106_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_106"](%/Constant_504_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_505_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_505"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_119_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_119"](%/ConstantOfShape_106_output_0, %/Constant_505_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_106_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_106"](%/Constant_499_output_0, %/Mul_119_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_106_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_106"](%/Equal_106_output_0, %/ConstantOfShape_106_output_0, %/Constant_499_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_133_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_133"](%/Constant_498_output_0, %/Where_106_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_106_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_106"](%/Expand_133_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_506_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_506"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_107_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_107"](%/Constant_506_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_507_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_507"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_120_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_120"](%/ConstantOfShape_107_output_0, %/Constant_507_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_107_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_107"](%/Constant_499_output_0, %/Mul_120_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_107_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_107"](%/Equal_107_output_0, %/ConstantOfShape_107_output_0, %/Constant_499_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_134_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_134"](%/Constant_495_output_0, %/Where_107_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_107_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_107"](%/Expand_134_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_52_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_52"](%/Unsqueeze_104_output_0, %/Unsqueeze_105_output_0, %/Unsqueeze_106_output_0, %/Unsqueeze_107_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_53_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_53"](%/ScatterND_24_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_508_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_508"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_509_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_509"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_510_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_510"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_26_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_26"](%/Shape_53_output_0, %/Constant_509_output_0, %/Constant_510_output_0, %/Constant_508_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_53_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_53"](%/Constant_499_output_0, %/Slice_26_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_53_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_53"](%/Expand_130_output_0, %/Concat_53_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_26_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_26"](%/ScatterND_24_output_0, %/Concat_52_output_0, %/Reshape_53_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_80_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_80"](%/MatMul_77_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_54_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_54"](%/MatMul_80_output_0, %/Constant_29_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_27_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_27"](%/ScatterND_25_output_0, %onnx::Gather_1908), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_54_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_54"](%/Gather_27_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_135_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_135"](%/Reshape_54_output_0, %/Shape_54_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_511_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_511"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_512_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_512"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_513_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_513"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_514_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={45}, onnx_name="/Constant_514"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_515_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_515"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_516_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_516"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_108_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_108"](%/Constant_516_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_517_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_517"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_121_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_121"](%/ConstantOfShape_108_output_0, %/Constant_517_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_108_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_108"](%/Constant_515_output_0, %/Mul_121_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_108_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_108"](%/Equal_108_output_0, %/ConstantOfShape_108_output_0, %/Constant_515_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_136_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_136"](%/Constant_512_output_0, %/Where_108_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_108_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_108"](%/Expand_136_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_518_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_518"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_109_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_109"](%/Constant_518_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_519_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_519"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_122_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_122"](%/ConstantOfShape_109_output_0, %/Constant_519_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_109_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_109"](%/Constant_515_output_0, %/Mul_122_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_109_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_109"](%/Equal_109_output_0, %/ConstantOfShape_109_output_0, %/Constant_515_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_137_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_137"](%/Constant_513_output_0, %/Where_109_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_109_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_109"](%/Expand_137_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_520_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_520"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_110_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_110"](%/Constant_520_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_521_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_521"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_123_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_123"](%/ConstantOfShape_110_output_0, %/Constant_521_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_110_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_110"](%/Constant_515_output_0, %/Mul_123_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_110_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_110"](%/Equal_110_output_0, %/ConstantOfShape_110_output_0, %/Constant_515_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_138_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_138"](%/Constant_514_output_0, %/Where_110_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_110_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_110"](%/Expand_138_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_522_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_522"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_111_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_111"](%/Constant_522_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_523_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_523"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_124_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_124"](%/ConstantOfShape_111_output_0, %/Constant_523_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_111_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_111"](%/Constant_515_output_0, %/Mul_124_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_111_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_111"](%/Equal_111_output_0, %/ConstantOfShape_111_output_0, %/Constant_515_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_139_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_139"](%/Constant_511_output_0, %/Where_111_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_111_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_111"](%/Expand_139_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_54_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_54"](%/Unsqueeze_108_output_0, %/Unsqueeze_109_output_0, %/Unsqueeze_110_output_0, %/Unsqueeze_111_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_55_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_55"](%/ScatterND_25_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_524_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_524"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_525_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_525"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_526_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_526"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_27_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_27"](%/Shape_55_output_0, %/Constant_525_output_0, %/Constant_526_output_0, %/Constant_524_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_55_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_55"](%/Constant_515_output_0, %/Slice_27_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_55_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_55"](%/Expand_135_output_0, %/Concat_55_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_27_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_27"](%/ScatterND_25_output_0, %/Concat_54_output_0, %/Reshape_55_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_26_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_26"](%/MatMul_78_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_81_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_81"](%/ScatterND_26_output_0, %/Transpose_26_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_27_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_27"](%/MatMul_81_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_527_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_527"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_125_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_125"](%/Transpose_27_output_0, %/Constant_527_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_13_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_13"](%/Mul_125_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_82_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_82"](%/Softmax_13_output_0, %/ScatterND_27_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_83_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_83"](%/MatMul_82_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_2034 : Long(device=cpu) = onnx::Constant[value={46}]()
  %/MatMul_84_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_84"](%/MatMul_83_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_85_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_85"](%/MatMul_83_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_56_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_56"](%/MatMul_85_output_0, %/Constant_30_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_28_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_28"](%/ScatterND_26_output_0, %onnx::Gather_2034), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_56_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_56"](%/Gather_28_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_140_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_140"](%/Reshape_56_output_0, %/Shape_56_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_528_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_528"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_529_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_529"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_530_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_530"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_531_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={46}, onnx_name="/Constant_531"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_532_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_532"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_533_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_533"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_112_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_112"](%/Constant_533_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_534_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_534"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_126_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_126"](%/ConstantOfShape_112_output_0, %/Constant_534_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_112_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_112"](%/Constant_532_output_0, %/Mul_126_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_112_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_112"](%/Equal_112_output_0, %/ConstantOfShape_112_output_0, %/Constant_532_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_141_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_141"](%/Constant_529_output_0, %/Where_112_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_112_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_112"](%/Expand_141_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_535_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_535"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_113_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_113"](%/Constant_535_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_536_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_536"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_127_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_127"](%/ConstantOfShape_113_output_0, %/Constant_536_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_113_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_113"](%/Constant_532_output_0, %/Mul_127_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_113_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_113"](%/Equal_113_output_0, %/ConstantOfShape_113_output_0, %/Constant_532_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_142_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_142"](%/Constant_530_output_0, %/Where_113_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_113_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_113"](%/Expand_142_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_537_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_537"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_114_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_114"](%/Constant_537_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_538_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_538"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_128_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_128"](%/ConstantOfShape_114_output_0, %/Constant_538_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_114_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_114"](%/Constant_532_output_0, %/Mul_128_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_114_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_114"](%/Equal_114_output_0, %/ConstantOfShape_114_output_0, %/Constant_532_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_143_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_143"](%/Constant_531_output_0, %/Where_114_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_114_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_114"](%/Expand_143_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_539_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_539"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_115_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_115"](%/Constant_539_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_540_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_540"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_129_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_129"](%/ConstantOfShape_115_output_0, %/Constant_540_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_115_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_115"](%/Constant_532_output_0, %/Mul_129_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_115_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_115"](%/Equal_115_output_0, %/ConstantOfShape_115_output_0, %/Constant_532_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_144_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_144"](%/Constant_528_output_0, %/Where_115_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_115_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_115"](%/Expand_144_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_56_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_56"](%/Unsqueeze_112_output_0, %/Unsqueeze_113_output_0, %/Unsqueeze_114_output_0, %/Unsqueeze_115_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_57_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_57"](%/ScatterND_26_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_541_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_541"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_542_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_542"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_543_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_543"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_28_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_28"](%/Shape_57_output_0, %/Constant_542_output_0, %/Constant_543_output_0, %/Constant_541_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_57_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_57"](%/Constant_532_output_0, %/Slice_28_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_57_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_57"](%/Expand_140_output_0, %/Concat_57_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_28_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_28"](%/ScatterND_26_output_0, %/Concat_56_output_0, %/Reshape_57_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_86_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_86"](%/MatMul_83_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_58_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_58"](%/MatMul_86_output_0, %/Constant_31_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_29_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_29"](%/ScatterND_27_output_0, %onnx::Gather_2034), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_58_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_58"](%/Gather_29_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_145_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_145"](%/Reshape_58_output_0, %/Shape_58_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_544_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_544"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_545_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_545"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_546_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_546"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_547_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={46}, onnx_name="/Constant_547"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_548_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_548"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_549_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_549"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_116_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_116"](%/Constant_549_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_550_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_550"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_130_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_130"](%/ConstantOfShape_116_output_0, %/Constant_550_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_116_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_116"](%/Constant_548_output_0, %/Mul_130_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_116_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_116"](%/Equal_116_output_0, %/ConstantOfShape_116_output_0, %/Constant_548_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_146_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_146"](%/Constant_545_output_0, %/Where_116_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_116_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_116"](%/Expand_146_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_551_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_551"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_117_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_117"](%/Constant_551_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_552_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_552"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_131_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_131"](%/ConstantOfShape_117_output_0, %/Constant_552_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_117_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_117"](%/Constant_548_output_0, %/Mul_131_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_117_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_117"](%/Equal_117_output_0, %/ConstantOfShape_117_output_0, %/Constant_548_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_147_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_147"](%/Constant_546_output_0, %/Where_117_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_117_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_117"](%/Expand_147_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_553_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_553"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_118_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_118"](%/Constant_553_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_554_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_554"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_132_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_132"](%/ConstantOfShape_118_output_0, %/Constant_554_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_118_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_118"](%/Constant_548_output_0, %/Mul_132_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_118_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_118"](%/Equal_118_output_0, %/ConstantOfShape_118_output_0, %/Constant_548_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_148_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_148"](%/Constant_547_output_0, %/Where_118_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_118_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_118"](%/Expand_148_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_555_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_555"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_119_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_119"](%/Constant_555_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_556_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_556"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_133_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_133"](%/ConstantOfShape_119_output_0, %/Constant_556_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_119_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_119"](%/Constant_548_output_0, %/Mul_133_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_119_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_119"](%/Equal_119_output_0, %/ConstantOfShape_119_output_0, %/Constant_548_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_149_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_149"](%/Constant_544_output_0, %/Where_119_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_119_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_119"](%/Expand_149_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_58_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_58"](%/Unsqueeze_116_output_0, %/Unsqueeze_117_output_0, %/Unsqueeze_118_output_0, %/Unsqueeze_119_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_59_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_59"](%/ScatterND_27_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_557_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_557"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_558_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_558"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_559_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_559"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_29_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_29"](%/Shape_59_output_0, %/Constant_558_output_0, %/Constant_559_output_0, %/Constant_557_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_59_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_59"](%/Constant_548_output_0, %/Slice_29_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_59_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_59"](%/Expand_145_output_0, %/Concat_59_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_29_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_29"](%/ScatterND_27_output_0, %/Concat_58_output_0, %/Reshape_59_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_28_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_28"](%/MatMul_84_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_87_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_87"](%/ScatterND_28_output_0, %/Transpose_28_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_29_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_29"](%/MatMul_87_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_560_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_560"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_134_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_134"](%/Transpose_29_output_0, %/Constant_560_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_14_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_14"](%/Mul_134_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_88_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_88"](%/Softmax_14_output_0, %/ScatterND_29_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_89_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_89"](%/MatMul_88_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_2160 : Long(device=cpu) = onnx::Constant[value={47}]()
  %/MatMul_90_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_90"](%/MatMul_89_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_91_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_91"](%/MatMul_89_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_60_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_60"](%/MatMul_91_output_0, %/Constant_32_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_30_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_30"](%/ScatterND_28_output_0, %onnx::Gather_2160), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_60_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_60"](%/Gather_30_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_150_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_150"](%/Reshape_60_output_0, %/Shape_60_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_561_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_561"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_562_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_562"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_563_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_563"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_564_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={47}, onnx_name="/Constant_564"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_565_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_565"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_566_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_566"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_120_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_120"](%/Constant_566_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_567_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_567"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_135_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_135"](%/ConstantOfShape_120_output_0, %/Constant_567_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_120_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_120"](%/Constant_565_output_0, %/Mul_135_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_120_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_120"](%/Equal_120_output_0, %/ConstantOfShape_120_output_0, %/Constant_565_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_151_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_151"](%/Constant_562_output_0, %/Where_120_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_120_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_120"](%/Expand_151_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_568_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_568"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_121_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_121"](%/Constant_568_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_569_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_569"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_136_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_136"](%/ConstantOfShape_121_output_0, %/Constant_569_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_121_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_121"](%/Constant_565_output_0, %/Mul_136_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_121_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_121"](%/Equal_121_output_0, %/ConstantOfShape_121_output_0, %/Constant_565_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_152_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_152"](%/Constant_563_output_0, %/Where_121_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_121_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_121"](%/Expand_152_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_570_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_570"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_122_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_122"](%/Constant_570_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_571_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_571"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_137_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_137"](%/ConstantOfShape_122_output_0, %/Constant_571_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_122_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_122"](%/Constant_565_output_0, %/Mul_137_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_122_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_122"](%/Equal_122_output_0, %/ConstantOfShape_122_output_0, %/Constant_565_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_153_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_153"](%/Constant_564_output_0, %/Where_122_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_122_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_122"](%/Expand_153_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_572_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_572"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_123_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_123"](%/Constant_572_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_573_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_573"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_138_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_138"](%/ConstantOfShape_123_output_0, %/Constant_573_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_123_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_123"](%/Constant_565_output_0, %/Mul_138_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_123_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_123"](%/Equal_123_output_0, %/ConstantOfShape_123_output_0, %/Constant_565_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_154_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_154"](%/Constant_561_output_0, %/Where_123_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_123_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_123"](%/Expand_154_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_60_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_60"](%/Unsqueeze_120_output_0, %/Unsqueeze_121_output_0, %/Unsqueeze_122_output_0, %/Unsqueeze_123_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_61_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_61"](%/ScatterND_28_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_574_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_574"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_575_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_575"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_576_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_576"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_30_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_30"](%/Shape_61_output_0, %/Constant_575_output_0, %/Constant_576_output_0, %/Constant_574_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_61_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_61"](%/Constant_565_output_0, %/Slice_30_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_61_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_61"](%/Expand_150_output_0, %/Concat_61_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_30_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_30"](%/ScatterND_28_output_0, %/Concat_60_output_0, %/Reshape_61_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_92_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_92"](%/MatMul_89_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_62_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_62"](%/MatMul_92_output_0, %/Constant_33_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_31_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_31"](%/ScatterND_29_output_0, %onnx::Gather_2160), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_62_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_62"](%/Gather_31_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_155_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_155"](%/Reshape_62_output_0, %/Shape_62_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_577_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_577"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_578_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_578"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_579_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_579"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_580_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={47}, onnx_name="/Constant_580"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_581_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_581"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_582_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_582"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_124_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_124"](%/Constant_582_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_583_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_583"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_139_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_139"](%/ConstantOfShape_124_output_0, %/Constant_583_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_124_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_124"](%/Constant_581_output_0, %/Mul_139_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_124_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_124"](%/Equal_124_output_0, %/ConstantOfShape_124_output_0, %/Constant_581_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_156_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_156"](%/Constant_578_output_0, %/Where_124_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_124_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_124"](%/Expand_156_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_584_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_584"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_125_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_125"](%/Constant_584_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_585_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_585"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_140_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_140"](%/ConstantOfShape_125_output_0, %/Constant_585_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_125_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_125"](%/Constant_581_output_0, %/Mul_140_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_125_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_125"](%/Equal_125_output_0, %/ConstantOfShape_125_output_0, %/Constant_581_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_157_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_157"](%/Constant_579_output_0, %/Where_125_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_125_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_125"](%/Expand_157_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_586_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_586"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_126_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_126"](%/Constant_586_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_587_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_587"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_141_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_141"](%/ConstantOfShape_126_output_0, %/Constant_587_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_126_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_126"](%/Constant_581_output_0, %/Mul_141_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_126_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_126"](%/Equal_126_output_0, %/ConstantOfShape_126_output_0, %/Constant_581_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_158_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_158"](%/Constant_580_output_0, %/Where_126_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_126_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_126"](%/Expand_158_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_588_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_588"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_127_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_127"](%/Constant_588_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_589_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_589"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_142_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_142"](%/ConstantOfShape_127_output_0, %/Constant_589_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_127_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_127"](%/Constant_581_output_0, %/Mul_142_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_127_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_127"](%/Equal_127_output_0, %/ConstantOfShape_127_output_0, %/Constant_581_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_159_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_159"](%/Constant_577_output_0, %/Where_127_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_127_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_127"](%/Expand_159_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_62_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_62"](%/Unsqueeze_124_output_0, %/Unsqueeze_125_output_0, %/Unsqueeze_126_output_0, %/Unsqueeze_127_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_63_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_63"](%/ScatterND_29_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_590_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_590"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_591_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_591"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_592_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_592"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_31_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_31"](%/Shape_63_output_0, %/Constant_591_output_0, %/Constant_592_output_0, %/Constant_590_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_63_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_63"](%/Constant_581_output_0, %/Slice_31_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_63_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_63"](%/Expand_155_output_0, %/Concat_63_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_31_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_31"](%/ScatterND_29_output_0, %/Concat_62_output_0, %/Reshape_63_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_30_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_30"](%/MatMul_90_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_93_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_93"](%/ScatterND_30_output_0, %/Transpose_30_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_31_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_31"](%/MatMul_93_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_593_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_593"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_143_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_143"](%/Transpose_31_output_0, %/Constant_593_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_15_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_15"](%/Mul_143_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_94_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_94"](%/Softmax_15_output_0, %/ScatterND_31_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_95_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_95"](%/MatMul_94_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_2286 : Long(device=cpu) = onnx::Constant[value={48}]()
  %/MatMul_96_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_96"](%/MatMul_95_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_97_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_97"](%/MatMul_95_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_64_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_64"](%/MatMul_97_output_0, %/Constant_34_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_32_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_32"](%/ScatterND_30_output_0, %onnx::Gather_2286), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_64_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_64"](%/Gather_32_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_160_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_160"](%/Reshape_64_output_0, %/Shape_64_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_594_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_594"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_595_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_595"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_596_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_596"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_597_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={48}, onnx_name="/Constant_597"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_598_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_598"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_599_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_599"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_128_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_128"](%/Constant_599_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_600_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_600"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_144_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_144"](%/ConstantOfShape_128_output_0, %/Constant_600_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_128_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_128"](%/Constant_598_output_0, %/Mul_144_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_128_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_128"](%/Equal_128_output_0, %/ConstantOfShape_128_output_0, %/Constant_598_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_161_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_161"](%/Constant_595_output_0, %/Where_128_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_128_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_128"](%/Expand_161_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_601_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_601"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_129_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_129"](%/Constant_601_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_602_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_602"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_145_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_145"](%/ConstantOfShape_129_output_0, %/Constant_602_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_129_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_129"](%/Constant_598_output_0, %/Mul_145_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_129_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_129"](%/Equal_129_output_0, %/ConstantOfShape_129_output_0, %/Constant_598_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_162_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_162"](%/Constant_596_output_0, %/Where_129_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_129_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_129"](%/Expand_162_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_603_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_603"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_130_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_130"](%/Constant_603_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_604_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_604"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_146_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_146"](%/ConstantOfShape_130_output_0, %/Constant_604_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_130_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_130"](%/Constant_598_output_0, %/Mul_146_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_130_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_130"](%/Equal_130_output_0, %/ConstantOfShape_130_output_0, %/Constant_598_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_163_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_163"](%/Constant_597_output_0, %/Where_130_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_130_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_130"](%/Expand_163_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_605_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_605"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_131_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_131"](%/Constant_605_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_606_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_606"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_147_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_147"](%/ConstantOfShape_131_output_0, %/Constant_606_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_131_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_131"](%/Constant_598_output_0, %/Mul_147_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_131_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_131"](%/Equal_131_output_0, %/ConstantOfShape_131_output_0, %/Constant_598_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_164_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_164"](%/Constant_594_output_0, %/Where_131_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_131_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_131"](%/Expand_164_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_64_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_64"](%/Unsqueeze_128_output_0, %/Unsqueeze_129_output_0, %/Unsqueeze_130_output_0, %/Unsqueeze_131_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_65_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_65"](%/ScatterND_30_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_607_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_607"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_608_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_608"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_609_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_609"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_32_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_32"](%/Shape_65_output_0, %/Constant_608_output_0, %/Constant_609_output_0, %/Constant_607_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_65_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_65"](%/Constant_598_output_0, %/Slice_32_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_65_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_65"](%/Expand_160_output_0, %/Concat_65_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_32_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_32"](%/ScatterND_30_output_0, %/Concat_64_output_0, %/Reshape_65_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_98_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_98"](%/MatMul_95_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_66_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_66"](%/MatMul_98_output_0, %/Constant_35_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_33_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_33"](%/ScatterND_31_output_0, %onnx::Gather_2286), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_66_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_66"](%/Gather_33_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_165_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_165"](%/Reshape_66_output_0, %/Shape_66_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_610_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_610"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_611_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_611"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_612_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_612"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_613_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={48}, onnx_name="/Constant_613"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_614_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_614"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_615_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_615"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_132_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_132"](%/Constant_615_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_616_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_616"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_148_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_148"](%/ConstantOfShape_132_output_0, %/Constant_616_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_132_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_132"](%/Constant_614_output_0, %/Mul_148_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_132_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_132"](%/Equal_132_output_0, %/ConstantOfShape_132_output_0, %/Constant_614_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_166_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_166"](%/Constant_611_output_0, %/Where_132_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_132_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_132"](%/Expand_166_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_617_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_617"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_133_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_133"](%/Constant_617_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_618_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_618"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_149_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_149"](%/ConstantOfShape_133_output_0, %/Constant_618_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_133_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_133"](%/Constant_614_output_0, %/Mul_149_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_133_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_133"](%/Equal_133_output_0, %/ConstantOfShape_133_output_0, %/Constant_614_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_167_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_167"](%/Constant_612_output_0, %/Where_133_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_133_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_133"](%/Expand_167_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_619_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_619"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_134_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_134"](%/Constant_619_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_620_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_620"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_150_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_150"](%/ConstantOfShape_134_output_0, %/Constant_620_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_134_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_134"](%/Constant_614_output_0, %/Mul_150_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_134_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_134"](%/Equal_134_output_0, %/ConstantOfShape_134_output_0, %/Constant_614_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_168_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_168"](%/Constant_613_output_0, %/Where_134_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_134_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_134"](%/Expand_168_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_621_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_621"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_135_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_135"](%/Constant_621_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_622_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_622"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_151_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_151"](%/ConstantOfShape_135_output_0, %/Constant_622_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_135_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_135"](%/Constant_614_output_0, %/Mul_151_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_135_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_135"](%/Equal_135_output_0, %/ConstantOfShape_135_output_0, %/Constant_614_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_169_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_169"](%/Constant_610_output_0, %/Where_135_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_135_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_135"](%/Expand_169_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_66_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_66"](%/Unsqueeze_132_output_0, %/Unsqueeze_133_output_0, %/Unsqueeze_134_output_0, %/Unsqueeze_135_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_67_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_67"](%/ScatterND_31_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_623_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_623"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_624_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_624"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_625_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_625"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_33_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_33"](%/Shape_67_output_0, %/Constant_624_output_0, %/Constant_625_output_0, %/Constant_623_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_67_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_67"](%/Constant_614_output_0, %/Slice_33_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_67_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_67"](%/Expand_165_output_0, %/Concat_67_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_33_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_33"](%/ScatterND_31_output_0, %/Concat_66_output_0, %/Reshape_67_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_32_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_32"](%/MatMul_96_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_99_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_99"](%/ScatterND_32_output_0, %/Transpose_32_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_33_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_33"](%/MatMul_99_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_626_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_626"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_152_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_152"](%/Transpose_33_output_0, %/Constant_626_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_16_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_16"](%/Mul_152_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_100_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_100"](%/Softmax_16_output_0, %/ScatterND_33_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_101_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_101"](%/MatMul_100_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_2412 : Long(device=cpu) = onnx::Constant[value={49}]()
  %/MatMul_102_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_102"](%/MatMul_101_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_103_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_103"](%/MatMul_101_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_68_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_68"](%/MatMul_103_output_0, %/Constant_36_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_34_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_34"](%/ScatterND_32_output_0, %onnx::Gather_2412), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_68_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_68"](%/Gather_34_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_170_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_170"](%/Reshape_68_output_0, %/Shape_68_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_627_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_627"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_628_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_628"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_629_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_629"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_630_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={49}, onnx_name="/Constant_630"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_631_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_631"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_632_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_632"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_136_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_136"](%/Constant_632_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_633_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_633"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_153_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_153"](%/ConstantOfShape_136_output_0, %/Constant_633_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_136_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_136"](%/Constant_631_output_0, %/Mul_153_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_136_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_136"](%/Equal_136_output_0, %/ConstantOfShape_136_output_0, %/Constant_631_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_171_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_171"](%/Constant_628_output_0, %/Where_136_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_136_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_136"](%/Expand_171_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_634_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_634"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_137_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_137"](%/Constant_634_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_635_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_635"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_154_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_154"](%/ConstantOfShape_137_output_0, %/Constant_635_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_137_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_137"](%/Constant_631_output_0, %/Mul_154_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_137_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_137"](%/Equal_137_output_0, %/ConstantOfShape_137_output_0, %/Constant_631_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_172_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_172"](%/Constant_629_output_0, %/Where_137_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_137_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_137"](%/Expand_172_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_636_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_636"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_138_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_138"](%/Constant_636_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_637_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_637"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_155_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_155"](%/ConstantOfShape_138_output_0, %/Constant_637_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_138_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_138"](%/Constant_631_output_0, %/Mul_155_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_138_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_138"](%/Equal_138_output_0, %/ConstantOfShape_138_output_0, %/Constant_631_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_173_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_173"](%/Constant_630_output_0, %/Where_138_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_138_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_138"](%/Expand_173_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_638_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_638"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_139_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_139"](%/Constant_638_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_639_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_639"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_156_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_156"](%/ConstantOfShape_139_output_0, %/Constant_639_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_139_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_139"](%/Constant_631_output_0, %/Mul_156_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_139_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_139"](%/Equal_139_output_0, %/ConstantOfShape_139_output_0, %/Constant_631_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_174_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_174"](%/Constant_627_output_0, %/Where_139_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_139_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_139"](%/Expand_174_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_68_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_68"](%/Unsqueeze_136_output_0, %/Unsqueeze_137_output_0, %/Unsqueeze_138_output_0, %/Unsqueeze_139_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_69_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_69"](%/ScatterND_32_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_640_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_640"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_641_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_641"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_642_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_642"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_34_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_34"](%/Shape_69_output_0, %/Constant_641_output_0, %/Constant_642_output_0, %/Constant_640_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_69_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_69"](%/Constant_631_output_0, %/Slice_34_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_69_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_69"](%/Expand_170_output_0, %/Concat_69_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_34_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_34"](%/ScatterND_32_output_0, %/Concat_68_output_0, %/Reshape_69_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_104_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_104"](%/MatMul_101_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_70_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_70"](%/MatMul_104_output_0, %/Constant_37_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_35_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_35"](%/ScatterND_33_output_0, %onnx::Gather_2412), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_70_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_70"](%/Gather_35_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_175_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_175"](%/Reshape_70_output_0, %/Shape_70_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_643_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_643"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_644_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_644"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_645_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_645"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_646_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={49}, onnx_name="/Constant_646"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_647_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_647"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_648_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_648"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_140_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_140"](%/Constant_648_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_649_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_649"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_157_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_157"](%/ConstantOfShape_140_output_0, %/Constant_649_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_140_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_140"](%/Constant_647_output_0, %/Mul_157_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_140_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_140"](%/Equal_140_output_0, %/ConstantOfShape_140_output_0, %/Constant_647_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_176_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_176"](%/Constant_644_output_0, %/Where_140_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_140_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_140"](%/Expand_176_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_650_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_650"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_141_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_141"](%/Constant_650_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_651_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_651"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_158_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_158"](%/ConstantOfShape_141_output_0, %/Constant_651_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_141_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_141"](%/Constant_647_output_0, %/Mul_158_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_141_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_141"](%/Equal_141_output_0, %/ConstantOfShape_141_output_0, %/Constant_647_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_177_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_177"](%/Constant_645_output_0, %/Where_141_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_141_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_141"](%/Expand_177_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_652_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_652"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_142_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_142"](%/Constant_652_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_653_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_653"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_159_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_159"](%/ConstantOfShape_142_output_0, %/Constant_653_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_142_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_142"](%/Constant_647_output_0, %/Mul_159_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_142_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_142"](%/Equal_142_output_0, %/ConstantOfShape_142_output_0, %/Constant_647_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_178_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_178"](%/Constant_646_output_0, %/Where_142_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_142_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_142"](%/Expand_178_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_654_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_654"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_143_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_143"](%/Constant_654_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_655_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_655"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_160_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_160"](%/ConstantOfShape_143_output_0, %/Constant_655_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_143_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_143"](%/Constant_647_output_0, %/Mul_160_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_143_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_143"](%/Equal_143_output_0, %/ConstantOfShape_143_output_0, %/Constant_647_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_179_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_179"](%/Constant_643_output_0, %/Where_143_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_143_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_143"](%/Expand_179_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_70_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_70"](%/Unsqueeze_140_output_0, %/Unsqueeze_141_output_0, %/Unsqueeze_142_output_0, %/Unsqueeze_143_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_71_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_71"](%/ScatterND_33_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_656_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_656"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_657_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_657"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_658_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_658"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_35_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_35"](%/Shape_71_output_0, %/Constant_657_output_0, %/Constant_658_output_0, %/Constant_656_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_71_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_71"](%/Constant_647_output_0, %/Slice_35_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_71_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_71"](%/Expand_175_output_0, %/Concat_71_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_35_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_35"](%/ScatterND_33_output_0, %/Concat_70_output_0, %/Reshape_71_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_34_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_34"](%/MatMul_102_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_105_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_105"](%/ScatterND_34_output_0, %/Transpose_34_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_35_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_35"](%/MatMul_105_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_659_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_659"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_161_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_161"](%/Transpose_35_output_0, %/Constant_659_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_17_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_17"](%/Mul_161_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_106_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_106"](%/Softmax_17_output_0, %/ScatterND_35_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_107_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_107"](%/MatMul_106_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_2538 : Long(device=cpu) = onnx::Constant[value={50}]()
  %/MatMul_108_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_108"](%/MatMul_107_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_109_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_109"](%/MatMul_107_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_72_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_72"](%/MatMul_109_output_0, %/Constant_38_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_36_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_36"](%/ScatterND_34_output_0, %onnx::Gather_2538), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_72_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_72"](%/Gather_36_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_180_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_180"](%/Reshape_72_output_0, %/Shape_72_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_660_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_660"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_661_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_661"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_662_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_662"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_663_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={50}, onnx_name="/Constant_663"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_664_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_664"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_665_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_665"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_144_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_144"](%/Constant_665_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_666_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_666"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_162_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_162"](%/ConstantOfShape_144_output_0, %/Constant_666_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_144_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_144"](%/Constant_664_output_0, %/Mul_162_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_144_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_144"](%/Equal_144_output_0, %/ConstantOfShape_144_output_0, %/Constant_664_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_181_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_181"](%/Constant_661_output_0, %/Where_144_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_144_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_144"](%/Expand_181_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_667_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_667"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_145_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_145"](%/Constant_667_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_668_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_668"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_163_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_163"](%/ConstantOfShape_145_output_0, %/Constant_668_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_145_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_145"](%/Constant_664_output_0, %/Mul_163_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_145_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_145"](%/Equal_145_output_0, %/ConstantOfShape_145_output_0, %/Constant_664_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_182_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_182"](%/Constant_662_output_0, %/Where_145_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_145_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_145"](%/Expand_182_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_669_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_669"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_146_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_146"](%/Constant_669_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_670_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_670"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_164_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_164"](%/ConstantOfShape_146_output_0, %/Constant_670_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_146_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_146"](%/Constant_664_output_0, %/Mul_164_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_146_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_146"](%/Equal_146_output_0, %/ConstantOfShape_146_output_0, %/Constant_664_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_183_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_183"](%/Constant_663_output_0, %/Where_146_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_146_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_146"](%/Expand_183_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_671_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_671"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_147_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_147"](%/Constant_671_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_672_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_672"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_165_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_165"](%/ConstantOfShape_147_output_0, %/Constant_672_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_147_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_147"](%/Constant_664_output_0, %/Mul_165_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_147_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_147"](%/Equal_147_output_0, %/ConstantOfShape_147_output_0, %/Constant_664_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_184_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_184"](%/Constant_660_output_0, %/Where_147_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_147_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_147"](%/Expand_184_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_72_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_72"](%/Unsqueeze_144_output_0, %/Unsqueeze_145_output_0, %/Unsqueeze_146_output_0, %/Unsqueeze_147_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_73_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_73"](%/ScatterND_34_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_673_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_673"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_674_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_674"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_675_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_675"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_36_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_36"](%/Shape_73_output_0, %/Constant_674_output_0, %/Constant_675_output_0, %/Constant_673_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_73_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_73"](%/Constant_664_output_0, %/Slice_36_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_73_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_73"](%/Expand_180_output_0, %/Concat_73_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_36_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_36"](%/ScatterND_34_output_0, %/Concat_72_output_0, %/Reshape_73_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_110_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_110"](%/MatMul_107_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_74_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_74"](%/MatMul_110_output_0, %/Constant_39_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_37_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_37"](%/ScatterND_35_output_0, %onnx::Gather_2538), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_74_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_74"](%/Gather_37_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_185_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_185"](%/Reshape_74_output_0, %/Shape_74_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_676_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_676"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_677_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_677"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_678_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_678"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_679_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={50}, onnx_name="/Constant_679"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_680_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_680"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_681_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_681"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_148_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_148"](%/Constant_681_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_682_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_682"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_166_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_166"](%/ConstantOfShape_148_output_0, %/Constant_682_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_148_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_148"](%/Constant_680_output_0, %/Mul_166_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_148_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_148"](%/Equal_148_output_0, %/ConstantOfShape_148_output_0, %/Constant_680_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_186_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_186"](%/Constant_677_output_0, %/Where_148_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_148_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_148"](%/Expand_186_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_683_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_683"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_149_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_149"](%/Constant_683_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_684_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_684"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_167_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_167"](%/ConstantOfShape_149_output_0, %/Constant_684_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_149_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_149"](%/Constant_680_output_0, %/Mul_167_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_149_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_149"](%/Equal_149_output_0, %/ConstantOfShape_149_output_0, %/Constant_680_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_187_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_187"](%/Constant_678_output_0, %/Where_149_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_149_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_149"](%/Expand_187_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_685_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_685"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_150_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_150"](%/Constant_685_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_686_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_686"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_168_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_168"](%/ConstantOfShape_150_output_0, %/Constant_686_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_150_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_150"](%/Constant_680_output_0, %/Mul_168_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_150_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_150"](%/Equal_150_output_0, %/ConstantOfShape_150_output_0, %/Constant_680_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_188_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_188"](%/Constant_679_output_0, %/Where_150_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_150_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_150"](%/Expand_188_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_687_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_687"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_151_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_151"](%/Constant_687_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_688_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_688"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_169_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_169"](%/ConstantOfShape_151_output_0, %/Constant_688_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_151_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_151"](%/Constant_680_output_0, %/Mul_169_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_151_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_151"](%/Equal_151_output_0, %/ConstantOfShape_151_output_0, %/Constant_680_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_189_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_189"](%/Constant_676_output_0, %/Where_151_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_151_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_151"](%/Expand_189_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_74_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_74"](%/Unsqueeze_148_output_0, %/Unsqueeze_149_output_0, %/Unsqueeze_150_output_0, %/Unsqueeze_151_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_75_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_75"](%/ScatterND_35_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_689_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_689"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_690_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_690"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_691_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_691"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_37_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_37"](%/Shape_75_output_0, %/Constant_690_output_0, %/Constant_691_output_0, %/Constant_689_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_75_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_75"](%/Constant_680_output_0, %/Slice_37_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_75_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_75"](%/Expand_185_output_0, %/Concat_75_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_37_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_37"](%/ScatterND_35_output_0, %/Concat_74_output_0, %/Reshape_75_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_36_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_36"](%/MatMul_108_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_111_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_111"](%/ScatterND_36_output_0, %/Transpose_36_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_37_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_37"](%/MatMul_111_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_692_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_692"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_170_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_170"](%/Transpose_37_output_0, %/Constant_692_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_18_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_18"](%/Mul_170_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_112_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_112"](%/Softmax_18_output_0, %/ScatterND_37_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_113_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_113"](%/MatMul_112_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_2664 : Long(device=cpu) = onnx::Constant[value={51}]()
  %/MatMul_114_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_114"](%/MatMul_113_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_115_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_115"](%/MatMul_113_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_76_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_76"](%/MatMul_115_output_0, %/Constant_40_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_38_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_38"](%/ScatterND_36_output_0, %onnx::Gather_2664), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_76_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_76"](%/Gather_38_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_190_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_190"](%/Reshape_76_output_0, %/Shape_76_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_693_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_693"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_694_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_694"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_695_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_695"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_696_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={51}, onnx_name="/Constant_696"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_697_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_697"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_698_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_698"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_152_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_152"](%/Constant_698_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_699_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_699"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_171_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_171"](%/ConstantOfShape_152_output_0, %/Constant_699_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_152_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_152"](%/Constant_697_output_0, %/Mul_171_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_152_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_152"](%/Equal_152_output_0, %/ConstantOfShape_152_output_0, %/Constant_697_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_191_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_191"](%/Constant_694_output_0, %/Where_152_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_152_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_152"](%/Expand_191_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_700_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_700"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_153_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_153"](%/Constant_700_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_701_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_701"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_172_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_172"](%/ConstantOfShape_153_output_0, %/Constant_701_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_153_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_153"](%/Constant_697_output_0, %/Mul_172_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_153_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_153"](%/Equal_153_output_0, %/ConstantOfShape_153_output_0, %/Constant_697_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_192_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_192"](%/Constant_695_output_0, %/Where_153_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_153_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_153"](%/Expand_192_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_702_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_702"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_154_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_154"](%/Constant_702_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_703_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_703"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_173_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_173"](%/ConstantOfShape_154_output_0, %/Constant_703_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_154_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_154"](%/Constant_697_output_0, %/Mul_173_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_154_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_154"](%/Equal_154_output_0, %/ConstantOfShape_154_output_0, %/Constant_697_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_193_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_193"](%/Constant_696_output_0, %/Where_154_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_154_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_154"](%/Expand_193_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_704_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_704"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_155_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_155"](%/Constant_704_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_705_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_705"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_174_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_174"](%/ConstantOfShape_155_output_0, %/Constant_705_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_155_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_155"](%/Constant_697_output_0, %/Mul_174_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_155_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_155"](%/Equal_155_output_0, %/ConstantOfShape_155_output_0, %/Constant_697_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_194_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_194"](%/Constant_693_output_0, %/Where_155_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_155_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_155"](%/Expand_194_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_76_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_76"](%/Unsqueeze_152_output_0, %/Unsqueeze_153_output_0, %/Unsqueeze_154_output_0, %/Unsqueeze_155_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_77_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_77"](%/ScatterND_36_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_706_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_706"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_707_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_707"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_708_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_708"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_38_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_38"](%/Shape_77_output_0, %/Constant_707_output_0, %/Constant_708_output_0, %/Constant_706_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_77_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_77"](%/Constant_697_output_0, %/Slice_38_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_77_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_77"](%/Expand_190_output_0, %/Concat_77_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_38_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_38"](%/ScatterND_36_output_0, %/Concat_76_output_0, %/Reshape_77_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_116_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_116"](%/MatMul_113_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_78_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_78"](%/MatMul_116_output_0, %/Constant_41_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_39_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_39"](%/ScatterND_37_output_0, %onnx::Gather_2664), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_78_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_78"](%/Gather_39_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_195_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_195"](%/Reshape_78_output_0, %/Shape_78_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_709_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_709"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_710_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_710"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_711_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_711"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_712_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={51}, onnx_name="/Constant_712"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_713_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_713"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_714_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_714"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_156_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_156"](%/Constant_714_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_715_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_715"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_175_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_175"](%/ConstantOfShape_156_output_0, %/Constant_715_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_156_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_156"](%/Constant_713_output_0, %/Mul_175_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_156_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_156"](%/Equal_156_output_0, %/ConstantOfShape_156_output_0, %/Constant_713_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_196_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_196"](%/Constant_710_output_0, %/Where_156_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_156_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_156"](%/Expand_196_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_716_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_716"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_157_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_157"](%/Constant_716_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_717_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_717"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_176_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_176"](%/ConstantOfShape_157_output_0, %/Constant_717_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_157_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_157"](%/Constant_713_output_0, %/Mul_176_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_157_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_157"](%/Equal_157_output_0, %/ConstantOfShape_157_output_0, %/Constant_713_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_197_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_197"](%/Constant_711_output_0, %/Where_157_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_157_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_157"](%/Expand_197_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_718_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_718"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_158_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_158"](%/Constant_718_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_719_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_719"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_177_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_177"](%/ConstantOfShape_158_output_0, %/Constant_719_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_158_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_158"](%/Constant_713_output_0, %/Mul_177_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_158_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_158"](%/Equal_158_output_0, %/ConstantOfShape_158_output_0, %/Constant_713_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_198_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_198"](%/Constant_712_output_0, %/Where_158_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_158_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_158"](%/Expand_198_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_720_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_720"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_159_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_159"](%/Constant_720_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_721_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_721"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_178_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_178"](%/ConstantOfShape_159_output_0, %/Constant_721_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_159_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_159"](%/Constant_713_output_0, %/Mul_178_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_159_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_159"](%/Equal_159_output_0, %/ConstantOfShape_159_output_0, %/Constant_713_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_199_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_199"](%/Constant_709_output_0, %/Where_159_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_159_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_159"](%/Expand_199_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_78_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_78"](%/Unsqueeze_156_output_0, %/Unsqueeze_157_output_0, %/Unsqueeze_158_output_0, %/Unsqueeze_159_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_79_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_79"](%/ScatterND_37_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_722_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_722"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_723_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_723"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_724_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_724"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_39_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_39"](%/Shape_79_output_0, %/Constant_723_output_0, %/Constant_724_output_0, %/Constant_722_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_79_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_79"](%/Constant_713_output_0, %/Slice_39_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_79_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_79"](%/Expand_195_output_0, %/Concat_79_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_39_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_39"](%/ScatterND_37_output_0, %/Concat_78_output_0, %/Reshape_79_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_38_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_38"](%/MatMul_114_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_117_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_117"](%/ScatterND_38_output_0, %/Transpose_38_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_39_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_39"](%/MatMul_117_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_725_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_725"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_179_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_179"](%/Transpose_39_output_0, %/Constant_725_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_19_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_19"](%/Mul_179_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_118_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_118"](%/Softmax_19_output_0, %/ScatterND_39_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_119_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_119"](%/MatMul_118_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_2790 : Long(device=cpu) = onnx::Constant[value={52}]()
  %/MatMul_120_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_120"](%/MatMul_119_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_121_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_121"](%/MatMul_119_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_80_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_80"](%/MatMul_121_output_0, %/Constant_42_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_40_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_40"](%/ScatterND_38_output_0, %onnx::Gather_2790), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_80_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_80"](%/Gather_40_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_200_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_200"](%/Reshape_80_output_0, %/Shape_80_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_726_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_726"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_727_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_727"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_728_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_728"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_729_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={52}, onnx_name="/Constant_729"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_730_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_730"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_731_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_731"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_160_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_160"](%/Constant_731_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_732_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_732"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_180_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_180"](%/ConstantOfShape_160_output_0, %/Constant_732_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_160_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_160"](%/Constant_730_output_0, %/Mul_180_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_160_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_160"](%/Equal_160_output_0, %/ConstantOfShape_160_output_0, %/Constant_730_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_201_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_201"](%/Constant_727_output_0, %/Where_160_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_160_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_160"](%/Expand_201_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_733_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_733"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_161_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_161"](%/Constant_733_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_734_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_734"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_181_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_181"](%/ConstantOfShape_161_output_0, %/Constant_734_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_161_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_161"](%/Constant_730_output_0, %/Mul_181_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_161_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_161"](%/Equal_161_output_0, %/ConstantOfShape_161_output_0, %/Constant_730_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_202_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_202"](%/Constant_728_output_0, %/Where_161_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_161_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_161"](%/Expand_202_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_735_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_735"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_162_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_162"](%/Constant_735_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_736_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_736"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_182_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_182"](%/ConstantOfShape_162_output_0, %/Constant_736_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_162_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_162"](%/Constant_730_output_0, %/Mul_182_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_162_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_162"](%/Equal_162_output_0, %/ConstantOfShape_162_output_0, %/Constant_730_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_203_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_203"](%/Constant_729_output_0, %/Where_162_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_162_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_162"](%/Expand_203_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_737_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_737"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_163_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_163"](%/Constant_737_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_738_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_738"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_183_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_183"](%/ConstantOfShape_163_output_0, %/Constant_738_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_163_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_163"](%/Constant_730_output_0, %/Mul_183_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_163_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_163"](%/Equal_163_output_0, %/ConstantOfShape_163_output_0, %/Constant_730_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_204_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_204"](%/Constant_726_output_0, %/Where_163_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_163_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_163"](%/Expand_204_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_80_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_80"](%/Unsqueeze_160_output_0, %/Unsqueeze_161_output_0, %/Unsqueeze_162_output_0, %/Unsqueeze_163_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_81_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_81"](%/ScatterND_38_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_739_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_739"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_740_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_740"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_741_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_741"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_40_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_40"](%/Shape_81_output_0, %/Constant_740_output_0, %/Constant_741_output_0, %/Constant_739_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_81_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_81"](%/Constant_730_output_0, %/Slice_40_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_81_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_81"](%/Expand_200_output_0, %/Concat_81_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_40_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_40"](%/ScatterND_38_output_0, %/Concat_80_output_0, %/Reshape_81_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_122_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_122"](%/MatMul_119_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_82_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_82"](%/MatMul_122_output_0, %/Constant_43_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_41_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_41"](%/ScatterND_39_output_0, %onnx::Gather_2790), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_82_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_82"](%/Gather_41_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_205_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_205"](%/Reshape_82_output_0, %/Shape_82_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_742_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_742"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_743_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_743"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_744_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_744"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_745_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={52}, onnx_name="/Constant_745"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_746_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_746"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_747_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_747"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_164_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_164"](%/Constant_747_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_748_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_748"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_184_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_184"](%/ConstantOfShape_164_output_0, %/Constant_748_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_164_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_164"](%/Constant_746_output_0, %/Mul_184_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_164_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_164"](%/Equal_164_output_0, %/ConstantOfShape_164_output_0, %/Constant_746_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_206_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_206"](%/Constant_743_output_0, %/Where_164_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_164_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_164"](%/Expand_206_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_749_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_749"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_165_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_165"](%/Constant_749_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_750_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_750"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_185_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_185"](%/ConstantOfShape_165_output_0, %/Constant_750_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_165_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_165"](%/Constant_746_output_0, %/Mul_185_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_165_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_165"](%/Equal_165_output_0, %/ConstantOfShape_165_output_0, %/Constant_746_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_207_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_207"](%/Constant_744_output_0, %/Where_165_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_165_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_165"](%/Expand_207_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_751_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_751"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_166_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_166"](%/Constant_751_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_752_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_752"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_186_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_186"](%/ConstantOfShape_166_output_0, %/Constant_752_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_166_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_166"](%/Constant_746_output_0, %/Mul_186_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_166_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_166"](%/Equal_166_output_0, %/ConstantOfShape_166_output_0, %/Constant_746_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_208_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_208"](%/Constant_745_output_0, %/Where_166_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_166_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_166"](%/Expand_208_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_753_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_753"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_167_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_167"](%/Constant_753_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_754_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_754"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_187_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_187"](%/ConstantOfShape_167_output_0, %/Constant_754_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_167_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_167"](%/Constant_746_output_0, %/Mul_187_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_167_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_167"](%/Equal_167_output_0, %/ConstantOfShape_167_output_0, %/Constant_746_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_209_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_209"](%/Constant_742_output_0, %/Where_167_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_167_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_167"](%/Expand_209_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_82_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_82"](%/Unsqueeze_164_output_0, %/Unsqueeze_165_output_0, %/Unsqueeze_166_output_0, %/Unsqueeze_167_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_83_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_83"](%/ScatterND_39_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_755_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_755"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_756_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_756"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_757_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_757"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_41_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_41"](%/Shape_83_output_0, %/Constant_756_output_0, %/Constant_757_output_0, %/Constant_755_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_83_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_83"](%/Constant_746_output_0, %/Slice_41_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_83_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_83"](%/Expand_205_output_0, %/Concat_83_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_41_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_41"](%/ScatterND_39_output_0, %/Concat_82_output_0, %/Reshape_83_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_40_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_40"](%/MatMul_120_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_123_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_123"](%/ScatterND_40_output_0, %/Transpose_40_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_41_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_41"](%/MatMul_123_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_758_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_758"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_188_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_188"](%/Transpose_41_output_0, %/Constant_758_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_20_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_20"](%/Mul_188_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_124_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_124"](%/Softmax_20_output_0, %/ScatterND_41_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_125_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_125"](%/MatMul_124_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_2916 : Long(device=cpu) = onnx::Constant[value={53}]()
  %/MatMul_126_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_126"](%/MatMul_125_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_127_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_127"](%/MatMul_125_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_84_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_84"](%/MatMul_127_output_0, %/Constant_44_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_42_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_42"](%/ScatterND_40_output_0, %onnx::Gather_2916), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_84_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_84"](%/Gather_42_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_210_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_210"](%/Reshape_84_output_0, %/Shape_84_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_759_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_759"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_760_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_760"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_761_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_761"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_762_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={53}, onnx_name="/Constant_762"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_763_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_763"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_764_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_764"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_168_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_168"](%/Constant_764_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_765_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_765"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_189_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_189"](%/ConstantOfShape_168_output_0, %/Constant_765_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_168_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_168"](%/Constant_763_output_0, %/Mul_189_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_168_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_168"](%/Equal_168_output_0, %/ConstantOfShape_168_output_0, %/Constant_763_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_211_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_211"](%/Constant_760_output_0, %/Where_168_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_168_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_168"](%/Expand_211_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_766_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_766"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_169_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_169"](%/Constant_766_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_767_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_767"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_190_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_190"](%/ConstantOfShape_169_output_0, %/Constant_767_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_169_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_169"](%/Constant_763_output_0, %/Mul_190_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_169_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_169"](%/Equal_169_output_0, %/ConstantOfShape_169_output_0, %/Constant_763_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_212_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_212"](%/Constant_761_output_0, %/Where_169_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_169_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_169"](%/Expand_212_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_768_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_768"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_170_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_170"](%/Constant_768_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_769_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_769"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_191_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_191"](%/ConstantOfShape_170_output_0, %/Constant_769_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_170_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_170"](%/Constant_763_output_0, %/Mul_191_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_170_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_170"](%/Equal_170_output_0, %/ConstantOfShape_170_output_0, %/Constant_763_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_213_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_213"](%/Constant_762_output_0, %/Where_170_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_170_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_170"](%/Expand_213_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_770_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_770"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_171_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_171"](%/Constant_770_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_771_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_771"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_192_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_192"](%/ConstantOfShape_171_output_0, %/Constant_771_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_171_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_171"](%/Constant_763_output_0, %/Mul_192_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_171_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_171"](%/Equal_171_output_0, %/ConstantOfShape_171_output_0, %/Constant_763_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_214_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_214"](%/Constant_759_output_0, %/Where_171_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_171_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_171"](%/Expand_214_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_84_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_84"](%/Unsqueeze_168_output_0, %/Unsqueeze_169_output_0, %/Unsqueeze_170_output_0, %/Unsqueeze_171_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_85_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_85"](%/ScatterND_40_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_772_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_772"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_773_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_773"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_774_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_774"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_42_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_42"](%/Shape_85_output_0, %/Constant_773_output_0, %/Constant_774_output_0, %/Constant_772_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_85_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_85"](%/Constant_763_output_0, %/Slice_42_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_85_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_85"](%/Expand_210_output_0, %/Concat_85_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_42_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_42"](%/ScatterND_40_output_0, %/Concat_84_output_0, %/Reshape_85_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_128_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_128"](%/MatMul_125_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_86_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_86"](%/MatMul_128_output_0, %/Constant_45_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_43_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_43"](%/ScatterND_41_output_0, %onnx::Gather_2916), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_86_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_86"](%/Gather_43_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_215_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_215"](%/Reshape_86_output_0, %/Shape_86_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_775_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_775"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_776_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_776"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_777_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_777"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_778_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={53}, onnx_name="/Constant_778"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_779_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_779"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_780_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_780"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_172_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_172"](%/Constant_780_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_781_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_781"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_193_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_193"](%/ConstantOfShape_172_output_0, %/Constant_781_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_172_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_172"](%/Constant_779_output_0, %/Mul_193_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_172_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_172"](%/Equal_172_output_0, %/ConstantOfShape_172_output_0, %/Constant_779_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_216_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_216"](%/Constant_776_output_0, %/Where_172_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_172_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_172"](%/Expand_216_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_782_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_782"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_173_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_173"](%/Constant_782_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_783_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_783"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_194_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_194"](%/ConstantOfShape_173_output_0, %/Constant_783_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_173_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_173"](%/Constant_779_output_0, %/Mul_194_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_173_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_173"](%/Equal_173_output_0, %/ConstantOfShape_173_output_0, %/Constant_779_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_217_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_217"](%/Constant_777_output_0, %/Where_173_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_173_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_173"](%/Expand_217_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_784_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_784"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_174_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_174"](%/Constant_784_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_785_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_785"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_195_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_195"](%/ConstantOfShape_174_output_0, %/Constant_785_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_174_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_174"](%/Constant_779_output_0, %/Mul_195_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_174_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_174"](%/Equal_174_output_0, %/ConstantOfShape_174_output_0, %/Constant_779_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_218_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_218"](%/Constant_778_output_0, %/Where_174_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_174_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_174"](%/Expand_218_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_786_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_786"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_175_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_175"](%/Constant_786_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_787_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_787"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_196_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_196"](%/ConstantOfShape_175_output_0, %/Constant_787_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_175_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_175"](%/Constant_779_output_0, %/Mul_196_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_175_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_175"](%/Equal_175_output_0, %/ConstantOfShape_175_output_0, %/Constant_779_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_219_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_219"](%/Constant_775_output_0, %/Where_175_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_175_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_175"](%/Expand_219_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_86_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_86"](%/Unsqueeze_172_output_0, %/Unsqueeze_173_output_0, %/Unsqueeze_174_output_0, %/Unsqueeze_175_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_87_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_87"](%/ScatterND_41_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_788_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_788"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_789_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_789"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_790_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_790"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_43_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_43"](%/Shape_87_output_0, %/Constant_789_output_0, %/Constant_790_output_0, %/Constant_788_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_87_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_87"](%/Constant_779_output_0, %/Slice_43_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_87_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_87"](%/Expand_215_output_0, %/Concat_87_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_43_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_43"](%/ScatterND_41_output_0, %/Concat_86_output_0, %/Reshape_87_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_42_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_42"](%/MatMul_126_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_129_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_129"](%/ScatterND_42_output_0, %/Transpose_42_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_43_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_43"](%/MatMul_129_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_791_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_791"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_197_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_197"](%/Transpose_43_output_0, %/Constant_791_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_21_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_21"](%/Mul_197_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_130_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_130"](%/Softmax_21_output_0, %/ScatterND_43_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_131_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_131"](%/MatMul_130_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_3042 : Long(device=cpu) = onnx::Constant[value={54}]()
  %/MatMul_132_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_132"](%/MatMul_131_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_133_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_133"](%/MatMul_131_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_88_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_88"](%/MatMul_133_output_0, %/Constant_46_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_44_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_44"](%/ScatterND_42_output_0, %onnx::Gather_3042), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_88_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_88"](%/Gather_44_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_220_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_220"](%/Reshape_88_output_0, %/Shape_88_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_792_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_792"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_793_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_793"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_794_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_794"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_795_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={54}, onnx_name="/Constant_795"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_796_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_796"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_797_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_797"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_176_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_176"](%/Constant_797_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_798_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_798"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_198_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_198"](%/ConstantOfShape_176_output_0, %/Constant_798_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_176_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_176"](%/Constant_796_output_0, %/Mul_198_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_176_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_176"](%/Equal_176_output_0, %/ConstantOfShape_176_output_0, %/Constant_796_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_221_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_221"](%/Constant_793_output_0, %/Where_176_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_176_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_176"](%/Expand_221_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_799_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_799"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_177_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_177"](%/Constant_799_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_800_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_800"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_199_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_199"](%/ConstantOfShape_177_output_0, %/Constant_800_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_177_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_177"](%/Constant_796_output_0, %/Mul_199_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_177_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_177"](%/Equal_177_output_0, %/ConstantOfShape_177_output_0, %/Constant_796_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_222_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_222"](%/Constant_794_output_0, %/Where_177_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_177_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_177"](%/Expand_222_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_801_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_801"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_178_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_178"](%/Constant_801_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_802_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_802"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_200_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_200"](%/ConstantOfShape_178_output_0, %/Constant_802_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_178_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_178"](%/Constant_796_output_0, %/Mul_200_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_178_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_178"](%/Equal_178_output_0, %/ConstantOfShape_178_output_0, %/Constant_796_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_223_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_223"](%/Constant_795_output_0, %/Where_178_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_178_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_178"](%/Expand_223_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_803_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_803"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_179_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_179"](%/Constant_803_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_804_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_804"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_201_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_201"](%/ConstantOfShape_179_output_0, %/Constant_804_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_179_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_179"](%/Constant_796_output_0, %/Mul_201_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_179_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_179"](%/Equal_179_output_0, %/ConstantOfShape_179_output_0, %/Constant_796_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_224_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_224"](%/Constant_792_output_0, %/Where_179_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_179_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_179"](%/Expand_224_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_88_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_88"](%/Unsqueeze_176_output_0, %/Unsqueeze_177_output_0, %/Unsqueeze_178_output_0, %/Unsqueeze_179_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_89_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_89"](%/ScatterND_42_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_805_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_805"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_806_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_806"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_807_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_807"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_44_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_44"](%/Shape_89_output_0, %/Constant_806_output_0, %/Constant_807_output_0, %/Constant_805_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_89_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_89"](%/Constant_796_output_0, %/Slice_44_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_89_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_89"](%/Expand_220_output_0, %/Concat_89_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_44_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_44"](%/ScatterND_42_output_0, %/Concat_88_output_0, %/Reshape_89_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_134_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_134"](%/MatMul_131_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_90_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_90"](%/MatMul_134_output_0, %/Constant_47_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_45_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_45"](%/ScatterND_43_output_0, %onnx::Gather_3042), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_90_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_90"](%/Gather_45_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_225_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_225"](%/Reshape_90_output_0, %/Shape_90_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_808_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_808"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_809_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_809"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_810_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_810"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_811_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={54}, onnx_name="/Constant_811"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_812_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_812"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_813_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_813"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_180_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_180"](%/Constant_813_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_814_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_814"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_202_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_202"](%/ConstantOfShape_180_output_0, %/Constant_814_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_180_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_180"](%/Constant_812_output_0, %/Mul_202_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_180_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_180"](%/Equal_180_output_0, %/ConstantOfShape_180_output_0, %/Constant_812_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_226_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_226"](%/Constant_809_output_0, %/Where_180_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_180_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_180"](%/Expand_226_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_815_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_815"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_181_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_181"](%/Constant_815_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_816_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_816"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_203_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_203"](%/ConstantOfShape_181_output_0, %/Constant_816_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_181_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_181"](%/Constant_812_output_0, %/Mul_203_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_181_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_181"](%/Equal_181_output_0, %/ConstantOfShape_181_output_0, %/Constant_812_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_227_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_227"](%/Constant_810_output_0, %/Where_181_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_181_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_181"](%/Expand_227_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_817_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_817"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_182_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_182"](%/Constant_817_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_818_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_818"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_204_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_204"](%/ConstantOfShape_182_output_0, %/Constant_818_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_182_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_182"](%/Constant_812_output_0, %/Mul_204_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_182_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_182"](%/Equal_182_output_0, %/ConstantOfShape_182_output_0, %/Constant_812_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_228_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_228"](%/Constant_811_output_0, %/Where_182_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_182_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_182"](%/Expand_228_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_819_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_819"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_183_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_183"](%/Constant_819_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_820_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_820"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_205_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_205"](%/ConstantOfShape_183_output_0, %/Constant_820_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_183_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_183"](%/Constant_812_output_0, %/Mul_205_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_183_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_183"](%/Equal_183_output_0, %/ConstantOfShape_183_output_0, %/Constant_812_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_229_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_229"](%/Constant_808_output_0, %/Where_183_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_183_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_183"](%/Expand_229_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_90_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_90"](%/Unsqueeze_180_output_0, %/Unsqueeze_181_output_0, %/Unsqueeze_182_output_0, %/Unsqueeze_183_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_91_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_91"](%/ScatterND_43_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_821_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_821"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_822_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_822"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_823_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_823"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_45_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_45"](%/Shape_91_output_0, %/Constant_822_output_0, %/Constant_823_output_0, %/Constant_821_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_91_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_91"](%/Constant_812_output_0, %/Slice_45_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_91_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_91"](%/Expand_225_output_0, %/Concat_91_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_45_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_45"](%/ScatterND_43_output_0, %/Concat_90_output_0, %/Reshape_91_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_44_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_44"](%/MatMul_132_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_135_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_135"](%/ScatterND_44_output_0, %/Transpose_44_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_45_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_45"](%/MatMul_135_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_824_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_824"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_206_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_206"](%/Transpose_45_output_0, %/Constant_824_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_22_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_22"](%/Mul_206_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_136_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_136"](%/Softmax_22_output_0, %/ScatterND_45_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_137_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_137"](%/MatMul_136_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_3168 : Long(device=cpu) = onnx::Constant[value={55}]()
  %/MatMul_138_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_138"](%/MatMul_137_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_139_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_139"](%/MatMul_137_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_92_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_92"](%/MatMul_139_output_0, %/Constant_48_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_46_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_46"](%/ScatterND_44_output_0, %onnx::Gather_3168), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_92_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_92"](%/Gather_46_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_230_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_230"](%/Reshape_92_output_0, %/Shape_92_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_825_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_825"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_826_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_826"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_827_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_827"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_828_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={55}, onnx_name="/Constant_828"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_829_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_829"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_830_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_830"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_184_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_184"](%/Constant_830_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_831_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_831"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_207_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_207"](%/ConstantOfShape_184_output_0, %/Constant_831_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_184_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_184"](%/Constant_829_output_0, %/Mul_207_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_184_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_184"](%/Equal_184_output_0, %/ConstantOfShape_184_output_0, %/Constant_829_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_231_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_231"](%/Constant_826_output_0, %/Where_184_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_184_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_184"](%/Expand_231_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_832_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_832"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_185_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_185"](%/Constant_832_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_833_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_833"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_208_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_208"](%/ConstantOfShape_185_output_0, %/Constant_833_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_185_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_185"](%/Constant_829_output_0, %/Mul_208_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_185_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_185"](%/Equal_185_output_0, %/ConstantOfShape_185_output_0, %/Constant_829_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_232_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_232"](%/Constant_827_output_0, %/Where_185_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_185_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_185"](%/Expand_232_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_834_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_834"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_186_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_186"](%/Constant_834_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_835_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_835"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_209_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_209"](%/ConstantOfShape_186_output_0, %/Constant_835_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_186_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_186"](%/Constant_829_output_0, %/Mul_209_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_186_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_186"](%/Equal_186_output_0, %/ConstantOfShape_186_output_0, %/Constant_829_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_233_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_233"](%/Constant_828_output_0, %/Where_186_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_186_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_186"](%/Expand_233_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_836_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_836"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_187_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_187"](%/Constant_836_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_837_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_837"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_210_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_210"](%/ConstantOfShape_187_output_0, %/Constant_837_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_187_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_187"](%/Constant_829_output_0, %/Mul_210_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_187_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_187"](%/Equal_187_output_0, %/ConstantOfShape_187_output_0, %/Constant_829_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_234_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_234"](%/Constant_825_output_0, %/Where_187_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_187_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_187"](%/Expand_234_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_92_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_92"](%/Unsqueeze_184_output_0, %/Unsqueeze_185_output_0, %/Unsqueeze_186_output_0, %/Unsqueeze_187_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_93_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_93"](%/ScatterND_44_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_838_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_838"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_839_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_839"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_840_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_840"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_46_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_46"](%/Shape_93_output_0, %/Constant_839_output_0, %/Constant_840_output_0, %/Constant_838_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_93_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_93"](%/Constant_829_output_0, %/Slice_46_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_93_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_93"](%/Expand_230_output_0, %/Concat_93_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_46_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_46"](%/ScatterND_44_output_0, %/Concat_92_output_0, %/Reshape_93_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_140_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_140"](%/MatMul_137_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_94_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_94"](%/MatMul_140_output_0, %/Constant_49_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_47_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_47"](%/ScatterND_45_output_0, %onnx::Gather_3168), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_94_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_94"](%/Gather_47_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_235_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_235"](%/Reshape_94_output_0, %/Shape_94_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_841_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_841"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_842_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_842"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_843_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_843"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_844_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={55}, onnx_name="/Constant_844"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_845_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_845"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_846_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_846"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_188_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_188"](%/Constant_846_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_847_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_847"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_211_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_211"](%/ConstantOfShape_188_output_0, %/Constant_847_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_188_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_188"](%/Constant_845_output_0, %/Mul_211_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_188_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_188"](%/Equal_188_output_0, %/ConstantOfShape_188_output_0, %/Constant_845_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_236_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_236"](%/Constant_842_output_0, %/Where_188_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_188_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_188"](%/Expand_236_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_848_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_848"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_189_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_189"](%/Constant_848_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_849_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_849"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_212_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_212"](%/ConstantOfShape_189_output_0, %/Constant_849_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_189_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_189"](%/Constant_845_output_0, %/Mul_212_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_189_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_189"](%/Equal_189_output_0, %/ConstantOfShape_189_output_0, %/Constant_845_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_237_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_237"](%/Constant_843_output_0, %/Where_189_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_189_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_189"](%/Expand_237_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_850_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_850"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_190_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_190"](%/Constant_850_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_851_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_851"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_213_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_213"](%/ConstantOfShape_190_output_0, %/Constant_851_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_190_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_190"](%/Constant_845_output_0, %/Mul_213_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_190_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_190"](%/Equal_190_output_0, %/ConstantOfShape_190_output_0, %/Constant_845_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_238_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_238"](%/Constant_844_output_0, %/Where_190_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_190_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_190"](%/Expand_238_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_852_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_852"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_191_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_191"](%/Constant_852_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_853_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_853"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_214_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_214"](%/ConstantOfShape_191_output_0, %/Constant_853_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_191_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_191"](%/Constant_845_output_0, %/Mul_214_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_191_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_191"](%/Equal_191_output_0, %/ConstantOfShape_191_output_0, %/Constant_845_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_239_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_239"](%/Constant_841_output_0, %/Where_191_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_191_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_191"](%/Expand_239_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_94_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_94"](%/Unsqueeze_188_output_0, %/Unsqueeze_189_output_0, %/Unsqueeze_190_output_0, %/Unsqueeze_191_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_95_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_95"](%/ScatterND_45_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_854_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_854"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_855_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_855"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_856_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_856"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_47_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_47"](%/Shape_95_output_0, %/Constant_855_output_0, %/Constant_856_output_0, %/Constant_854_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_95_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_95"](%/Constant_845_output_0, %/Slice_47_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_95_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_95"](%/Expand_235_output_0, %/Concat_95_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_47_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_47"](%/ScatterND_45_output_0, %/Concat_94_output_0, %/Reshape_95_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_46_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_46"](%/MatMul_138_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_141_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_141"](%/ScatterND_46_output_0, %/Transpose_46_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_47_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_47"](%/MatMul_141_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_857_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_857"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_215_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_215"](%/Transpose_47_output_0, %/Constant_857_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_23_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_23"](%/Mul_215_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_142_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_142"](%/Softmax_23_output_0, %/ScatterND_47_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_143_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_143"](%/MatMul_142_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_3294 : Long(device=cpu) = onnx::Constant[value={56}]()
  %/MatMul_144_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_144"](%/MatMul_143_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_145_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_145"](%/MatMul_143_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_96_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_96"](%/MatMul_145_output_0, %/Constant_50_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_48_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_48"](%/ScatterND_46_output_0, %onnx::Gather_3294), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_96_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_96"](%/Gather_48_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_240_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_240"](%/Reshape_96_output_0, %/Shape_96_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_858_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_858"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_859_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_859"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_860_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_860"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_861_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={56}, onnx_name="/Constant_861"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_862_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_862"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_863_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_863"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_192_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_192"](%/Constant_863_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_864_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_864"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_216_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_216"](%/ConstantOfShape_192_output_0, %/Constant_864_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_192_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_192"](%/Constant_862_output_0, %/Mul_216_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_192_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_192"](%/Equal_192_output_0, %/ConstantOfShape_192_output_0, %/Constant_862_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_241_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_241"](%/Constant_859_output_0, %/Where_192_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_192_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_192"](%/Expand_241_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_865_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_865"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_193_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_193"](%/Constant_865_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_866_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_866"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_217_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_217"](%/ConstantOfShape_193_output_0, %/Constant_866_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_193_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_193"](%/Constant_862_output_0, %/Mul_217_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_193_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_193"](%/Equal_193_output_0, %/ConstantOfShape_193_output_0, %/Constant_862_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_242_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_242"](%/Constant_860_output_0, %/Where_193_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_193_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_193"](%/Expand_242_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_867_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_867"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_194_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_194"](%/Constant_867_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_868_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_868"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_218_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_218"](%/ConstantOfShape_194_output_0, %/Constant_868_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_194_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_194"](%/Constant_862_output_0, %/Mul_218_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_194_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_194"](%/Equal_194_output_0, %/ConstantOfShape_194_output_0, %/Constant_862_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_243_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_243"](%/Constant_861_output_0, %/Where_194_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_194_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_194"](%/Expand_243_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_869_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_869"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_195_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_195"](%/Constant_869_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_870_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_870"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_219_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_219"](%/ConstantOfShape_195_output_0, %/Constant_870_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_195_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_195"](%/Constant_862_output_0, %/Mul_219_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_195_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_195"](%/Equal_195_output_0, %/ConstantOfShape_195_output_0, %/Constant_862_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_244_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_244"](%/Constant_858_output_0, %/Where_195_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_195_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_195"](%/Expand_244_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_96_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_96"](%/Unsqueeze_192_output_0, %/Unsqueeze_193_output_0, %/Unsqueeze_194_output_0, %/Unsqueeze_195_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_97_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_97"](%/ScatterND_46_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_871_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_871"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_872_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_872"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_873_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_873"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_48_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_48"](%/Shape_97_output_0, %/Constant_872_output_0, %/Constant_873_output_0, %/Constant_871_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_97_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_97"](%/Constant_862_output_0, %/Slice_48_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_97_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_97"](%/Expand_240_output_0, %/Concat_97_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_48_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_48"](%/ScatterND_46_output_0, %/Concat_96_output_0, %/Reshape_97_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_146_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_146"](%/MatMul_143_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_98_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_98"](%/MatMul_146_output_0, %/Constant_51_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_49_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_49"](%/ScatterND_47_output_0, %onnx::Gather_3294), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_98_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_98"](%/Gather_49_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_245_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_245"](%/Reshape_98_output_0, %/Shape_98_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_874_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_874"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_875_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_875"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_876_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_876"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_877_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={56}, onnx_name="/Constant_877"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_878_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_878"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_879_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_879"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_196_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_196"](%/Constant_879_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_880_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_880"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_220_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_220"](%/ConstantOfShape_196_output_0, %/Constant_880_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_196_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_196"](%/Constant_878_output_0, %/Mul_220_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_196_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_196"](%/Equal_196_output_0, %/ConstantOfShape_196_output_0, %/Constant_878_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_246_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_246"](%/Constant_875_output_0, %/Where_196_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_196_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_196"](%/Expand_246_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_881_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_881"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_197_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_197"](%/Constant_881_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_882_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_882"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_221_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_221"](%/ConstantOfShape_197_output_0, %/Constant_882_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_197_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_197"](%/Constant_878_output_0, %/Mul_221_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_197_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_197"](%/Equal_197_output_0, %/ConstantOfShape_197_output_0, %/Constant_878_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_247_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_247"](%/Constant_876_output_0, %/Where_197_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_197_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_197"](%/Expand_247_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_883_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_883"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_198_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_198"](%/Constant_883_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_884_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_884"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_222_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_222"](%/ConstantOfShape_198_output_0, %/Constant_884_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_198_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_198"](%/Constant_878_output_0, %/Mul_222_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_198_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_198"](%/Equal_198_output_0, %/ConstantOfShape_198_output_0, %/Constant_878_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_248_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_248"](%/Constant_877_output_0, %/Where_198_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_198_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_198"](%/Expand_248_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_885_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_885"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_199_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_199"](%/Constant_885_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_886_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_886"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_223_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_223"](%/ConstantOfShape_199_output_0, %/Constant_886_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_199_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_199"](%/Constant_878_output_0, %/Mul_223_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_199_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_199"](%/Equal_199_output_0, %/ConstantOfShape_199_output_0, %/Constant_878_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_249_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_249"](%/Constant_874_output_0, %/Where_199_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_199_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_199"](%/Expand_249_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_98_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_98"](%/Unsqueeze_196_output_0, %/Unsqueeze_197_output_0, %/Unsqueeze_198_output_0, %/Unsqueeze_199_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_99_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_99"](%/ScatterND_47_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_887_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_887"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_888_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_888"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_889_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_889"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_49_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_49"](%/Shape_99_output_0, %/Constant_888_output_0, %/Constant_889_output_0, %/Constant_887_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_99_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_99"](%/Constant_878_output_0, %/Slice_49_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_99_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_99"](%/Expand_245_output_0, %/Concat_99_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_49_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_49"](%/ScatterND_47_output_0, %/Concat_98_output_0, %/Reshape_99_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_48_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_48"](%/MatMul_144_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_147_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_147"](%/ScatterND_48_output_0, %/Transpose_48_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_49_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_49"](%/MatMul_147_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_890_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_890"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_224_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_224"](%/Transpose_49_output_0, %/Constant_890_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_24_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_24"](%/Mul_224_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_148_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_148"](%/Softmax_24_output_0, %/ScatterND_49_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_149_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_149"](%/MatMul_148_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_3420 : Long(device=cpu) = onnx::Constant[value={57}]()
  %/MatMul_150_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_150"](%/MatMul_149_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_151_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_151"](%/MatMul_149_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_100_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_100"](%/MatMul_151_output_0, %/Constant_52_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_50_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_50"](%/ScatterND_48_output_0, %onnx::Gather_3420), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_100_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_100"](%/Gather_50_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_250_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_250"](%/Reshape_100_output_0, %/Shape_100_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_891_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_891"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_892_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_892"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_893_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_893"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_894_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={57}, onnx_name="/Constant_894"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_895_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_895"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_896_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_896"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_200_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_200"](%/Constant_896_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_897_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_897"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_225_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_225"](%/ConstantOfShape_200_output_0, %/Constant_897_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_200_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_200"](%/Constant_895_output_0, %/Mul_225_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_200_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_200"](%/Equal_200_output_0, %/ConstantOfShape_200_output_0, %/Constant_895_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_251_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_251"](%/Constant_892_output_0, %/Where_200_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_200_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_200"](%/Expand_251_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_898_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_898"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_201_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_201"](%/Constant_898_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_899_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_899"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_226_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_226"](%/ConstantOfShape_201_output_0, %/Constant_899_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_201_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_201"](%/Constant_895_output_0, %/Mul_226_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_201_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_201"](%/Equal_201_output_0, %/ConstantOfShape_201_output_0, %/Constant_895_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_252_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_252"](%/Constant_893_output_0, %/Where_201_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_201_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_201"](%/Expand_252_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_900_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_900"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_202_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_202"](%/Constant_900_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_901_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_901"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_227_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_227"](%/ConstantOfShape_202_output_0, %/Constant_901_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_202_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_202"](%/Constant_895_output_0, %/Mul_227_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_202_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_202"](%/Equal_202_output_0, %/ConstantOfShape_202_output_0, %/Constant_895_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_253_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_253"](%/Constant_894_output_0, %/Where_202_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_202_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_202"](%/Expand_253_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_902_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_902"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_203_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_203"](%/Constant_902_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_903_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_903"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_228_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_228"](%/ConstantOfShape_203_output_0, %/Constant_903_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_203_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_203"](%/Constant_895_output_0, %/Mul_228_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_203_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_203"](%/Equal_203_output_0, %/ConstantOfShape_203_output_0, %/Constant_895_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_254_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_254"](%/Constant_891_output_0, %/Where_203_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_203_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_203"](%/Expand_254_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_100_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_100"](%/Unsqueeze_200_output_0, %/Unsqueeze_201_output_0, %/Unsqueeze_202_output_0, %/Unsqueeze_203_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_101_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_101"](%/ScatterND_48_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_904_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_904"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_905_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_905"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_906_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_906"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_50_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_50"](%/Shape_101_output_0, %/Constant_905_output_0, %/Constant_906_output_0, %/Constant_904_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_101_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_101"](%/Constant_895_output_0, %/Slice_50_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_101_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_101"](%/Expand_250_output_0, %/Concat_101_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_50_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_50"](%/ScatterND_48_output_0, %/Concat_100_output_0, %/Reshape_101_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_152_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_152"](%/MatMul_149_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_102_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_102"](%/MatMul_152_output_0, %/Constant_53_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_51_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_51"](%/ScatterND_49_output_0, %onnx::Gather_3420), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_102_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_102"](%/Gather_51_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_255_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_255"](%/Reshape_102_output_0, %/Shape_102_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_907_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_907"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_908_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_908"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_909_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_909"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_910_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={57}, onnx_name="/Constant_910"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_911_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_911"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_912_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_912"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_204_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_204"](%/Constant_912_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_913_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_913"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_229_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_229"](%/ConstantOfShape_204_output_0, %/Constant_913_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_204_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_204"](%/Constant_911_output_0, %/Mul_229_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_204_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_204"](%/Equal_204_output_0, %/ConstantOfShape_204_output_0, %/Constant_911_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_256_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_256"](%/Constant_908_output_0, %/Where_204_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_204_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_204"](%/Expand_256_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_914_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_914"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_205_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_205"](%/Constant_914_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_915_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_915"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_230_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_230"](%/ConstantOfShape_205_output_0, %/Constant_915_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_205_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_205"](%/Constant_911_output_0, %/Mul_230_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_205_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_205"](%/Equal_205_output_0, %/ConstantOfShape_205_output_0, %/Constant_911_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_257_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_257"](%/Constant_909_output_0, %/Where_205_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_205_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_205"](%/Expand_257_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_916_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_916"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_206_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_206"](%/Constant_916_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_917_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_917"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_231_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_231"](%/ConstantOfShape_206_output_0, %/Constant_917_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_206_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_206"](%/Constant_911_output_0, %/Mul_231_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_206_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_206"](%/Equal_206_output_0, %/ConstantOfShape_206_output_0, %/Constant_911_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_258_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_258"](%/Constant_910_output_0, %/Where_206_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_206_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_206"](%/Expand_258_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_918_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_918"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_207_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_207"](%/Constant_918_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_919_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_919"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_232_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_232"](%/ConstantOfShape_207_output_0, %/Constant_919_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_207_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_207"](%/Constant_911_output_0, %/Mul_232_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_207_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_207"](%/Equal_207_output_0, %/ConstantOfShape_207_output_0, %/Constant_911_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_259_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_259"](%/Constant_907_output_0, %/Where_207_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_207_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_207"](%/Expand_259_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_102_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_102"](%/Unsqueeze_204_output_0, %/Unsqueeze_205_output_0, %/Unsqueeze_206_output_0, %/Unsqueeze_207_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_103_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_103"](%/ScatterND_49_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_920_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_920"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_921_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_921"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_922_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_922"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_51_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_51"](%/Shape_103_output_0, %/Constant_921_output_0, %/Constant_922_output_0, %/Constant_920_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_103_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_103"](%/Constant_911_output_0, %/Slice_51_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_103_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_103"](%/Expand_255_output_0, %/Concat_103_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_51_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_51"](%/ScatterND_49_output_0, %/Concat_102_output_0, %/Reshape_103_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_50_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_50"](%/MatMul_150_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_153_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_153"](%/ScatterND_50_output_0, %/Transpose_50_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_51_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_51"](%/MatMul_153_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_923_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_923"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_233_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_233"](%/Transpose_51_output_0, %/Constant_923_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_25_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_25"](%/Mul_233_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_154_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_154"](%/Softmax_25_output_0, %/ScatterND_51_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_155_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_155"](%/MatMul_154_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_3546 : Long(device=cpu) = onnx::Constant[value={58}]()
  %/MatMul_156_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_156"](%/MatMul_155_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_157_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_157"](%/MatMul_155_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_104_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_104"](%/MatMul_157_output_0, %/Constant_54_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_52_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_52"](%/ScatterND_50_output_0, %onnx::Gather_3546), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_104_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_104"](%/Gather_52_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_260_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_260"](%/Reshape_104_output_0, %/Shape_104_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_924_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_924"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_925_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_925"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_926_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_926"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_927_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={58}, onnx_name="/Constant_927"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_928_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_928"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_929_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_929"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_208_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_208"](%/Constant_929_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_930_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_930"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_234_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_234"](%/ConstantOfShape_208_output_0, %/Constant_930_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_208_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_208"](%/Constant_928_output_0, %/Mul_234_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_208_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_208"](%/Equal_208_output_0, %/ConstantOfShape_208_output_0, %/Constant_928_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_261_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_261"](%/Constant_925_output_0, %/Where_208_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_208_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_208"](%/Expand_261_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_931_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_931"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_209_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_209"](%/Constant_931_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_932_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_932"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_235_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_235"](%/ConstantOfShape_209_output_0, %/Constant_932_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_209_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_209"](%/Constant_928_output_0, %/Mul_235_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_209_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_209"](%/Equal_209_output_0, %/ConstantOfShape_209_output_0, %/Constant_928_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_262_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_262"](%/Constant_926_output_0, %/Where_209_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_209_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_209"](%/Expand_262_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_933_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_933"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_210_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_210"](%/Constant_933_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_934_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_934"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_236_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_236"](%/ConstantOfShape_210_output_0, %/Constant_934_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_210_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_210"](%/Constant_928_output_0, %/Mul_236_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_210_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_210"](%/Equal_210_output_0, %/ConstantOfShape_210_output_0, %/Constant_928_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_263_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_263"](%/Constant_927_output_0, %/Where_210_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_210_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_210"](%/Expand_263_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_935_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_935"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_211_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_211"](%/Constant_935_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_936_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_936"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_237_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_237"](%/ConstantOfShape_211_output_0, %/Constant_936_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_211_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_211"](%/Constant_928_output_0, %/Mul_237_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_211_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_211"](%/Equal_211_output_0, %/ConstantOfShape_211_output_0, %/Constant_928_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_264_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_264"](%/Constant_924_output_0, %/Where_211_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_211_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_211"](%/Expand_264_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_104_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_104"](%/Unsqueeze_208_output_0, %/Unsqueeze_209_output_0, %/Unsqueeze_210_output_0, %/Unsqueeze_211_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_105_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_105"](%/ScatterND_50_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_937_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_937"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_938_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_938"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_939_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_939"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_52_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_52"](%/Shape_105_output_0, %/Constant_938_output_0, %/Constant_939_output_0, %/Constant_937_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_105_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_105"](%/Constant_928_output_0, %/Slice_52_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_105_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_105"](%/Expand_260_output_0, %/Concat_105_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_52_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_52"](%/ScatterND_50_output_0, %/Concat_104_output_0, %/Reshape_105_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_158_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_158"](%/MatMul_155_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_106_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_106"](%/MatMul_158_output_0, %/Constant_55_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_53_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_53"](%/ScatterND_51_output_0, %onnx::Gather_3546), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_106_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_106"](%/Gather_53_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_265_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_265"](%/Reshape_106_output_0, %/Shape_106_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_940_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_940"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_941_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_941"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_942_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_942"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_943_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={58}, onnx_name="/Constant_943"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_944_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_944"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_945_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_945"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_212_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_212"](%/Constant_945_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_946_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_946"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_238_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_238"](%/ConstantOfShape_212_output_0, %/Constant_946_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_212_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_212"](%/Constant_944_output_0, %/Mul_238_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_212_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_212"](%/Equal_212_output_0, %/ConstantOfShape_212_output_0, %/Constant_944_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_266_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_266"](%/Constant_941_output_0, %/Where_212_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_212_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_212"](%/Expand_266_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_947_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_947"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_213_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_213"](%/Constant_947_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_948_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_948"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_239_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_239"](%/ConstantOfShape_213_output_0, %/Constant_948_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_213_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_213"](%/Constant_944_output_0, %/Mul_239_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_213_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_213"](%/Equal_213_output_0, %/ConstantOfShape_213_output_0, %/Constant_944_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_267_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_267"](%/Constant_942_output_0, %/Where_213_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_213_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_213"](%/Expand_267_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_949_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_949"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_214_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_214"](%/Constant_949_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_950_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_950"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_240_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_240"](%/ConstantOfShape_214_output_0, %/Constant_950_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_214_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_214"](%/Constant_944_output_0, %/Mul_240_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_214_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_214"](%/Equal_214_output_0, %/ConstantOfShape_214_output_0, %/Constant_944_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_268_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_268"](%/Constant_943_output_0, %/Where_214_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_214_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_214"](%/Expand_268_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_951_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_951"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_215_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_215"](%/Constant_951_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_952_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_952"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_241_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_241"](%/ConstantOfShape_215_output_0, %/Constant_952_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_215_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_215"](%/Constant_944_output_0, %/Mul_241_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_215_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_215"](%/Equal_215_output_0, %/ConstantOfShape_215_output_0, %/Constant_944_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_269_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_269"](%/Constant_940_output_0, %/Where_215_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_215_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_215"](%/Expand_269_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_106_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_106"](%/Unsqueeze_212_output_0, %/Unsqueeze_213_output_0, %/Unsqueeze_214_output_0, %/Unsqueeze_215_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_107_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_107"](%/ScatterND_51_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_953_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_953"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_954_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_954"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_955_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_955"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_53_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_53"](%/Shape_107_output_0, %/Constant_954_output_0, %/Constant_955_output_0, %/Constant_953_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_107_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_107"](%/Constant_944_output_0, %/Slice_53_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_107_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_107"](%/Expand_265_output_0, %/Concat_107_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_53_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_53"](%/ScatterND_51_output_0, %/Concat_106_output_0, %/Reshape_107_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_52_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_52"](%/MatMul_156_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_159_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_159"](%/ScatterND_52_output_0, %/Transpose_52_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_53_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_53"](%/MatMul_159_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_956_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_956"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_242_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_242"](%/Transpose_53_output_0, %/Constant_956_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_26_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_26"](%/Mul_242_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_160_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_160"](%/Softmax_26_output_0, %/ScatterND_53_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_161_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_161"](%/MatMul_160_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_3672 : Long(device=cpu) = onnx::Constant[value={59}]()
  %/MatMul_162_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_162"](%/MatMul_161_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_163_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_163"](%/MatMul_161_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_108_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_108"](%/MatMul_163_output_0, %/Constant_56_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_54_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_54"](%/ScatterND_52_output_0, %onnx::Gather_3672), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_108_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_108"](%/Gather_54_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_270_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_270"](%/Reshape_108_output_0, %/Shape_108_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_957_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_957"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_958_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_958"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_959_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_959"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_960_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={59}, onnx_name="/Constant_960"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_961_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_961"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_962_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_962"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_216_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_216"](%/Constant_962_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_963_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_963"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_243_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_243"](%/ConstantOfShape_216_output_0, %/Constant_963_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_216_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_216"](%/Constant_961_output_0, %/Mul_243_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_216_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_216"](%/Equal_216_output_0, %/ConstantOfShape_216_output_0, %/Constant_961_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_271_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_271"](%/Constant_958_output_0, %/Where_216_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_216_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_216"](%/Expand_271_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_964_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_964"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_217_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_217"](%/Constant_964_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_965_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_965"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_244_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_244"](%/ConstantOfShape_217_output_0, %/Constant_965_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_217_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_217"](%/Constant_961_output_0, %/Mul_244_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_217_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_217"](%/Equal_217_output_0, %/ConstantOfShape_217_output_0, %/Constant_961_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_272_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_272"](%/Constant_959_output_0, %/Where_217_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_217_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_217"](%/Expand_272_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_966_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_966"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_218_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_218"](%/Constant_966_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_967_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_967"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_245_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_245"](%/ConstantOfShape_218_output_0, %/Constant_967_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_218_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_218"](%/Constant_961_output_0, %/Mul_245_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_218_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_218"](%/Equal_218_output_0, %/ConstantOfShape_218_output_0, %/Constant_961_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_273_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_273"](%/Constant_960_output_0, %/Where_218_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_218_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_218"](%/Expand_273_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_968_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_968"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_219_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_219"](%/Constant_968_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_969_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_969"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_246_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_246"](%/ConstantOfShape_219_output_0, %/Constant_969_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_219_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_219"](%/Constant_961_output_0, %/Mul_246_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_219_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_219"](%/Equal_219_output_0, %/ConstantOfShape_219_output_0, %/Constant_961_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_274_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_274"](%/Constant_957_output_0, %/Where_219_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_219_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_219"](%/Expand_274_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_108_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_108"](%/Unsqueeze_216_output_0, %/Unsqueeze_217_output_0, %/Unsqueeze_218_output_0, %/Unsqueeze_219_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_109_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_109"](%/ScatterND_52_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_970_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_970"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_971_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_971"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_972_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_972"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_54_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_54"](%/Shape_109_output_0, %/Constant_971_output_0, %/Constant_972_output_0, %/Constant_970_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_109_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_109"](%/Constant_961_output_0, %/Slice_54_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_109_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_109"](%/Expand_270_output_0, %/Concat_109_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_54_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_54"](%/ScatterND_52_output_0, %/Concat_108_output_0, %/Reshape_109_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_164_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_164"](%/MatMul_161_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_110_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_110"](%/MatMul_164_output_0, %/Constant_57_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_55_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_55"](%/ScatterND_53_output_0, %onnx::Gather_3672), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_110_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_110"](%/Gather_55_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_275_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_275"](%/Reshape_110_output_0, %/Shape_110_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_973_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_973"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_974_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_974"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_975_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_975"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_976_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={59}, onnx_name="/Constant_976"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_977_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_977"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_978_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_978"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_220_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_220"](%/Constant_978_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_979_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_979"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_247_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_247"](%/ConstantOfShape_220_output_0, %/Constant_979_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_220_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_220"](%/Constant_977_output_0, %/Mul_247_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_220_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_220"](%/Equal_220_output_0, %/ConstantOfShape_220_output_0, %/Constant_977_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_276_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_276"](%/Constant_974_output_0, %/Where_220_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_220_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_220"](%/Expand_276_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_980_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_980"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_221_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_221"](%/Constant_980_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_981_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_981"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_248_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_248"](%/ConstantOfShape_221_output_0, %/Constant_981_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_221_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_221"](%/Constant_977_output_0, %/Mul_248_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_221_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_221"](%/Equal_221_output_0, %/ConstantOfShape_221_output_0, %/Constant_977_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_277_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_277"](%/Constant_975_output_0, %/Where_221_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_221_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_221"](%/Expand_277_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_982_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_982"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_222_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_222"](%/Constant_982_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_983_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_983"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_249_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_249"](%/ConstantOfShape_222_output_0, %/Constant_983_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_222_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_222"](%/Constant_977_output_0, %/Mul_249_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_222_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_222"](%/Equal_222_output_0, %/ConstantOfShape_222_output_0, %/Constant_977_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_278_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_278"](%/Constant_976_output_0, %/Where_222_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_222_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_222"](%/Expand_278_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_984_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_984"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_223_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_223"](%/Constant_984_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_985_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_985"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_250_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_250"](%/ConstantOfShape_223_output_0, %/Constant_985_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_223_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_223"](%/Constant_977_output_0, %/Mul_250_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_223_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_223"](%/Equal_223_output_0, %/ConstantOfShape_223_output_0, %/Constant_977_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_279_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_279"](%/Constant_973_output_0, %/Where_223_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_223_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_223"](%/Expand_279_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_110_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_110"](%/Unsqueeze_220_output_0, %/Unsqueeze_221_output_0, %/Unsqueeze_222_output_0, %/Unsqueeze_223_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_111_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_111"](%/ScatterND_53_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_986_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_986"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_987_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_987"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_988_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_988"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_55_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_55"](%/Shape_111_output_0, %/Constant_987_output_0, %/Constant_988_output_0, %/Constant_986_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_111_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_111"](%/Constant_977_output_0, %/Slice_55_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_111_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_111"](%/Expand_275_output_0, %/Concat_111_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_55_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_55"](%/ScatterND_53_output_0, %/Concat_110_output_0, %/Reshape_111_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_54_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_54"](%/MatMul_162_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_165_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_165"](%/ScatterND_54_output_0, %/Transpose_54_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_55_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_55"](%/MatMul_165_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_989_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_989"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_251_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_251"](%/Transpose_55_output_0, %/Constant_989_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_27_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_27"](%/Mul_251_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_166_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_166"](%/Softmax_27_output_0, %/ScatterND_55_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_167_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_167"](%/MatMul_166_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_3798 : Long(device=cpu) = onnx::Constant[value={60}]()
  %/MatMul_168_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_168"](%/MatMul_167_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_169_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_169"](%/MatMul_167_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_112_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_112"](%/MatMul_169_output_0, %/Constant_58_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_56_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_56"](%/ScatterND_54_output_0, %onnx::Gather_3798), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_112_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_112"](%/Gather_56_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_280_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_280"](%/Reshape_112_output_0, %/Shape_112_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_990_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_990"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_991_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_991"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_992_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_992"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_993_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={60}, onnx_name="/Constant_993"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_994_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_994"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_995_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_995"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_224_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_224"](%/Constant_995_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_996_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_996"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_252_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_252"](%/ConstantOfShape_224_output_0, %/Constant_996_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_224_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_224"](%/Constant_994_output_0, %/Mul_252_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_224_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_224"](%/Equal_224_output_0, %/ConstantOfShape_224_output_0, %/Constant_994_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_281_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_281"](%/Constant_991_output_0, %/Where_224_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_224_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_224"](%/Expand_281_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_997_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_997"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_225_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_225"](%/Constant_997_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_998_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_998"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_253_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_253"](%/ConstantOfShape_225_output_0, %/Constant_998_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_225_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_225"](%/Constant_994_output_0, %/Mul_253_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_225_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_225"](%/Equal_225_output_0, %/ConstantOfShape_225_output_0, %/Constant_994_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_282_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_282"](%/Constant_992_output_0, %/Where_225_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_225_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_225"](%/Expand_282_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_999_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_999"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_226_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_226"](%/Constant_999_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1000_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1000"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_254_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_254"](%/ConstantOfShape_226_output_0, %/Constant_1000_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_226_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_226"](%/Constant_994_output_0, %/Mul_254_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_226_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_226"](%/Equal_226_output_0, %/ConstantOfShape_226_output_0, %/Constant_994_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_283_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_283"](%/Constant_993_output_0, %/Where_226_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_226_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_226"](%/Expand_283_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1001_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1001"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_227_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_227"](%/Constant_1001_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1002_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1002"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_255_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_255"](%/ConstantOfShape_227_output_0, %/Constant_1002_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_227_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_227"](%/Constant_994_output_0, %/Mul_255_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_227_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_227"](%/Equal_227_output_0, %/ConstantOfShape_227_output_0, %/Constant_994_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_284_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_284"](%/Constant_990_output_0, %/Where_227_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_227_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_227"](%/Expand_284_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_112_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_112"](%/Unsqueeze_224_output_0, %/Unsqueeze_225_output_0, %/Unsqueeze_226_output_0, %/Unsqueeze_227_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_113_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_113"](%/ScatterND_54_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1003_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_1003"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1004_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1004"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1005_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_1005"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_56_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_56"](%/Shape_113_output_0, %/Constant_1004_output_0, %/Constant_1005_output_0, %/Constant_1003_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_113_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_113"](%/Constant_994_output_0, %/Slice_56_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_113_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_113"](%/Expand_280_output_0, %/Concat_113_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_56_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_56"](%/ScatterND_54_output_0, %/Concat_112_output_0, %/Reshape_113_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_170_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_170"](%/MatMul_167_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_114_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_114"](%/MatMul_170_output_0, %/Constant_59_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_57_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_57"](%/ScatterND_55_output_0, %onnx::Gather_3798), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_114_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_114"](%/Gather_57_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_285_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_285"](%/Reshape_114_output_0, %/Shape_114_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1006_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1006"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1007_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1007"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1008_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1008"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1009_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={60}, onnx_name="/Constant_1009"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1010_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_1010"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1011_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1011"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_228_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_228"](%/Constant_1011_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1012_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1012"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_256_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_256"](%/ConstantOfShape_228_output_0, %/Constant_1012_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_228_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_228"](%/Constant_1010_output_0, %/Mul_256_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_228_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_228"](%/Equal_228_output_0, %/ConstantOfShape_228_output_0, %/Constant_1010_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_286_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_286"](%/Constant_1007_output_0, %/Where_228_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_228_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_228"](%/Expand_286_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1013_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1013"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_229_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_229"](%/Constant_1013_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1014_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1014"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_257_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_257"](%/ConstantOfShape_229_output_0, %/Constant_1014_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_229_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_229"](%/Constant_1010_output_0, %/Mul_257_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_229_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_229"](%/Equal_229_output_0, %/ConstantOfShape_229_output_0, %/Constant_1010_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_287_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_287"](%/Constant_1008_output_0, %/Where_229_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_229_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_229"](%/Expand_287_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1015_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1015"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_230_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_230"](%/Constant_1015_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1016_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1016"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_258_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_258"](%/ConstantOfShape_230_output_0, %/Constant_1016_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_230_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_230"](%/Constant_1010_output_0, %/Mul_258_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_230_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_230"](%/Equal_230_output_0, %/ConstantOfShape_230_output_0, %/Constant_1010_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_288_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_288"](%/Constant_1009_output_0, %/Where_230_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_230_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_230"](%/Expand_288_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1017_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1017"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_231_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_231"](%/Constant_1017_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1018_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1018"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_259_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_259"](%/ConstantOfShape_231_output_0, %/Constant_1018_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_231_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_231"](%/Constant_1010_output_0, %/Mul_259_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_231_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_231"](%/Equal_231_output_0, %/ConstantOfShape_231_output_0, %/Constant_1010_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_289_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_289"](%/Constant_1006_output_0, %/Where_231_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_231_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_231"](%/Expand_289_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_114_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_114"](%/Unsqueeze_228_output_0, %/Unsqueeze_229_output_0, %/Unsqueeze_230_output_0, %/Unsqueeze_231_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_115_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_115"](%/ScatterND_55_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1019_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_1019"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1020_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1020"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1021_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_1021"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_57_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_57"](%/Shape_115_output_0, %/Constant_1020_output_0, %/Constant_1021_output_0, %/Constant_1019_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_115_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_115"](%/Constant_1010_output_0, %/Slice_57_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_115_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_115"](%/Expand_285_output_0, %/Concat_115_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_57_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_57"](%/ScatterND_55_output_0, %/Concat_114_output_0, %/Reshape_115_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_56_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_56"](%/MatMul_168_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_171_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_171"](%/ScatterND_56_output_0, %/Transpose_56_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_57_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_57"](%/MatMul_171_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_1022_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_1022"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_260_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_260"](%/Transpose_57_output_0, %/Constant_1022_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_28_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_28"](%/Mul_260_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_172_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_172"](%/Softmax_28_output_0, %/ScatterND_57_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_173_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_173"](%/MatMul_172_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_3924 : Long(device=cpu) = onnx::Constant[value={61}]()
  %/MatMul_174_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_174"](%/MatMul_173_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_175_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_175"](%/MatMul_173_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_116_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_116"](%/MatMul_175_output_0, %/Constant_60_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_58_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_58"](%/ScatterND_56_output_0, %onnx::Gather_3924), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_116_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_116"](%/Gather_58_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_290_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_290"](%/Reshape_116_output_0, %/Shape_116_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1023_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1023"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1024_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1024"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1025_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1025"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1026_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={61}, onnx_name="/Constant_1026"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1027_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_1027"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1028_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1028"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_232_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_232"](%/Constant_1028_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1029_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1029"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_261_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_261"](%/ConstantOfShape_232_output_0, %/Constant_1029_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_232_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_232"](%/Constant_1027_output_0, %/Mul_261_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_232_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_232"](%/Equal_232_output_0, %/ConstantOfShape_232_output_0, %/Constant_1027_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_291_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_291"](%/Constant_1024_output_0, %/Where_232_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_232_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_232"](%/Expand_291_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1030_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1030"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_233_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_233"](%/Constant_1030_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1031_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1031"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_262_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_262"](%/ConstantOfShape_233_output_0, %/Constant_1031_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_233_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_233"](%/Constant_1027_output_0, %/Mul_262_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_233_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_233"](%/Equal_233_output_0, %/ConstantOfShape_233_output_0, %/Constant_1027_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_292_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_292"](%/Constant_1025_output_0, %/Where_233_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_233_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_233"](%/Expand_292_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1032_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1032"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_234_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_234"](%/Constant_1032_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1033_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1033"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_263_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_263"](%/ConstantOfShape_234_output_0, %/Constant_1033_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_234_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_234"](%/Constant_1027_output_0, %/Mul_263_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_234_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_234"](%/Equal_234_output_0, %/ConstantOfShape_234_output_0, %/Constant_1027_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_293_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_293"](%/Constant_1026_output_0, %/Where_234_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_234_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_234"](%/Expand_293_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1034_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1034"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_235_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_235"](%/Constant_1034_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1035_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1035"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_264_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_264"](%/ConstantOfShape_235_output_0, %/Constant_1035_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_235_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_235"](%/Constant_1027_output_0, %/Mul_264_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_235_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_235"](%/Equal_235_output_0, %/ConstantOfShape_235_output_0, %/Constant_1027_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_294_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_294"](%/Constant_1023_output_0, %/Where_235_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_235_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_235"](%/Expand_294_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_116_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_116"](%/Unsqueeze_232_output_0, %/Unsqueeze_233_output_0, %/Unsqueeze_234_output_0, %/Unsqueeze_235_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_117_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_117"](%/ScatterND_56_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1036_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_1036"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1037_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1037"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1038_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_1038"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_58_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_58"](%/Shape_117_output_0, %/Constant_1037_output_0, %/Constant_1038_output_0, %/Constant_1036_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_117_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_117"](%/Constant_1027_output_0, %/Slice_58_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_117_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_117"](%/Expand_290_output_0, %/Concat_117_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_58_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_58"](%/ScatterND_56_output_0, %/Concat_116_output_0, %/Reshape_117_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_176_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_176"](%/MatMul_173_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_118_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_118"](%/MatMul_176_output_0, %/Constant_61_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_59_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_59"](%/ScatterND_57_output_0, %onnx::Gather_3924), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_118_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_118"](%/Gather_59_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_295_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_295"](%/Reshape_118_output_0, %/Shape_118_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1039_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1039"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1040_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1040"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1041_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1041"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1042_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={61}, onnx_name="/Constant_1042"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1043_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_1043"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1044_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1044"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_236_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_236"](%/Constant_1044_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1045_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1045"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_265_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_265"](%/ConstantOfShape_236_output_0, %/Constant_1045_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_236_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_236"](%/Constant_1043_output_0, %/Mul_265_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_236_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_236"](%/Equal_236_output_0, %/ConstantOfShape_236_output_0, %/Constant_1043_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_296_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_296"](%/Constant_1040_output_0, %/Where_236_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_236_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_236"](%/Expand_296_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1046_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1046"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_237_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_237"](%/Constant_1046_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1047_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1047"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_266_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_266"](%/ConstantOfShape_237_output_0, %/Constant_1047_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_237_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_237"](%/Constant_1043_output_0, %/Mul_266_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_237_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_237"](%/Equal_237_output_0, %/ConstantOfShape_237_output_0, %/Constant_1043_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_297_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_297"](%/Constant_1041_output_0, %/Where_237_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_237_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_237"](%/Expand_297_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1048_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1048"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_238_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_238"](%/Constant_1048_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1049_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1049"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_267_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_267"](%/ConstantOfShape_238_output_0, %/Constant_1049_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_238_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_238"](%/Constant_1043_output_0, %/Mul_267_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_238_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_238"](%/Equal_238_output_0, %/ConstantOfShape_238_output_0, %/Constant_1043_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_298_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_298"](%/Constant_1042_output_0, %/Where_238_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_238_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_238"](%/Expand_298_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1050_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1050"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_239_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_239"](%/Constant_1050_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1051_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1051"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_268_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_268"](%/ConstantOfShape_239_output_0, %/Constant_1051_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_239_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_239"](%/Constant_1043_output_0, %/Mul_268_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_239_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_239"](%/Equal_239_output_0, %/ConstantOfShape_239_output_0, %/Constant_1043_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_299_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_299"](%/Constant_1039_output_0, %/Where_239_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_239_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_239"](%/Expand_299_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_118_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_118"](%/Unsqueeze_236_output_0, %/Unsqueeze_237_output_0, %/Unsqueeze_238_output_0, %/Unsqueeze_239_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_119_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_119"](%/ScatterND_57_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1052_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_1052"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1053_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1053"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1054_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_1054"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_59_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_59"](%/Shape_119_output_0, %/Constant_1053_output_0, %/Constant_1054_output_0, %/Constant_1052_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_119_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_119"](%/Constant_1043_output_0, %/Slice_59_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_119_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_119"](%/Expand_295_output_0, %/Concat_119_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_59_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_59"](%/ScatterND_57_output_0, %/Concat_118_output_0, %/Reshape_119_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_58_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_58"](%/MatMul_174_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_177_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_177"](%/ScatterND_58_output_0, %/Transpose_58_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_59_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_59"](%/MatMul_177_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_1055_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_1055"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_269_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_269"](%/Transpose_59_output_0, %/Constant_1055_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_29_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_29"](%/Mul_269_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_178_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_178"](%/Softmax_29_output_0, %/ScatterND_59_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_179_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_179"](%/MatMul_178_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_4050 : Long(device=cpu) = onnx::Constant[value={62}]()
  %/MatMul_180_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_180"](%/MatMul_179_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_181_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_181"](%/MatMul_179_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_120_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_120"](%/MatMul_181_output_0, %/Constant_62_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_60_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_60"](%/ScatterND_58_output_0, %onnx::Gather_4050), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_120_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_120"](%/Gather_60_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_300_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_300"](%/Reshape_120_output_0, %/Shape_120_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1056_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1056"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1057_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1057"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1058_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1058"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1059_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={62}, onnx_name="/Constant_1059"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1060_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_1060"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1061_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1061"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_240_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_240"](%/Constant_1061_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1062_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1062"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_270_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_270"](%/ConstantOfShape_240_output_0, %/Constant_1062_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_240_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_240"](%/Constant_1060_output_0, %/Mul_270_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_240_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_240"](%/Equal_240_output_0, %/ConstantOfShape_240_output_0, %/Constant_1060_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_301_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_301"](%/Constant_1057_output_0, %/Where_240_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_240_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_240"](%/Expand_301_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1063_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1063"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_241_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_241"](%/Constant_1063_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1064_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1064"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_271_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_271"](%/ConstantOfShape_241_output_0, %/Constant_1064_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_241_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_241"](%/Constant_1060_output_0, %/Mul_271_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_241_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_241"](%/Equal_241_output_0, %/ConstantOfShape_241_output_0, %/Constant_1060_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_302_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_302"](%/Constant_1058_output_0, %/Where_241_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_241_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_241"](%/Expand_302_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1065_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1065"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_242_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_242"](%/Constant_1065_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1066_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1066"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_272_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_272"](%/ConstantOfShape_242_output_0, %/Constant_1066_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_242_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_242"](%/Constant_1060_output_0, %/Mul_272_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_242_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_242"](%/Equal_242_output_0, %/ConstantOfShape_242_output_0, %/Constant_1060_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_303_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_303"](%/Constant_1059_output_0, %/Where_242_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_242_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_242"](%/Expand_303_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1067_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1067"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_243_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_243"](%/Constant_1067_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1068_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1068"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_273_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_273"](%/ConstantOfShape_243_output_0, %/Constant_1068_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_243_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_243"](%/Constant_1060_output_0, %/Mul_273_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_243_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_243"](%/Equal_243_output_0, %/ConstantOfShape_243_output_0, %/Constant_1060_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_304_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_304"](%/Constant_1056_output_0, %/Where_243_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_243_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_243"](%/Expand_304_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_120_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_120"](%/Unsqueeze_240_output_0, %/Unsqueeze_241_output_0, %/Unsqueeze_242_output_0, %/Unsqueeze_243_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_121_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_121"](%/ScatterND_58_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1069_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_1069"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1070_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1070"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1071_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_1071"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_60_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_60"](%/Shape_121_output_0, %/Constant_1070_output_0, %/Constant_1071_output_0, %/Constant_1069_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_121_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_121"](%/Constant_1060_output_0, %/Slice_60_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_121_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_121"](%/Expand_300_output_0, %/Concat_121_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ScatterND_60_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_60"](%/ScatterND_58_output_0, %/Concat_120_output_0, %/Reshape_121_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_182_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_182"](%/MatMul_179_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_122_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_122"](%/MatMul_182_output_0, %/Constant_63_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_61_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_61"](%/ScatterND_59_output_0, %onnx::Gather_4050), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_122_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_122"](%/Gather_61_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_305_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_305"](%/Reshape_122_output_0, %/Shape_122_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1072_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1072"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1073_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1073"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1074_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1074"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1075_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={62}, onnx_name="/Constant_1075"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1076_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_1076"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1077_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1077"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_244_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_244"](%/Constant_1077_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1078_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1078"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_274_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_274"](%/ConstantOfShape_244_output_0, %/Constant_1078_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_244_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_244"](%/Constant_1076_output_0, %/Mul_274_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_244_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_244"](%/Equal_244_output_0, %/ConstantOfShape_244_output_0, %/Constant_1076_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_306_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_306"](%/Constant_1073_output_0, %/Where_244_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_244_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_244"](%/Expand_306_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1079_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1079"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_245_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_245"](%/Constant_1079_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1080_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1080"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_275_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_275"](%/ConstantOfShape_245_output_0, %/Constant_1080_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_245_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_245"](%/Constant_1076_output_0, %/Mul_275_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_245_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_245"](%/Equal_245_output_0, %/ConstantOfShape_245_output_0, %/Constant_1076_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_307_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_307"](%/Constant_1074_output_0, %/Where_245_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_245_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_245"](%/Expand_307_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1081_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1081"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_246_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_246"](%/Constant_1081_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1082_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1082"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_276_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_276"](%/ConstantOfShape_246_output_0, %/Constant_1082_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_246_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_246"](%/Constant_1076_output_0, %/Mul_276_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_246_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_246"](%/Equal_246_output_0, %/ConstantOfShape_246_output_0, %/Constant_1076_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_308_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_308"](%/Constant_1075_output_0, %/Where_246_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_246_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_246"](%/Expand_308_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1083_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1083"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_247_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_247"](%/Constant_1083_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1084_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1084"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_277_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_277"](%/ConstantOfShape_247_output_0, %/Constant_1084_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_247_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_247"](%/Constant_1076_output_0, %/Mul_277_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_247_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_247"](%/Equal_247_output_0, %/ConstantOfShape_247_output_0, %/Constant_1076_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_309_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_309"](%/Constant_1072_output_0, %/Where_247_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_247_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_247"](%/Expand_309_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_122_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_122"](%/Unsqueeze_244_output_0, %/Unsqueeze_245_output_0, %/Unsqueeze_246_output_0, %/Unsqueeze_247_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_123_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_123"](%/ScatterND_59_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1085_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_1085"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1086_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1086"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1087_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_1087"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_61_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_61"](%/Shape_123_output_0, %/Constant_1086_output_0, %/Constant_1087_output_0, %/Constant_1085_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_123_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_123"](%/Constant_1076_output_0, %/Slice_61_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_123_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_123"](%/Expand_305_output_0, %/Concat_123_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ScatterND_61_output_0 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], device=cpu) = onnx::ScatterND[onnx_name="/ScatterND_61"](%/ScatterND_59_output_0, %/Concat_122_output_0, %/Reshape_123_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_60_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_60"](%/MatMul_180_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_183_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_183"](%/ScatterND_60_output_0, %/Transpose_60_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_61_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_61"](%/MatMul_183_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_1088_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_1088"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_278_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_278"](%/Transpose_61_output_0, %/Constant_1088_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_30_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_30"](%/Mul_278_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_184_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_184"](%/Softmax_30_output_0, %/ScatterND_61_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %/MatMul_185_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_185"](%/MatMul_184_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  %onnx::Gather_4176 : Long(device=cpu) = onnx::Constant[value={63}]()
  %/MatMul_186_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_186"](%/MatMul_185_output_0, %weight_q), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:57:16
  %/MatMul_187_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_187"](%/MatMul_185_output_0, %weight_k), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:47
  %/Reshape_124_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_124"](%/MatMul_187_output_0, %/Constant_64_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:33
  %/Gather_62_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_62"](%/ScatterND_60_output_0, %onnx::Gather_4176), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_124_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_124"](%/Gather_62_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_310_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_310"](%/Reshape_124_output_0, %/Shape_124_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1089_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1089"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1090_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1090"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1091_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1091"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1092_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={63}, onnx_name="/Constant_1092"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1093_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_1093"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1094_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1094"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_248_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_248"](%/Constant_1094_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1095_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1095"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_279_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_279"](%/ConstantOfShape_248_output_0, %/Constant_1095_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_248_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_248"](%/Constant_1093_output_0, %/Mul_279_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_248_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_248"](%/Equal_248_output_0, %/ConstantOfShape_248_output_0, %/Constant_1093_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_311_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_311"](%/Constant_1090_output_0, %/Where_248_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_248_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_248"](%/Expand_311_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1096_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1096"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_249_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_249"](%/Constant_1096_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1097_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1097"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_280_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_280"](%/ConstantOfShape_249_output_0, %/Constant_1097_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_249_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_249"](%/Constant_1093_output_0, %/Mul_280_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_249_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_249"](%/Equal_249_output_0, %/ConstantOfShape_249_output_0, %/Constant_1093_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_312_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_312"](%/Constant_1091_output_0, %/Where_249_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_249_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_249"](%/Expand_312_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1098_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1098"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_250_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_250"](%/Constant_1098_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1099_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1099"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_281_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_281"](%/ConstantOfShape_250_output_0, %/Constant_1099_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_250_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_250"](%/Constant_1093_output_0, %/Mul_281_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_250_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_250"](%/Equal_250_output_0, %/ConstantOfShape_250_output_0, %/Constant_1093_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_313_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_313"](%/Constant_1092_output_0, %/Where_250_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_250_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_250"](%/Expand_313_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1100_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1100"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/ConstantOfShape_251_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_251"](%/Constant_1100_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1101_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1101"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Mul_282_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_282"](%/ConstantOfShape_251_output_0, %/Constant_1101_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Equal_251_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_251"](%/Constant_1093_output_0, %/Mul_282_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Where_251_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_251"](%/Equal_251_output_0, %/ConstantOfShape_251_output_0, %/Constant_1093_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Expand_314_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_314"](%/Constant_1089_output_0, %/Where_251_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Unsqueeze_251_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_251"](%/Expand_314_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_124_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_124"](%/Unsqueeze_248_output_0, %/Unsqueeze_249_output_0, %/Unsqueeze_250_output_0, %/Unsqueeze_251_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Shape_125_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_125"](%/ScatterND_60_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1102_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_1102"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1103_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1103"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Constant_1104_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_1104"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Slice_62_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_62"](%/Shape_125_output_0, %/Constant_1103_output_0, %/Constant_1104_output_0, %/Constant_1102_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Concat_125_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_125"](%/Constant_1093_output_0, %/Slice_62_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/Reshape_125_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_125"](%/Expand_310_output_0, %/Concat_125_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %onnx::MatMul_4235 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], requires_grad=0, device=cuda:0) = onnx::ScatterND[onnx_name="/ScatterND_62"](%/ScatterND_60_output_0, %/Concat_124_output_0, %/Reshape_125_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:58:12
  %/MatMul_188_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_188"](%/MatMul_185_output_0, %weight_v), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:47
  %/Reshape_126_output_0 : Float(64, 12, 64, device=cpu) = onnx::Reshape[onnx_name="/Reshape_126"](%/MatMul_188_output_0, %/Constant_65_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:33
  %/Gather_63_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Gather[axis=2, onnx_name="/Gather_63"](%/ScatterND_61_output_0, %onnx::Gather_4176), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_126_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_126"](%/Gather_63_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_315_output_0 : Float(64, 12, 64, strides=[768, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_315"](%/Reshape_126_output_0, %/Shape_126_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1105_output_0 : Long(64, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1105"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1106_output_0 : Long(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1106"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1107_output_0 : Long(12, 1, 1, strides=[1, 1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>, onnx_name="/Constant_1107"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1108_output_0 : Long(1, 1, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value={63}, onnx_name="/Constant_1108"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1109_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 64  12   1  64 [ CPULongType{4} ], onnx_name="/Constant_1109"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1110_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1110"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_252_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_252"](%/Constant_1110_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1111_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1111"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_283_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_283"](%/ConstantOfShape_252_output_0, %/Constant_1111_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_252_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_252"](%/Constant_1109_output_0, %/Mul_283_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_252_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_252"](%/Equal_252_output_0, %/ConstantOfShape_252_output_0, %/Constant_1109_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_316_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_316"](%/Constant_1106_output_0, %/Where_252_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_252_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_252"](%/Expand_316_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1112_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1112"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_253_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_253"](%/Constant_1112_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1113_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1113"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_284_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_284"](%/ConstantOfShape_253_output_0, %/Constant_1113_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_253_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_253"](%/Constant_1109_output_0, %/Mul_284_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_253_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_253"](%/Equal_253_output_0, %/ConstantOfShape_253_output_0, %/Constant_1109_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_317_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_317"](%/Constant_1107_output_0, %/Where_253_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_253_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_253"](%/Expand_317_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1114_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1114"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_254_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_254"](%/Constant_1114_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1115_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1115"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_285_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_285"](%/ConstantOfShape_254_output_0, %/Constant_1115_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_254_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_254"](%/Constant_1109_output_0, %/Mul_285_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_254_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_254"](%/Equal_254_output_0, %/ConstantOfShape_254_output_0, %/Constant_1109_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_318_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_318"](%/Constant_1108_output_0, %/Where_254_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_254_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_254"](%/Expand_318_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1116_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1116"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/ConstantOfShape_255_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={1}, onnx_name="/ConstantOfShape_255"](%/Constant_1116_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1117_output_0 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name="/Constant_1117"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Mul_286_output_0 : Long(4, strides=[1], device=cpu) = onnx::Mul[onnx_name="/Mul_286"](%/ConstantOfShape_255_output_0, %/Constant_1117_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Equal_255_output_0 : Bool(4, strides=[1], device=cpu) = onnx::Equal[onnx_name="/Equal_255"](%/Constant_1109_output_0, %/Mul_286_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Where_255_output_0 : Long(4, strides=[1], device=cpu) = onnx::Where[onnx_name="/Where_255"](%/Equal_255_output_0, %/ConstantOfShape_255_output_0, %/Constant_1109_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Expand_319_output_0 : Long(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Expand[onnx_name="/Expand_319"](%/Constant_1105_output_0, %/Where_255_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Unsqueeze_255_output_0 : Long(64, 12, 1, 64, 1, strides=[768, 64, 64, 1, 1], device=cpu) = onnx::Unsqueeze[axes=[-1], onnx_name="/Unsqueeze_255"](%/Expand_319_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_126_output_0 : Long(64, 12, 1, 64, 4, strides=[3072, 256, 256, 4, 1], device=cpu) = onnx::Concat[axis=-1, onnx_name="/Concat_126"](%/Unsqueeze_252_output_0, %/Unsqueeze_253_output_0, %/Unsqueeze_254_output_0, %/Unsqueeze_255_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Shape_127_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name="/Shape_127"](%/ScatterND_61_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1118_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name="/Constant_1118"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1119_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={4}, onnx_name="/Constant_1119"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Constant_1120_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={9223372036854775807}, onnx_name="/Constant_1120"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Slice_63_output_0 : Long(0, strides=[1], device=cpu) = onnx::Slice[onnx_name="/Slice_63"](%/Shape_127_output_0, %/Constant_1119_output_0, %/Constant_1120_output_0, %/Constant_1118_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Concat_127_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name="/Concat_127"](%/Constant_1109_output_0, %/Slice_63_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Reshape_127_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Reshape[onnx_name="/Reshape_127"](%/Expand_315_output_0, %/Concat_127_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %onnx::MatMul_4293 : Float(64, 12, 64, 64, strides=[49152, 4096, 64, 1], requires_grad=0, device=cuda:0) = onnx::ScatterND[onnx_name="/ScatterND_63"](%/ScatterND_61_output_0, %/Concat_126_output_0, %/Reshape_127_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:59:12
  %/Transpose_62_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_62"](%/MatMul_186_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:35
  %/MatMul_189_output_0 : Float(64, 12, 64, 1, strides=[768, 64, 1, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_189"](%onnx::MatMul_4235, %/Transpose_62_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Transpose_63_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name="/Transpose_63"](%/MatMul_189_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:60:19
  %/Constant_1121_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.125}, onnx_name="/Constant_1121"](), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Mul_287_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Mul[onnx_name="/Mul_287"](%/Transpose_63_output_0, %/Constant_1121_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:61:19
  %/Softmax_31_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::Softmax[axis=3, onnx_name="/Softmax_31"](%/Mul_287_output_0), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:62:19
  %/MatMul_190_output_0 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], device=cpu) = onnx::MatMul[onnx_name="/MatMul_190"](%/Softmax_31_output_0, %onnx::MatMul_4293), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:63:16
  %x.252 : Float(64, 12, 1, 64, strides=[768, 64, 64, 1], requires_grad=0, device=cuda:0) = onnx::MatMul[onnx_name="/MatMul_191"](%/MatMul_190_output_0, %weight_o), scope: Attention:: # /root/mlir/baseline/attention/attention_pytorch.py:64:16
  return (%onnx::MatMul_4235, %onnx::MatMul_4293, %x.252)

----batch_size=64---torchscript=True----
[warmup]
Time 73.88877868652344 ms
Time 539.1044616699219 ms
Time 7.6427459716796875 ms
Time 7.44318962097168 ms
Time 7.676124572753906 ms
Time 7.396697998046875 ms
Time 7.321834564208984 ms
Time 7.35020637512207 ms
Time 7.431268692016602 ms
Time 7.367372512817383 ms
Time 7.30443000793457 ms
Time 7.472515106201172 ms
Time 7.198572158813477 ms
Time 7.373332977294922 ms
Time 7.348060607910156 ms
Time 7.434606552124023 ms
Time 7.38215446472168 ms
Time 7.333517074584961 ms
Time 7.441997528076172 ms
Time 7.26771354675293 ms
Time 7.74383544921875 ms
Time 7.854223251342773 ms
Time 7.647991180419922 ms
Time 7.661581039428711 ms
Time 7.582426071166992 ms
Time 7.336616516113281 ms
Time 7.401704788208008 ms
Time 7.4100494384765625 ms
Time 7.4462890625 ms
Time 7.364511489868164 ms
Time 7.38978385925293 ms
Time 7.320880889892578 ms
Time 7.51185417175293 ms
Time 7.175922393798828 ms
Time 7.185459136962891 ms
Time 7.381677627563477 ms
Time 7.345438003540039 ms
Time 7.244586944580078 ms
Time 7.341861724853516 ms
Time 7.436275482177734 ms
Time 7.377386093139648 ms
Time 7.785558700561523 ms
Time 7.909536361694336 ms
Time 7.723808288574219 ms
Time 7.523059844970703 ms
Time 7.367134094238281 ms
Time 7.336616516113281 ms
Time 7.249593734741211 ms
Time 7.238149642944336 ms
Time 7.53331184387207 ms
Time 7.471323013305664 ms
Time 7.456541061401367 ms
Time 7.435321807861328 ms
Time 7.259845733642578 ms
Time 7.353782653808594 ms
Time 7.45701789855957 ms
Time 7.471799850463867 ms
Time 7.265567779541016 ms
Time 7.338762283325195 ms
Time 7.3795318603515625 ms
Time 7.460594177246094 ms
Time 7.631063461303711 ms
Time 7.858037948608398 ms
Time 7.62939453125 ms
Time 7.926464080810547 ms
Time 7.622480392456055 ms
Time 7.353544235229492 ms
Time 7.24339485168457 ms
Time 7.319927215576172 ms
Time 7.503509521484375 ms
Time 7.369756698608398 ms
Time 7.406711578369141 ms
Time 7.344961166381836 ms
Time 7.493257522583008 ms
Time 7.383108139038086 ms
Time 7.332801818847656 ms
Time 7.459878921508789 ms
Time 7.343053817749023 ms
Time 7.33637809753418 ms
Time 7.309198379516602 ms
Time 7.412910461425781 ms
Time 7.308721542358398 ms
Time 7.709741592407227 ms
Time 7.917881011962891 ms
Time 7.623910903930664 ms
Time 7.616758346557617 ms
Time 7.538318634033203 ms
Time 7.311344146728516 ms
Time 7.249355316162109 ms
Time 7.396221160888672 ms
Time 7.387876510620117 ms
Time 7.257223129272461 ms
Time 7.387399673461914 ms
Time 7.336139678955078 ms
Time 7.505893707275391 ms
Time 7.429361343383789 ms
Time 7.325649261474609 ms
Time 7.403850555419922 ms
Time 7.427692413330078 ms
Time 7.30443000793457 ms
[run]
[31m100 iters, min = 7.1929 ms, max = 7.9045 ms, avg = 7.4285 ms[m
============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============
verbose: False, log level: Level.ERROR
======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================

Generating '/tmp/nsys-report-e8eb.qdstrm'
[1/7] [0%                          ] report2.nsys-rep[1/7] [0%                          ] report2.nsys-rep[1/7] [8%                          ] report2.nsys-rep[1/7] [7%                          ] report2.nsys-rep[1/7] [6%                          ] report2.nsys-rep[1/7] [5%                          ] report2.nsys-rep[1/7] [7%                          ] report2.nsys-rep[1/7] [6%                          ] report2.nsys-rep[1/7] [5%                          ] report2.nsys-rep[1/7] [9%                          ] report2.nsys-rep[1/7] [12%                         ] report2.nsys-rep[1/7] [=15%                        ] report2.nsys-rep[1/7] [==18%                       ] report2.nsys-rep[1/7] [==21%                       ] report2.nsys-rep[1/7] [===24%                      ] report2.nsys-rep[1/7] [====27%                     ] report2.nsys-rep[1/7] [=====30%                    ] report2.nsys-rep[1/7] [======34%                   ] report2.nsys-rep[1/7] [=======37%                  ] report2.nsys-rep[1/7] [========40%                 ] report2.nsys-rep[1/7] [=========43%                ] report2.nsys-rep[1/7] [=========46%                ] report2.nsys-rep[1/7] [==========49%               ] report2.nsys-rep[1/7] [===========52%              ] report2.nsys-rep[1/7] [============56%             ] report2.nsys-rep[1/7] [=============59%            ] report2.nsys-rep[1/7] [==============62%           ] report2.nsys-rep[1/7] [==================75%       ] report2.nsys-rep[1/7] [========================100%] report2.nsys-rep[1/7] [========================100%] report2.nsys-rep
[2/7] [0%                          ] report2.sqlite[2/7] [1%                          ] report2.sqlite[2/7] [2%                          ] report2.sqlite[2/7] [3%                          ] report2.sqlite[2/7] [4%                          ] report2.sqlite[2/7] [5%                          ] report2.sqlite[2/7] [6%                          ] report2.sqlite[2/7] [7%                          ] report2.sqlite[2/7] [8%                          ] report2.sqlite[2/7] [9%                          ] report2.sqlite[2/7] [10%                         ] report2.sqlite[2/7] [11%                         ] report2.sqlite[2/7] [12%                         ] report2.sqlite[2/7] [13%                         ] report2.sqlite[2/7] [14%                         ] report2.sqlite[2/7] [=15%                        ] report2.sqlite[2/7] [=16%                        ] report2.sqlite[2/7] [=17%                        ] report2.sqlite[2/7] [==18%                       ] report2.sqlite[2/7] [==19%                       ] report2.sqlite[2/7] [==20%                       ] report2.sqlite[2/7] [==21%                       ] report2.sqlite[2/7] [===22%                      ] report2.sqlite[2/7] [===23%                      ] report2.sqlite[2/7] [===24%                      ] report2.sqlite[2/7] [====25%                     ] report2.sqlite[2/7] [====26%                     ] report2.sqlite[2/7] [====27%                     ] report2.sqlite[2/7] [====28%                     ] report2.sqlite[2/7] [=====29%                    ] report2.sqlite[2/7] [=====30%                    ] report2.sqlite[2/7] [=====31%                    ] report2.sqlite[2/7] [=====32%                    ] report2.sqlite[2/7] [======33%                   ] report2.sqlite[2/7] [======34%                   ] report2.sqlite[2/7] [======35%                   ] report2.sqlite[2/7] [=======36%                  ] report2.sqlite[2/7] [=======37%                  ] report2.sqlite[2/7] [=======38%                  ] report2.sqlite[2/7] [=======39%                  ] report2.sqlite[2/7] [========40%                 ] report2.sqlite[2/7] [========41%                 ] report2.sqlite[2/7] [========42%                 ] report2.sqlite[2/7] [=========43%                ] report2.sqlite[2/7] [=========44%                ] report2.sqlite[2/7] [=========45%                ] report2.sqlite[2/7] [=========46%                ] report2.sqlite[2/7] [==========47%               ] report2.sqlite[2/7] [==========48%               ] report2.sqlite[2/7] [==========49%               ] report2.sqlite[2/7] [===========50%              ] report2.sqlite[2/7] [===========51%              ] report2.sqlite[2/7] [===========52%              ] report2.sqlite[2/7] [===========53%              ] report2.sqlite[2/7] [============54%             ] report2.sqlite[2/7] [============55%             ] report2.sqlite[2/7] [============56%             ] report2.sqlite[2/7] [============57%             ] report2.sqlite[2/7] [=============58%            ] report2.sqlite[2/7] [=============59%            ] report2.sqlite[2/7] [=============60%            ] report2.sqlite[2/7] [==============61%           ] report2.sqlite[2/7] [==============62%           ] report2.sqlite[2/7] [==============63%           ] report2.sqlite[2/7] [==============64%           ] report2.sqlite[2/7] [===============65%          ] report2.sqlite[2/7] [===============66%          ] report2.sqlite[2/7] [===============67%          ] report2.sqlite[2/7] [================68%         ] report2.sqlite[2/7] [================69%         ] report2.sqlite[2/7] [================70%         ] report2.sqlite[2/7] [================71%         ] report2.sqlite[2/7] [=================72%        ] report2.sqlite[2/7] [=================73%        ] report2.sqlite[2/7] [=================74%        ] report2.sqlite[2/7] [==================75%       ] report2.sqlite[2/7] [==================76%       ] report2.sqlite[2/7] [==================77%       ] report2.sqlite[2/7] [==================78%       ] report2.sqlite[2/7] [===================79%      ] report2.sqlite[2/7] [===================80%      ] report2.sqlite[2/7] [===================81%      ] report2.sqlite[2/7] [===================82%      ] report2.sqlite[2/7] [====================83%     ] report2.sqlite[2/7] [====================84%     ] report2.sqlite[2/7] [====================85%     ] report2.sqlite[2/7] [=====================86%    ] report2.sqlite[2/7] [=====================87%    ] report2.sqlite[2/7] [=====================88%    ] report2.sqlite[2/7] [=====================89%    ] report2.sqlite[2/7] [======================90%   ] report2.sqlite[2/7] [======================91%   ] report2.sqlite[2/7] [======================92%   ] report2.sqlite[2/7] [=======================93%  ] report2.sqlite[2/7] [=======================94%  ] report2.sqlite[2/7] [=======================95%  ] report2.sqlite[2/7] [=======================96%  ] report2.sqlite[2/7] [========================97% ] report2.sqlite[2/7] [========================98% ] report2.sqlite[2/7] [========================99% ] report2.sqlite[2/7] [========================100%] report2.sqlite[2/7] [========================100%] report2.sqlite
[3/7] Executing 'nvtx_sum' stats report

 Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)   Style                                 Range                               
 --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  -------  ------------------------------------------------------------------
     11.7        592314916       6400      92549.2      28555.5      26388  401028218    5012481.2  PushPop  nvFuser::Manager::runCudaFusionGroup                              
     11.5        582478153       6400      91012.2      27100.5      24974  401023210    5012437.2  PushPop  GraphCache::runGraphWithInputs                                    
     11.3        573134162       6400      89552.2      25697.0      23676  401012445    5012320.7  PushPop  FusionExecutorCache::runFusionWithInputs                          
     10.0        506052523       6400      79070.7      22053.0      20319  358015540    4474906.4  PushPop  FusionKernelRuntime::runWithInput                                 
      9.6        486516295      12800      38009.1      10491.5       7447  201533733    2254981.3  PushPop  FusionKernelRuntime::runKernelWithInput                           
      7.0        357321039          2  178660519.5  178660519.5  156182944  201138095   31788092.1  PushPop  FusionKernelRuntime::runKernelWithInput::Compile                  
      6.8        345063358         42    8215794.2        388.5         90  115021728   29468810.6  PushPop  compileFusionRecursive                                            
      6.7        342305116          2  171152558.0  171152558.0  152322976  189982140   26629050.2  PushPop  compileFusion                                                     
      6.1        310938167          2  155469083.5  155469083.5  143450458  167487709   16996903.2  PushPop  executor_utils::NVRTC                                             
      6.1        310465970          2  155232985.0  155232985.0  143185538  167280432   17037662.9  PushPop  executor_utils::Nvrtc::CompileProgram                             
      2.4        119395025          1  119395025.0  119395025.0  119395025  119395025          0.0  PushPop  nvFuser::Manager::CudaFuseGraph                                   
      2.3        115792814      12800       9046.3       9316.0       6810     392149       4802.1  PushPop  FusionExecutor::RunFusion                                         
      2.3        114982277          9   12775808.6     616072.0     583828  110075597   36487426.5  PushPop  nvFuser::Manager::compileCudaFusionGroup                          
      0.9         43830226      12800       3424.2       3425.0       2750     174125       1731.8  PushPop  ExecutorRunFusion::cuLaunchKernel                                 
      0.8         42966692          1   42966692.0   42966692.0   42966692   42966692          0.0  PushPop  FusionKernelRuntime::FusionKernelRuntime                          
      0.7         37977598          1   37977598.0   37977598.0   37977598   37977598          0.0  PushPop  Finding valid fusion segment solutions                            
      0.6         30314736      12798       2368.7       2534.5       1723      19834        619.2  PushPop  ExecutorRunFusion::OutputAlloc                                    
      0.6         30251198          2   15125599.0   15125599.0    8367337   21883861    9557625.8  PushPop  GpuLower::lower                                                   
      0.4         18337145          2    9168572.5    9168572.5    4325975   14011170    6848467.1  PushPop  GpuLower::Lower::IndexLowering::getIndexedExprs                   
      0.3         12968717       6400       2026.4       1926.0       1710      20182        624.7  PushPop  FusionExecutorCache::prepareInputs                                
      0.2         11517992         33     349030.1     370896.0     279605     382253      33975.2  PushPop  GpuLower::Lower::Index::getProducerStridedIndices                 
      0.2         10822692          1   10822692.0   10822692.0   10822692   10822692          0.0  PushPop  Schedule Persistent Fusion                                        
      0.2         10821838          1   10821838.0   10821838.0   10821838   10821838          0.0  PushPop  schedulePersistentKernel                                          
      0.2          9409839         46     204561.7      11887.5       5932    1092190     231965.7  PushPop  GpuLower::Lower::UnswitchPredicate::openLoop                      
      0.1          6527615         36     181322.6     186053.5     123184     220930      26565.5  PushPop  GpuLower::Lower::Index::getConsumerStridedIndices                 
      0.1          4669621        106      44053.0      47136.0      21476      68130       8259.9  PushPop  transform_replay.cpp::getMatchedLeafPosWithoutReplayCasP          
      0.1          4042374          2    2021187.0    2021187.0    2020102    2022272       1534.4  PushPop  ConditionalFromPredicateModifier::ConditionalFromPredicateModifier
      0.1          3667004          1    3667004.0    3667004.0    3667004    3667004          0.0  PushPop  Schedule PointWise Fusion                                         
      0.1          3593629         11     326693.5     331369.0     197796     405926      65006.8  PushPop  GpuLower::Lower::Index::getReferenceRootPredicates                
      0.1          3534297         14     252449.8     176715.5       6985    1094549     310103.7  PushPop  GpuLower::Lower::UnswitchPredicate::get                           
      0.1          3490536         20     174526.8       9652.0       5104     433878     191481.3  PushPop  GpuLower::Lower::UnswitchPredicate::predicateOn                   
      0.1          3049777         42      72613.7      74355.0      56141      87546       6543.5  PushPop  TransformReplay::replayPasC                                       
      0.1          2835625         89      31861.0      32417.0      16914      43751       5668.1  PushPop  GpuLower::Lower::updateIndexCompute                               
      0.0          2475287          6     412547.8     466194.0     286093     487083      97413.1  PushPop  PersistentKernelScheduler::canSchedule                            
      0.0          2320624      12798        181.3        153.0         94       8357        123.1  PushPop  ExecutorRunFusion::IntermediateBufferAlloc                        
      0.0          1109242          4     277310.5     348616.5       6410     405599     182761.3  PushPop  GpuLower::Lower::UnswitchPredicate::openIte                       
      0.0           940613         11      85510.3      33813.0       1040     366716     113742.4  PushPop  Fusion clear                                                      
      0.0           929512         25      37180.5        711.0        195     366085      84877.0  PushPop  IrContainer clear                                                 
      0.0           929186          3     309728.7     299475.0     288722     340989      27601.0  PushPop  GpuLower::Lower::getGlobalProducerIndex                           
      0.0           927500          2     463750.0     463750.0     263303     664197     283474.9  PushPop  GpuLower::Lower::replaceSymbolicSizes                             
      0.0           906275        127       7136.0       5485.0       1073      21462       5357.0  PushPop  Fusion::resetTvUses                                               
      0.0           842540          3     280846.7     222201.0      95769     524570     220334.0  PushPop  Fusion copy                                                       
      0.0           770151          4     192537.8     193262.0     170948     212679      21806.3  PushPop  GpuLower::Lower::getGlobalConsumerIndex                           
      0.0           669523          2     334761.5     334761.5     317467     352056      24458.1  PushPop  generateCudaKernel                                                
      0.0           596543          2     298271.5     298271.5     245263     351280      74965.3  PushPop  ExecutorRunFusion::ValidateAndInitialize                          
      0.0           560285          2     280142.5     280142.5       4781     555504     389420.0  PushPop  getPersistentHeuristics                                           
      0.0           517807        529        978.8        459.0         93       8039       1205.1  PushPop  kir::ExpressionEvaluator::evaluate                                
      0.0           500444        523        956.9        651.0        116      14131        867.0  PushPop  isFusibleCudaFusionGroup                                          
      0.0           485279         13      37329.2      32398.0      30283      46848       7389.3  PushPop  transform_replay.cpp::getMatchedLeafPosWithoutReplayPasC          
      0.0           472108         18      26228.2       2780.5       2211     226947      68603.0  PushPop  GpuLower::Lower::getInlinePredicate                               
      0.0           470047          3     156682.3     118883.0      76814     274350     104051.5  PushPop  IrContainer copy                                                  
      0.0           415812          2     207906.0     207906.0      45085     370727     230263.7  PushPop  GpuLower::Lower::UnrollPass::runPass                              
      0.0           391883          7      55983.3      67305.0      20478      71276      21813.3  PushPop  TransformReplay::replayCasP                                       
      0.0           362743        767        472.9        205.0         82      11791       1283.0  PushPop  getTerminatingOutputs                                             
      0.0           259754          2     129877.0     129877.0       4553     255201     177234.9  PushPop  getPointwiseHeuristics                                            
      0.0           232475          2     116237.5     116237.5      50006     182469      93665.5  PushPop  GpuLower::Lower::LoopNestGenerator::loweredExprs                  
      0.0           198145          2      99072.5      99072.5      50037     148108      69346.7  PushPop  GpuLower::Lower::validateIr                                       
      0.0           196898          1     196898.0     196898.0     196898     196898          0.0  PushPop  GraphCache::GraphCache                                            
      0.0           196635          2      98317.5      98317.5      96252     100383       2921.1  PushPop  executor_utils::Nvrtc::LoadPTX                                    
      0.0           194812          1     194812.0     194812.0     194812     194812          0.0  PushPop  GraphCache::createFusion                                          
      0.0           193302          1     193302.0     193302.0     193302     193302          0.0  PushPop  parseJitIR                                                        
      0.0           178184          2      89092.0      89092.0      87613      90571       2091.6  PushPop  TransformRFactor::runReplay                                       
      0.0           176925          2      88462.5      88462.5      42996     133929      64299.3  PushPop  GpuLower::Lower::validateIterDomainUse                            
      0.0           159086          2      79543.0      79543.0      23842     135244      78773.1  PushPop  GpuLower::Lower::insertAllocations                                
      0.0           147488          2      73744.0      73744.0      40238     107250      47384.6  PushPop  GpuLower::Lower::validatePartialSplit                             
      0.0           119670          2      59835.0      59835.0      44751      74919      21332.0  PushPop  reuseMemoryAllocations                                            
      0.0           106305          2      53152.5      53152.5      31424      74881      30728.7  PushPop  GpuLower::Lower::ThreadPredicateMap                               
      0.0           105991          4      26497.8      20703.5      16196      48388      14794.9  PushPop  FusionExecutor::ComputeLaunchParams                               
      0.0           102151          1     102151.0     102151.0     102151     102151          0.0  PushPop  TransposeScheduler::canScheduleRunTime                            
      0.0            98998        240        412.5        238.0        136       3883        407.0  PushPop  GpuLower::Lower::IndexCompute::IndexCompute                       
      0.0            85876          2      42938.0      42938.0      26615      59261      23084.2  PushPop  GpuLower::Lower::validateParallelize                              
      0.0            61323          2      30661.5      30661.5      19391      41932      15938.9  PushPop  executor_utils::NvrtcCreateProgram                                
      0.0            55499          8       6937.4       8266.5        789      10471       3083.7  PushPop  scheduler_utils::persistentBufferSize                             
      0.0            50196          2      25098.0      25098.0      21644      28552       4884.7  PushPop  GpuLower::Lower::validateVectorize                                
      0.0            47871          2      23935.5      23935.5      23014      24857       1303.2  PushPop  FusionExecutor::AllocOutputs                                      
      0.0            47097          2      23548.5      23548.5      16908      30189       9391.1  PushPop  executor_utils::Nvrtc::GetPTX                                     
      0.0            44004          2      22002.0      22002.0      21112      22892       1258.7  PushPop  inferAndAlloc                                                     
      0.0            36791          9       4087.9       2639.0       2376      15550       4309.3  PushPop  TypePropagate                                                     
      0.0            34727         10       3472.7       1440.0       1020      17670       5149.6  PushPop  FusionExecutor::ParallelBindingResolution                         
      0.0            27702          2      13851.0      13851.0       6928      20774       9790.6  PushPop  GpuLower::Lower::insertMagicZero                                  
      0.0            25345          2      12672.5      12672.5      10728      14617       2749.9  PushPop  GpuLower::Lower::insertRawThreadSynchronization                   
      0.0            22556          2      11278.0      11278.0       6851      15705       6260.7  PushPop  Kernel::analyze                                                   
      0.0            16272          2       8136.0       8136.0       7926       8346        297.0  PushPop  executor_utils::ValidateKernelInputs                              
      0.0            15992         16        999.5        979.5        306       1685        412.3  PushPop  GpuLower::Lower::ThreadPredicateMap::updateBitSet                 
      0.0            14337          2       7168.5       7168.5       4046      10291       4415.9  PushPop  GpuLower::Lower::processMisalignedVectorization                   
      0.0            13260         21        631.4         95.0         88       5971       1567.6  PushPop  PeepholeOptimizeShapeExpressions                                  
      0.0            13204          2       6602.0       6602.0       3387       9817       4546.7  PushPop  GpuLower::Lower::MisalignedVectorizationModifier                  
      0.0            10779         19        567.3        541.0        454        747         91.7  PushPop  ~Segmenter::FusionSegmentGuard                                    
      0.0            10281          2       5140.5       5140.5       5049       5232        129.4  PushPop  FusionExecutor::validateVectorizedTensors                         
      0.0             9456         19        497.7        391.0        284       1819        381.4  PushPop  Segmenter::FusionSegmentGuard                                     
      0.0             8964          2       4482.0       4482.0       2774       6190       2415.5  PushPop  GpuLower::Lower::insertWarThreadSynchronization                   
      0.0             7558          2       3779.0       3779.0       3580       3978        281.4  PushPop  executor_utils::BindKernelInputs                                  
      0.0             7388          4       1847.0       1833.5        591       3130       1307.3  PushPop  PrecomputedValues::Evaluate                                       
      0.0             6066          3       2022.0       1757.0       1334       2975        852.0  PushPop  executor_utils::BindFusionInputs                                  
      0.0             4761          2       2380.5       2380.5       1970       2791        580.5  PushPop  executor_utils::NvrtcDestroyProgram                               
      0.0             2683          2       1341.5       1341.5       1280       1403         87.0  PushPop  ExecutorRunFusion::FillCacheEntry                                 
      0.0             1125          4        281.3        286.5        157        395        108.4  PushPop  PrecomputedValuess::Validate                                      
      0.0              873          2        436.5        436.5        354        519        116.7  PushPop  FusionExecutor::AllocGlobalVals                                   
      0.0              859          4        214.8        121.0        108        509        196.3  PushPop  computeSharedMemory                                               

[4/7] Executing 'cuda_api_sum' stats report

 Time (%)  Total Time (ns)  Num Calls  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)              Name            
 --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  ----------------------------
     68.3        311679946      77668     4013.0     3552.0      2896   7627411      49100.8  cudaLaunchKernel            
     13.6         62065084         13  4774237.2     1375.0       777  62047106   17208355.4  cudaStreamIsCapturing_v10000
      8.2         37610235      12800     2938.3     2932.0      2429     24590        783.7  cuLaunchKernel              
      5.2         23731701        278    85365.8    53181.5      7967   9821927     586139.0  cudaMemcpyAsync             
      2.4         10889422          2  5444711.0  5444711.0   1189999   9699423    6017071.4  cudaFree                    
      1.5          6671946          1  6671946.0  6671946.0   6671946   6671946          0.0  cudaProfilerStop            
      0.4          1903965         14   135997.5   100273.0      3971    578098     139487.7  cudaMalloc                  
      0.2           707605        202     3503.0     3290.0      2681     13964        892.0  cudaDeviceSynchronize       
      0.1           673071        275     2447.5     2199.0       837     20881       2411.7  cudaStreamSynchronize       
      0.1           347440       1536      226.2      183.0        91      2449        137.2  cuGetProcAddress            
      0.0           179715          2    89857.5    89857.5     87036     92679       3990.2  cuModuleLoadDataEx          
      0.0            17471         18      970.6      330.0       311     11259       2569.1  cudaEventCreateWithFlags    
      0.0            15817          1    15817.0    15817.0     15817     15817          0.0  cudaProfilerStart           
      0.0             5111          4     1277.8     1293.0       824      1701        359.7  cuInit                      
      0.0             2034          5      406.8      226.0       165       976        337.2  cuModuleGetLoadingMode      

[5/7] Executing 'cuda_gpu_kern_sum' stats report

 Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                                                  Name                                                
 --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------------------------------------------------------------------------------------
     54.9        234357935      38594    6072.4    8320.0      1535      8960       3195.5  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::
     30.2        128953440      32160    4009.7    4000.0      3872      6209         49.2  std::enable_if<!T7, void>::type internal::gemvx::kernel<int, int, float, float, float, float, (bool
      9.3         39842356       6432    6194.4    6177.0      6048      6432         45.4  std::enable_if<!T7, void>::type internal::gemvx::kernel<int, int, float, float, float, float, (bool
      3.1         13248633       6400    2070.1    2080.0      2016      2240         16.2  CudaCodeGen::kernel2(CudaCodeGen::Tensor<float, (int)4>, CudaCodeGen::Tensor<float, (int)4>)        
      1.8          7550448       6400    1179.8    1184.0      1151      1217         13.0  CudaCodeGen::kernel1(CudaCodeGen::Tensor<float, (int)4>, CudaCodeGen::Tensor<float, (int)4>)        
      0.6          2573720        402    6402.3    6400.0      5984      7200         64.9  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctorOnSelf_add<float>, at
      0.0            46239         32    1445.0    1440.0      1408      1472         14.3  void <unnamed>::softmax_warp_forward<float, float, float, (int)6, (bool)0, (bool)0>(T2 *, const T1 
      0.0            34368         32    1074.0    1088.0      1056      1120         18.1  void at::native::vectorized_elementwise_kernel<(int)4, at::native::AUnaryFunctor<float, float, floa
      0.0            33153          6    5525.5    5472.0      5408      5888        179.4  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<bool, at::native::func_wrappe
      0.0            10816          2    5408.0    5408.0      5056      5760        497.8  void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<float>, at::detail::
      0.0            10496          2    5248.0    5248.0      5152      5344        135.8  void at::native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, (int)4, void at::nat
      0.0             6880          6    1146.7    1120.0      1088      1280         74.1  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BinaryFunctor<float, float, bool

[6/7] Executing 'cuda_gpu_mem_time_sum' stats report

 Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)            Operation           
 --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------------------
     97.5          4100905    270   15188.5   15489.0       832     15873       2165.1  [CUDA memcpy Device-to-Host]  
      2.1            89537      5   17907.4   17920.0     17664     18112        160.9  [CUDA memcpy Host-to-Device]  
      0.3            14304      3    4768.0    6528.0      1120      6656       3159.9  [CUDA memcpy Device-to-Device]

[7/7] Executing 'cuda_gpu_mem_size_sum' stats report

 Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)            Operation           
 ----------  -----  --------  --------  --------  --------  -----------  ------------------------------
     51.905    270     0.192     0.197     0.000     0.197        0.029  [CUDA memcpy Device-to-Host]  
     25.362      3     8.454    12.583     0.197    12.583        7.151  [CUDA memcpy Device-to-Device]
      0.983      5     0.197     0.197     0.197     0.197        0.000  [CUDA memcpy Host-to-Device]  

Generated:
    /root/mlir/baseline/attention/report2.nsys-rep
    /root/mlir/baseline/attention/report2.sqlite
